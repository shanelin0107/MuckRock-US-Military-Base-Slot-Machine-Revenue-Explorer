{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber tabula-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iH6EUeiPO7K",
        "outputId": "957ea8e9-d81a-41c1-820e-d82ff89d0045"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.7-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tabula-py\n",
            "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting pdfminer.six==20250506 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20250506-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.2.2)\n",
            "Requirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.0.2)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.12/dist-packages (from tabula-py) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.7-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250506-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, tabula-py, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20250506 pdfplumber-0.11.7 pypdfium2-4.30.0 tabula-py-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = 'https://drive.google.com/uc?export=download&id=1jKoXzzd5bqpcJeT_TtchmoGza3WRY8Xc'\n",
        "output = 'Marine_Revenue_FY20-FY24.pdf'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "url = 'https://drive.google.com/uc?export=download&id=1QtmW3UhIMXDLQoI3wI7zH88SACFurjcR'\n",
        "output = 'FY2023_Asset_Report.pdf'\n",
        "gdown.download(url, output, quiet=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "DAybzsS7IIKP",
        "outputId": "b75d5db4-4928-462f-e626-420136aa52b6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1jKoXzzd5bqpcJeT_TtchmoGza3WRY8Xc\n",
            "To: /content/Marine_Revenue_FY20-FY24.pdf\n",
            "100%|██████████| 1.64M/1.64M [00:00<00:00, 19.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1QtmW3UhIMXDLQoI3wI7zH88SACFurjcR\n",
            "To: /content/FY2023_Asset_Report.pdf\n",
            "100%|██████████| 13.5M/13.5M [00:00<00:00, 88.6MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FY2023_Asset_Report.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdfplumber\n",
        "from pathlib import Path\n",
        "\n",
        "# ---- Paths ----\n",
        "PDF = Path(\"/content/Marine_Revenue_FY20-FY24.pdf\")\n",
        "OUT = PDF.parent / \"Marine_Revenue_FY20-FY24__p1to120_skipped_stable.csv\"\n",
        "\n",
        "# ---- Columns and Extraction Parameters ----\n",
        "COLS = [\"Loc #\", \"Location\", \"Month\", \"Revenue\", \"NAFI Amt\", \"Annual Revenue\", \"Annual NAFI\"]\n",
        "\n",
        "# Base vertical cut lines (as a fallback)\n",
        "BASE_CUTS = [-1e9, 156.21, 214.20, 269.19, 327.93, 393.67, 460.17, 1e9]\n",
        "# Allowed deviation clamp for each cut line\n",
        "CLAMP_DELTA = [0, 30, 40, 40, 30, 30, 30, 0]\n",
        "\n",
        "# Layout/Aggregation Parameters\n",
        "Y_TOL = 3.0        # Vertical merge tolerance\n",
        "X_JOIN = 3.0       # Horizontal text join tolerance (character spacing)\n",
        "GAP_RATIO = 0.8    # Smart word join space threshold\n",
        "DROP_MIN = 2       # Minimum non-empty columns to keep a row\n",
        "EDGE_PAD = 2.0     # Cut line edge padding\n",
        "LEFT_SHIFT_MONTH   = 18   # Month left boundary adjustment\n",
        "LEFT_SHIFT_REVENUE = 8    # Revenue left boundary adjustment\n",
        "\n",
        "# ---- Regex ----\n",
        "MONTH_FULL = re.compile(r\"^(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[-/]?\\d{2}$\", re.I)\n",
        "MONTH_REV  = re.compile(r\"^\\d{2}[-/](?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)$\", re.I)\n",
        "MONTH_ONLY = re.compile(r\"^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)$\", re.I)\n",
        "NUM_RE     = re.compile(r\"^-?\\(?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\)?$\")\n",
        "INT_RE     = re.compile(r\"^\\d+$\")\n",
        "YEAR_FRAG  = {-17, -18, -19, -20}\n",
        "NUM_RE_STRICT = re.compile(r\"^-?\\(?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\)?$\")\n",
        "\n",
        "# ---- Header Patterns ----\n",
        "HEADER_PATTERNS = {\n",
        "    \"Loc #\": re.compile(r\"^Loc\\s*#?$\", re.I),\n",
        "    \"Location\": re.compile(r\"^Location$\", re.I),\n",
        "    \"Month\": re.compile(r\"^Month$\", re.I),\n",
        "    \"Revenue\": re.compile(r\"^Revenue$\", re.I),\n",
        "    \"NAFI Amt\": re.compile(r\"^NAFI\\s*Amt$\", re.I),\n",
        "    \"Annual Revenue\": re.compile(r\"^Annual\\s+Revenue$\", re.I),\n",
        "    \"Annual NAFI\": re.compile(r\"^Annual\\s+NAFI$\", re.I),\n",
        "}\n",
        "HEADER_ORDER = [\"Loc #\", \"Location\", \"Month\", \"Revenue\", \"NAFI Amt\", \"Annual Revenue\", \"Annual NAFI\"]\n",
        "\n",
        "# ==== Header/Noise Keywords (to exclude non-data row tokens) ====\n",
        "EXCLUDE_LINE_PATTERNS = [\n",
        "    re.compile(r\"^ARMP\\s+Marine\\s+Slot\\s+Report$\", re.I),\n",
        "    re.compile(r\"^Monthly\\s+Summary\\s+by\\s+Location$\", re.I),\n",
        "    re.compile(r\"^Slot\\s+Revenue$\", re.I),\n",
        "    re.compile(r\"^NAFI\\s+Reimbursement\\s+from\\s+ARMP$\", re.I),\n",
        "    re.compile(r\"^Region$\", re.I),\n",
        "    re.compile(r\"^Loc\\s*#?$\", re.I),\n",
        "    re.compile(r\"^Location$\", re.I),\n",
        "    re.compile(r\"^Month$\", re.I),\n",
        "    re.compile(r\"^Revenue$\", re.I),\n",
        "    re.compile(r\"^NAFI\\s*Amt$\", re.I),\n",
        "    re.compile(r\"^Annual\\s+Revenue$\", re.I),\n",
        "    re.compile(r\"^Annual\\s+NAFI$\", re.I),\n",
        "]\n",
        "\n",
        "def _is_exclude_token(t):\n",
        "    txt = t[\"text\"].strip()\n",
        "    return any(p.match(txt) for p in EXCLUDE_LINE_PATTERNS)\n",
        "\n",
        "# ---------------- Basic Utilities ----------------\n",
        "def chars_to_words(page, x_tol=X_JOIN, y_tol=Y_TOL):\n",
        "    chars = sorted(page.chars, key=lambda c: (round(c[\"top\"], 1), c[\"x0\"]))\n",
        "    lines, words = [], []\n",
        "    for ch in chars:\n",
        "        if not lines or abs(ch[\"top\"] - lines[-1][\"y\"]) > y_tol:\n",
        "            lines.append({\"y\": ch[\"top\"], \"chars\": [ch]})\n",
        "        else:\n",
        "            lines[-1][\"chars\"].append(ch)\n",
        "    for line in lines:\n",
        "        row = sorted(line[\"chars\"], key=lambda c: c[\"x0\"])\n",
        "        cur = [row[0]]\n",
        "        for c in row[1:]:\n",
        "            if c[\"x0\"] - cur[-1][\"x1\"] <= x_tol:\n",
        "                cur.append(c)\n",
        "            else:\n",
        "                words.append({\"text\": \"\".join(x[\"text\"] for x in cur), \"x0\": cur[0][\"x0\"], \"x1\": cur[-1][\"x1\"], \"top\": line[\"y\"]})\n",
        "                cur = [c]\n",
        "        if cur:\n",
        "            words.append({\"text\": \"\".join(x[\"text\"] for x in cur), \"x0\": cur[0][\"x0\"], \"x1\": cur[-1][\"x1\"], \"top\": line[\"y\"]})\n",
        "    return words\n",
        "\n",
        "def smart_join(tokens):\n",
        "    if not tokens: return \"\"\n",
        "    tokens = sorted(tokens, key=lambda w: w[\"x0\"])\n",
        "    widths = [t[\"x1\"] - t[\"x0\"] for t in tokens]\n",
        "    cw = (np.median(widths) or 1.0) * GAP_RATIO\n",
        "    s, prev = tokens[0][\"text\"], tokens[0]\n",
        "    for t in tokens[1:]:\n",
        "        s += (\"\" if (t[\"x0\"] - prev[\"x1\"]) <= cw else \" \") + t[\"text\"]\n",
        "        prev = t\n",
        "    return s.strip()\n",
        "\n",
        "def assign_bin(xmid, cuts):\n",
        "    for i in range(len(cuts) - 1):\n",
        "        if cuts[i] - EDGE_PAD <= xmid <= cuts[i + 1] + EDGE_PAD:\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "def header_y_of(words):\n",
        "    ys = [w[\"top\"] for w in words if w[\"text\"] in (\"Location\", \"Month\", \"Revenue\")]\n",
        "    return min(ys) if ys else 0\n",
        "\n",
        "def group_by_y(words, y_tol=Y_TOL):\n",
        "    lines = []\n",
        "    for w in sorted(words, key=lambda w: (round(w[\"top\"], 1), w[\"x0\"])):\n",
        "        if not lines or abs(w[\"top\"] - lines[-1][\"y\"]) > y_tol:\n",
        "            lines.append({\"y\": w[\"top\"], \"tokens\": [w]})\n",
        "        else:\n",
        "            lines[-1][\"tokens\"].append(w)\n",
        "    return lines\n",
        "\n",
        "# ---------------- Infer Cut Lines from Header ----------------\n",
        "def detect_header_line(words):\n",
        "    candidates = []\n",
        "    for line in group_by_y(words):\n",
        "        texts = [t[\"text\"].strip() for t in line[\"tokens\"]]\n",
        "        hit = sum(1 for pat in HEADER_PATTERNS.values() if any(pat.match(tx) for tx in texts))\n",
        "        if hit >= 3:\n",
        "            candidates.append((line[\"y\"], line[\"tokens\"], hit))\n",
        "    if not candidates:\n",
        "        return 0, None\n",
        "    candidates.sort(key=lambda x: (x[2], x[0]))\n",
        "    y, toks, _ = candidates[-1]\n",
        "    return y, toks\n",
        "\n",
        "def cuts_from_header(words, base_cuts, prev_cuts=None):\n",
        "    header_y, header_tokens = detect_header_line(words)\n",
        "    if not header_tokens:\n",
        "        # Use default cuts to prevent accumulating errors\n",
        "        cuts = list(base_cuts)\n",
        "        return cuts, (header_y or header_y_of(words))\n",
        "\n",
        "    name_to_x0 = {}\n",
        "    for name, pat in HEADER_PATTERNS.items():\n",
        "        xs = [t[\"x0\"] for t in header_tokens if pat.match(t[\"text\"].strip())]\n",
        "        if xs:\n",
        "            name_to_x0[name] = float(min(xs))\n",
        "\n",
        "    starts = []\n",
        "    ref = prev_cuts or base_cuts\n",
        "    for name in HEADER_ORDER:\n",
        "        if name in name_to_x0:\n",
        "            starts.append(name_to_x0[name])\n",
        "        else:\n",
        "            col_idx = HEADER_ORDER.index(name)\n",
        "            left_edge = ref[col_idx]\n",
        "            right_edge = ref[col_idx + 1]\n",
        "            starts.append((left_edge * 0.65 + right_edge * 0.35))\n",
        "\n",
        "    cuts = [-1e9]\n",
        "    for i in range(1, len(starts)):\n",
        "        cuts.append((starts[i - 1] + starts[i]) / 2.0)\n",
        "    cuts.append(1e9)\n",
        "\n",
        "    idx_month = HEADER_ORDER.index(\"Month\")\n",
        "    idx_revenue = HEADER_ORDER.index(\"Revenue\")\n",
        "    cuts[idx_month]  -= LEFT_SHIFT_MONTH\n",
        "    cuts[idx_revenue] -= LEFT_SHIFT_REVENUE\n",
        "\n",
        "    clamped = []\n",
        "    for i, c in enumerate(cuts):\n",
        "        base = base_cuts[i] if i < len(base_cuts) else c\n",
        "        delta = CLAMP_DELTA[i] if i < len(CLAMP_DELTA) else 50\n",
        "        clamped.append(max(base - delta, min(base + delta, c)))\n",
        "    return clamped, header_y\n",
        "\n",
        "# ---------------- Type-Aware Cut Line Refinement ----------------\n",
        "def _is_month_token(txt: str) -> bool:\n",
        "    s = str(txt).strip()\n",
        "    return bool(MONTH_FULL.match(s) or MONTH_REV.match(s) or MONTH_ONLY.match(s))\n",
        "\n",
        "def _is_money_token(txt: str) -> bool:\n",
        "    s = str(txt).strip().replace(\",\", \"\")\n",
        "    return bool(NUM_RE.match(s))\n",
        "\n",
        "def refine_cuts_typeaware(page, cuts, hy, sample_rows=None):\n",
        "    words = chars_to_words(page)\n",
        "    body = [w for w in words if w[\"top\"] > hy + 1 and not _is_exclude_token(w)]\n",
        "\n",
        "    rows, cur_y, cur = [], None, []\n",
        "    for w in sorted(body, key=lambda w: (round(w[\"top\"], 1), w[\"x0\"])):\n",
        "        y = round(w[\"top\"], 1)\n",
        "        if cur_y is None or abs(y - cur_y) <= Y_TOL:\n",
        "            cur.append(w); cur_y = y if cur_y is None else (cur_y + y) / 2\n",
        "        else:\n",
        "            rows.append(cur); cur = [w]; cur_y = y\n",
        "    if cur: rows.append(cur)\n",
        "\n",
        "    month_right, revenue_left = [], []\n",
        "    revenue_right, nafi_left = [], []\n",
        "    nafi_right, annual_rev_left = [], []\n",
        "    annual_rev_right, annual_nafi_left = [], []\n",
        "\n",
        "    for r in rows:\n",
        "        for w in r:\n",
        "            xmid = (w[\"x0\"] + w[\"x1\"]) / 2\n",
        "            bi = assign_bin(xmid, cuts)\n",
        "            if bi is None:\n",
        "                continue\n",
        "            txt = w[\"text\"]\n",
        "            if bi == 2 and _is_month_token(txt):  # Month bucket\n",
        "                month_right.append(w[\"x1\"])\n",
        "            if bi == 3 and _is_money_token(txt):  # Revenue bucket\n",
        "                revenue_left.append(w[\"x0\"])\n",
        "                revenue_right.append(w[\"x1\"])\n",
        "            if bi == 4 and _is_money_token(txt):  # NAFI Amt bucket\n",
        "                nafi_left.append(w[\"x0\"])\n",
        "                nafi_right.append(w[\"x1\"])\n",
        "            if bi == 5 and _is_money_token(txt):  # Annual Revenue bucket\n",
        "                annual_rev_left.append(w[\"x0\"])\n",
        "                annual_rev_right.append(w[\"x1\"])\n",
        "            if bi == 6 and _is_money_token(txt):  # Annual NAFI bucket\n",
        "                annual_nafi_left.append(w[\"x0\"])\n",
        "\n",
        "    def pct(a, p):\n",
        "        return float(np.percentile(a, p)) if a else None\n",
        "\n",
        "    # Month↔Revenue boundary (cuts[3])\n",
        "    left_p = pct(month_right, 100)\n",
        "    right_p = pct(revenue_left, 0)\n",
        "    if left_p is not None and right_p is not None and right_p > left_p:\n",
        "        target = (left_p + right_p) / 2\n",
        "        base = BASE_CUTS[3]; delta = CLAMP_DELTA[3]\n",
        "        cuts[3] = max(base - delta, min(base + delta, target))\n",
        "    if left_p is not None and right_p is None:\n",
        "        cuts[3] = max(cuts[3], left_p + 1)\n",
        "\n",
        "    # Revenue↔NAFI boundary (cuts[4])\n",
        "    left_p = pct(revenue_right, 100)\n",
        "    right_p = pct(nafi_left, 0)\n",
        "    if left_p is not None and right_p is not None and right_p > left_p:\n",
        "        target = (left_p + right_p) / 2\n",
        "        base = BASE_CUTS[4]; delta = CLAMP_DELTA[4]\n",
        "        cuts[4] = max(base - delta, min(base + delta, target))\n",
        "    if left_p is not None and right_p is None:\n",
        "        cuts[4] = max(cuts[4], left_p + 1)\n",
        "\n",
        "    # NAFI Amt↔Annual Revenue boundary (cuts[5])\n",
        "    left_p = pct(nafi_right, 100)\n",
        "    right_p = pct(annual_rev_left, 0)\n",
        "    if left_p is not None and right_p is not None and right_p > left_p:\n",
        "        target = (left_p + right_p) / 2\n",
        "        base = BASE_CUTS[5]; delta = CLAMP_DELTA[5]\n",
        "        cuts[5] = max(base - delta, min(base + delta, target))\n",
        "    if left_p is not None and right_p is None:\n",
        "        cuts[5] = max(cuts[5], left_p + 1)\n",
        "\n",
        "    # Annual Revenue↔Annual NAFI boundary (cuts[6])\n",
        "    left_p = pct(annual_rev_right, 100)\n",
        "    right_p = pct(annual_nafi_left, 0)\n",
        "    if left_p is not None and right_p is not None and right_p > left_p:\n",
        "        target = (left_p + right_p) / 2\n",
        "        base = BASE_CUTS[6]; delta = CLAMP_DELTA[6]\n",
        "        cuts[6] = max(base - delta, min(base + delta, target))\n",
        "    if left_p is not None and right_p is None:\n",
        "        cuts[6] = max(cuts[6], left_p + 1)\n",
        "\n",
        "    return cuts\n",
        "\n",
        "# ---------------- Extract Rows Using Cut Lines ----------------\n",
        "def rows_from_page(page, cuts, hy):\n",
        "    words = chars_to_words(page)\n",
        "    body = [w for w in words if w[\"top\"] > hy + 1 and not _is_exclude_token(w)]\n",
        "\n",
        "    rows, cur_y, cur = [], None, []\n",
        "    for w in sorted(body, key=lambda w: (round(w[\"top\"], 1), w[\"x0\"])):\n",
        "        y = round(w[\"top\"], 1)\n",
        "        if cur_y is None or abs(y - cur_y) <= Y_TOL:\n",
        "            cur.append(w); cur_y = y if cur_y is None else (cur_y + y) / 2\n",
        "        else:\n",
        "            rows.append(cur); cur = [w]; cur_y = y\n",
        "    if cur: rows.append(cur)\n",
        "\n",
        "    out = []\n",
        "    for r in rows:\n",
        "        buckets = {i: [] for i in range(len(cuts) - 1)}\n",
        "        for w in r:\n",
        "            xmid = (w[\"x0\"] + w[\"x1\"]) / 2\n",
        "            bi = assign_bin(xmid, cuts)\n",
        "            if bi is not None:\n",
        "                buckets[bi].append(w)\n",
        "\n",
        "        vals = [smart_join(buckets[i]) for i in range(len(cuts) - 1)]\n",
        "\n",
        "        # If the 3rd column (Month) looks like a monetary value and the 4th (Revenue) looks like a month -> swap them\n",
        "        if len(vals) >= 4:\n",
        "            m, rv = vals[2].strip(), vals[3].strip()\n",
        "            if (m and NUM_RE_STRICT.match(m)) and (rv and (MONTH_FULL.match(rv) or MONTH_REV.match(rv) or MONTH_ONLY.match(rv))):\n",
        "                vals[2], vals[3] = rv, m\n",
        "\n",
        "        if sum(1 for v in vals if v not in (\"\", \"-\")) >= DROP_MIN:\n",
        "            out.append(vals)\n",
        "\n",
        "    return pd.DataFrame(out, columns=COLS)\n",
        "\n",
        "# ---------------- Repair Functions ----------------\n",
        "def repair_loc_and_location(df):\n",
        "    mask = df[\"Loc #\"].astype(str).str.match(r\"^\\d+\\s+\\S+\", na=False)\n",
        "    if mask.any():\n",
        "        ex = df.loc[mask, \"Loc #\"].astype(str).str.extract(r\"^(\\d+)\\s+(.*)$\")\n",
        "        df.loc[mask, \"Loc #\"] = ex[0]\n",
        "        df.loc[mask, \"Location\"] = (ex[1].fillna(\"\").str.strip() + \" \" + df.loc[mask, \"Location\"].fillna(\"\")).str.strip().replace({\"\": None})\n",
        "\n",
        "    # Added: Handle cases where Location is a pure 6-digit number (append to the previous row's Location)\n",
        "    for i in range(len(df)-1, 0, -1):\n",
        "        cur_loc = str(df.at[i, \"Location\"]).strip()\n",
        "        if re.fullmatch(r\"\\d{6}\", cur_loc):\n",
        "            prev_loc = str(df.at[i-1, \"Location\"]).strip()\n",
        "            if prev_loc:\n",
        "                df.at[i-1, \"Location\"] = (prev_loc + \" \" + cur_loc).strip()\n",
        "                df.at[i, \"Location\"] = np.nan\n",
        "\n",
        "    for i in range(1, len(df)):\n",
        "        cur_locnum = str(df.at[i, \"Loc #\"]) if pd.notna(df.at[i, \"Loc #\"]) else \"\"\n",
        "        cur_loc = str(df.at[i, \"Location\"]) if pd.notna(df.at[i, \"Location\"]) else \"\"\n",
        "        prev_loc = str(df.at[i - 1, \"Location\"]) if pd.notna(df.at[i - 1, \"Location\"]) else \"\"\n",
        "        if re.fullmatch(r\"\\d{6}\", cur_locnum) and (cur_loc == \"\" or cur_loc.lower() == \"nan\") and prev_loc not in (\"\", \"nan\"):\n",
        "            df.at[i - 1, \"Location\"] = (prev_loc + \" \" + cur_locnum).strip()\n",
        "            df.at[i, \"Loc #\"] = df.at[i - 1, \"Loc #\"]\n",
        "    return df\n",
        "\n",
        "def split_and_swap_month_revenue(df):\n",
        "    def split_cell(mon, rev):\n",
        "        ms = (None if pd.isna(mon) else str(mon).strip())\n",
        "        rs = (None if pd.isna(rev) else str(rev).strip())\n",
        "        if ms:\n",
        "            cleaned = ms.replace(\",\", \"\")\n",
        "            toks = re.split(r\"\\s+\", cleaned)\n",
        "            mon_tok = next((t for t in toks if _is_month_token(t)), None)\n",
        "            money_toks = [t for t in toks if not _is_month_token(t)]\n",
        "            money = \"\".join(money_toks)\n",
        "            if mon_tok: ms = mon_tok\n",
        "            if money and not (rs and NUM_RE_STRICT.match(rs)): rs = money\n",
        "        if rs and _is_month_token(rs):\n",
        "            if ms and (NUM_RE_STRICT.match(ms) or NUM_RE_STRICT.match(ms.replace(\"$\",\"\"))):\n",
        "                ms, rs = rs, ms\n",
        "        ms = ms if (ms and ms.strip(\"-\")) else None\n",
        "        rs = rs if (rs and rs.strip(\"-\")) else None\n",
        "        return ms, rs\n",
        "    tmp = df.apply(lambda r: split_cell(r[\"Month\"], r[\"Revenue\"]), axis=1, result_type=\"expand\")\n",
        "    df[\"Month\"], df[\"Revenue\"] = tmp[0], tmp[1]\n",
        "    return df\n",
        "\n",
        "def normalize_months(df):\n",
        "    def norm(m):\n",
        "        if pd.isna(m):\n",
        "            return m\n",
        "        s = str(m).strip()\n",
        "        if MONTH_REV.match(s):\n",
        "            parts = re.split(r'[-/]', s)\n",
        "            if len(parts) == 2:\n",
        "                year, mon = parts\n",
        "                mon = mon[:3].capitalize()\n",
        "                return mon + '-' + year\n",
        "        return s\n",
        "    df[\"Month\"] = df[\"Month\"].apply(norm)\n",
        "    return df\n",
        "\n",
        "def _seeded_ffill(series: pd.Series, seed):\n",
        "    s = series.replace({\"\": None}).copy()\n",
        "    if s.empty: return s\n",
        "    if pd.isna(s.iloc[0]) and seed is not None:\n",
        "        s.iloc[0] = seed\n",
        "    return s.ffill()\n",
        "\n",
        "def pagewise_seeded_ffill(dfp: pd.DataFrame, prev_locnum, prev_loc):\n",
        "    for col in [\"Loc #\", \"Location\"]:\n",
        "        dfp[col] = dfp[col].replace({\"\": None})\n",
        "    dfp[\"Loc #\"]    = _seeded_ffill(dfp[\"Loc #\"], prev_locnum)\n",
        "    dfp[\"Location\"] = _seeded_ffill(dfp[\"Location\"], prev_loc)\n",
        "    return dfp\n",
        "\n",
        "def finalize(df):\n",
        "    mask = df[\"Month\"].astype(str).str.match(MONTH_ONLY, na=False) & df[\"Revenue\"].astype(str).isin({str(x) for x in YEAR_FRAG})\n",
        "    df.loc[mask, \"Month\"] = df.loc[mask, \"Month\"] + \"-\" + df.loc[mask, \"Revenue\"].astype(str).str[-2:]\n",
        "    df.loc[mask, \"Revenue\"] = np.nan\n",
        "\n",
        "    df[\"Loc #\"] = df[\"Loc #\"].replace({\"\": None}).ffill()\n",
        "    df[\"Location\"] = df[\"Location\"].replace({\"\": None}).ffill()\n",
        "\n",
        "    for c in [\"Revenue\", \"NAFI Amt\", \"Annual Revenue\", \"Annual NAFI\"]:\n",
        "        df[c] = (df[c].astype(str)\n",
        "                 .str.replace(\" \", \"\", regex=False)\n",
        "                 .str.replace(\",\", \"\", regex=False)\n",
        "                 .str.replace(\"$\", \"\", regex=False)\n",
        "                 .str.replace(\"(\", \"-\", regex=False)\n",
        "                 .str.replace(\")\", \"\", regex=False))\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    return df.dropna(how=\"all\").reset_index(drop=True)\n",
        "\n",
        "# --- Left-pack Monthly Numeric Values ---\n",
        "def monthly_numeric_left_pack(df):\n",
        "    num_cols = [\"Revenue\", \"NAFI Amt\", \"Annual Revenue\", \"Annual NAFI\"]\n",
        "    def is_monthlike(x):\n",
        "        if pd.isna(x): return False\n",
        "        s = str(x).strip()\n",
        "        return bool(MONTH_FULL.match(s) or MONTH_REV.match(s) or MONTH_ONLY.match(s))\n",
        "    def pack_row(r):\n",
        "        if not is_monthlike(r.get(\"Month\")):\n",
        "            return r\n",
        "        vals = [r[c] for c in num_cols]\n",
        "        avail = [v for v in vals if pd.notna(v)]\n",
        "        if avail:\n",
        "            r[\"Revenue\"] = avail[0]\n",
        "            r[\"NAFI Amt\"] = avail[1] if len(avail) > 1 else np.nan\n",
        "            r[\"Annual Revenue\"] = avail[2] if len(avail) > 2 else np.nan\n",
        "            r[\"Annual NAFI\"] = avail[3] if len(avail) > 3 else np.nan\n",
        "        return r\n",
        "    return df.apply(pack_row, axis=1)\n",
        "\n",
        "# ---------------- Fill Missing Annual Totals ----------------\n",
        "def fill_missing_annual(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    month_map = {m: i for i, m in enumerate([\"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Oct\",\"Nov\",\"Dec\"], start=1)}\n",
        "    def parse_date(m):\n",
        "        if pd.isna(m):\n",
        "            return None\n",
        "        m = str(m)\n",
        "        if re.match(r\"^[A-Za-z]{3}-\\d{2}$\", m):\n",
        "            mon, yr = m.split('-')\n",
        "            return (2000 + int(yr), month_map.get(mon[:3], 0))\n",
        "        return None\n",
        "    df[\"_ym\"] = df[\"Month\"].apply(parse_date)\n",
        "    for (locnum, loc), idxs in df.groupby([\"Loc #\", \"Location\"], dropna=False).groups.items():\n",
        "        idxs = list(idxs)\n",
        "        sub = df.loc[idxs].copy()\n",
        "        sub[\"_ym_key\"] = sub[\"_ym\"].apply(lambda t: t[0]*100 + t[1] if t else None)\n",
        "        sub = sub.sort_values(\"_ym_key\")\n",
        "        revenues = sub[\"Revenue\"].fillna(0).to_list()\n",
        "        nafis    = sub[\"NAFI Amt\"].fillna(0).to_list()\n",
        "        for i, (idx, row) in enumerate(sub.iterrows()):\n",
        "            ym = row[\"_ym\"]\n",
        "            if (pd.isna(row[\"Annual Revenue\"]) or pd.isna(row[\"Annual NAFI\"])) and ym and ym[1] == 9:\n",
        "                start = max(0, i-11)\n",
        "                s_rev = sum(revenues[start:i+1])\n",
        "                s_naf = sum(nafis[start:i+1])\n",
        "                if pd.isna(row[\"Annual Revenue\"]):\n",
        "                    df.at[idx,\"Annual Revenue\"] = s_rev\n",
        "                if pd.isna(row[\"Annual NAFI\"]):\n",
        "                    df.at[idx,\"Annual NAFI\"] = s_naf\n",
        "    df.drop(columns=[\"_ym\"], inplace=True, errors=\"ignore\")\n",
        "    df.drop(columns=[\"_ym_key\"], inplace=True, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "# ---------------- Main Process ----------------\n",
        "def main():\n",
        "    all_pages = []\n",
        "    prev_loc = None\n",
        "    prev_locnum = None\n",
        "    prev_cuts = None\n",
        "\n",
        "    with pdfplumber.open(PDF) as pdf:\n",
        "        # MODIFIED: Changed page limit to 120\n",
        "        last = min(120, len(pdf.pages))\n",
        "\n",
        "        # This fallback for header y-position is based on page 2 (index 1).\n",
        "        # This is safe because page 2 is not in the skip list.\n",
        "        words2 = chars_to_words(pdf.pages[1])\n",
        "        header2_y = header_y_of(words2)\n",
        "\n",
        "        # MODIFIED: Define pages to skip\n",
        "        # Note: The loop starts from page 2, which already skips page 1.\n",
        "        skip_pages = {1, 35, 73, 115}\n",
        "\n",
        "        for page_num in range(1, last + 1):\n",
        "            # MODIFIED: Skip specified pages\n",
        "            if page_num in skip_pages:\n",
        "                print(f\"[p{page_num}] SKIPPED.\")\n",
        "                continue\n",
        "\n",
        "            page = pdf.pages[page_num - 1]\n",
        "            words = chars_to_words(page)\n",
        "\n",
        "            # 1) Infer cut lines from header; fallback to default cuts if not found\n",
        "            cuts, hy = cuts_from_header(words, BASE_CUTS, prev_cuts=prev_cuts)\n",
        "            hy = hy or header2_y\n",
        "\n",
        "            # 2) Type-aware refinement\n",
        "            cuts = refine_cuts_typeaware(page, cuts, hy)\n",
        "\n",
        "            # 3) Extract row data\n",
        "            dfp = rows_from_page(page, cuts, hy)\n",
        "            if dfp.empty:\n",
        "                print(f\"[p{page_num}] rows=0\")\n",
        "                continue\n",
        "\n",
        "            # 4) Repair columns, split month and amount\n",
        "            dfp = repair_loc_and_location(dfp)\n",
        "            dfp = split_and_swap_month_revenue(dfp)\n",
        "            dfp = normalize_months(dfp)\n",
        "\n",
        "            # 5) Forward fill across pages\n",
        "            dfp = pagewise_seeded_ffill(dfp, prev_locnum, prev_loc)\n",
        "\n",
        "            # 6) Data cleaning and transformation\n",
        "            dfp = finalize(dfp)\n",
        "\n",
        "            # 7) Left-pack numeric values\n",
        "            dfp = monthly_numeric_left_pack(dfp)\n",
        "\n",
        "            # 8) Update state and cuts from the previous page\n",
        "            if dfp[\"Location\"].notna().any():\n",
        "                prev_loc = dfp[\"Location\"].dropna().iloc[-1]\n",
        "            if dfp[\"Loc #\"].notna().any():\n",
        "                prev_locnum = dfp[\"Loc #\"].dropna().iloc[-1]\n",
        "            prev_cuts = cuts\n",
        "\n",
        "            dfp.insert(0, \"Page\", page_num)\n",
        "            all_pages.append(dfp)\n",
        "            print(f\"[p{page_num}] rows={len(dfp)}\")\n",
        "\n",
        "    if all_pages:\n",
        "        out = pd.concat(all_pages, ignore_index=True)\n",
        "        # Fill missing annual totals\n",
        "        out = fill_missing_annual(out)\n",
        "        out.to_csv(OUT, index=False)\n",
        "        print(f\"✅ Done. rows={len(out)} → {OUT}\")\n",
        "    else:\n",
        "        print(\"⚠️ No valid data extracted.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU14ufBTZ0AB",
        "outputId": "61a895d9-6c94-4a8f-9f7d-ad9bda9f1e96"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[p1] SKIPPED.\n",
            "[p2] rows=14\n",
            "[p3] rows=38\n",
            "[p4] rows=20\n",
            "[p5] rows=41\n",
            "[p6] rows=43\n",
            "[p7] rows=29\n",
            "[p8] rows=41\n",
            "[p9] rows=43\n",
            "[p10] rows=8\n",
            "[p11] rows=40\n",
            "[p12] rows=42\n",
            "[p13] rows=47\n",
            "[p14] rows=39\n",
            "[p15] rows=44\n",
            "[p16] rows=41\n",
            "[p17] rows=44\n",
            "[p18] rows=41\n",
            "[p19] rows=44\n",
            "[p20] rows=38\n",
            "[p21] rows=40\n",
            "[p22] rows=44\n",
            "[p23] rows=33\n",
            "[p24] rows=39\n",
            "[p25] rows=4\n",
            "[p26] rows=26\n",
            "[p27] rows=41\n",
            "[p28] rows=42\n",
            "[p29] rows=39\n",
            "[p30] rows=17\n",
            "[p31] rows=39\n",
            "[p32] rows=38\n",
            "[p33] rows=36\n",
            "[p34] rows=10\n",
            "[p35] SKIPPED.\n",
            "[p36] rows=14\n",
            "[p37] rows=40\n",
            "[p38] rows=30\n",
            "[p39] rows=36\n",
            "[p40] rows=38\n",
            "[p41] rows=37\n",
            "[p42] rows=15\n",
            "[p43] rows=44\n",
            "[p44] rows=47\n",
            "[p45] rows=21\n",
            "[p46] rows=38\n",
            "[p47] rows=42\n",
            "[p48] rows=41\n",
            "[p49] rows=44\n",
            "[p50] rows=25\n",
            "[p51] rows=44\n",
            "[p52] rows=43\n",
            "[p53] rows=42\n",
            "[p54] rows=44\n",
            "[p55] rows=40\n",
            "[p56] rows=44\n",
            "[p57] rows=40\n",
            "[p58] rows=40\n",
            "[p59] rows=44\n",
            "[p60] rows=32\n",
            "[p61] rows=41\n",
            "[p62] rows=16\n",
            "[p63] rows=26\n",
            "[p64] rows=40\n",
            "[p65] rows=44\n",
            "[p66] rows=11\n",
            "[p67] rows=41\n",
            "[p68] rows=31\n",
            "[p69] rows=41\n",
            "[p70] rows=38\n",
            "[p71] rows=40\n",
            "[p72] rows=13\n",
            "[p73] SKIPPED.\n",
            "[p74] rows=14\n",
            "[p75] rows=38\n",
            "[p76] rows=35\n",
            "[p77] rows=8\n",
            "[p78] rows=36\n",
            "[p79] rows=38\n",
            "[p80] rows=37\n",
            "[p81] rows=27\n",
            "[p82] rows=41\n",
            "[p83] rows=44\n",
            "[p84] rows=30\n",
            "[p85] rows=22\n",
            "[p86] rows=41\n",
            "[p87] rows=45\n",
            "[p88] rows=44\n",
            "[p89] rows=47\n",
            "[p90] rows=38\n",
            "[p91] rows=44\n",
            "[p92] rows=43\n",
            "[p93] rows=42\n",
            "[p94] rows=44\n",
            "[p95] rows=40\n",
            "[p96] rows=44\n",
            "[p97] rows=42\n",
            "[p98] rows=42\n",
            "[p99] rows=40\n",
            "[p100] rows=43\n",
            "[p101] rows=33\n",
            "[p102] rows=42\n",
            "[p103] rows=32\n",
            "[p104] rows=26\n",
            "[p105] rows=40\n",
            "[p106] rows=44\n",
            "[p107] rows=23\n",
            "[p108] rows=39\n",
            "[p109] rows=29\n",
            "[p110] rows=39\n",
            "[p111] rows=38\n",
            "[p112] rows=37\n",
            "[p113] rows=36\n",
            "[p114] rows=11\n",
            "[p115] SKIPPED.\n",
            "[p116] rows=14\n",
            "[p117] rows=39\n",
            "[p118] rows=36\n",
            "[p119] rows=19\n",
            "[p120] rows=40\n",
            "✅ Done. rows=4078 → /content/Marine_Revenue_FY20-FY24__p1to120_skipped_stable.csv\n"
          ]
        }
      ]
    }
  ]
}