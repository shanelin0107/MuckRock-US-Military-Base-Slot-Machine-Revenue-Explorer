{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "21313c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dawid\\AppData\\Local\\Temp\\ipykernel_11948\\1546939692.py:288: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  installed_df = pd.concat(installed_frames, ignore_index=True) if installed_frames else pd.DataFrame()\n",
      "C:\\Users\\dawid\\AppData\\Local\\Temp\\ipykernel_11948\\1546939692.py:826: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  .applymap(lambda v: pd.NA if isinstance(v, str) and not v.strip() else v))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel workbook written to: C:\\Users\\dawid\\Desktop\\MUCKROCK\\database\\FY2021_CSVs\\FY2021_Asset_Report.xlsx\n",
      "Extraction complete.\n",
      "Region summary rows: 68\n",
      "Field office rows: 424\n",
      "Installed assets rows: 1066\n",
      "Asset details rows: 4383\n",
      "Floor asset details rows: 22940\n",
      "Site operational rows: 925\n",
      "Years in storage rows: 182\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "extract multiple tables from the FY2021 Asset Report PDF.\n",
    "\n",
    "This script brings together parsers for the following tables and writes the\n",
    "results to CSV files:\n",
    "\n",
    "* Assets by Region, Service (region summary)\n",
    "* Assets by Field Office\n",
    "* Installed assets by location & manufacture\n",
    "* Asset details (blue header)\n",
    "* Floor asset details (white 'Place' floor)\n",
    "* Site operational status\n",
    "* Years in storage pivot (summary of asset details by age and years in storage)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import subprocess\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Parsers for the region summary and field office tables\n",
    "###############################################################################\n",
    "\n",
    "def parse_region_and_field_office(lines: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Extract the \"Assets by Region, Service\" summary and the \"Assets by Field Office\"\n",
    "    table from a list of lines representing a single page.  Each table appears\n",
    "    once per month.  The region summary table begins after a header line\n",
    "    containing \"Slots Only\" and continues until the next heading or blank line.\n",
    "    The field office table begins after a header containing \"Assets by Field Office\"\n",
    "    and continues until the end of that section.  Both tables are returned as\n",
    "    pandas DataFrames (which may be empty if no rows were detected).\n",
    "    \"\"\"\n",
    "    region_df = pd.DataFrame()\n",
    "    field_df = pd.DataFrame()\n",
    "\n",
    "    # ----- Region summary -----\n",
    "    # Locate the index of the region table by searching for 'Slots Only'\n",
    "    idx_slots = next((i for i, l in enumerate(lines) if l.strip().startswith('Slots Only')), None)\n",
    "    if idx_slots is not None:\n",
    "        region_rows: List[List] = []\n",
    "        i = idx_slots + 1\n",
    "        while i < len(lines):\n",
    "            line = lines[i].strip()\n",
    "            if not line:\n",
    "                i += 1\n",
    "                continue\n",
    "            # Stop at the start of another table or explanatory note\n",
    "            if (\n",
    "                line.startswith('Locations by Service') or\n",
    "                'Assets by Field Office' in line or\n",
    "                'EGMs by Field Office' in line or\n",
    "                'Installed Assets by Location' in line\n",
    "            ):\n",
    "                break\n",
    "            parts = [p for p in re.split(r'\\s{2,}', line) if p]\n",
    "            # A valid row has at least five tokens (region name plus at least four numbers)\n",
    "            if len(parts) >= 5:\n",
    "                region_name = parts[0]\n",
    "                values: List = []\n",
    "                for p in parts[1:]:\n",
    "                    x = p.replace('%', '')\n",
    "                    if x in ('-', ''):\n",
    "                        values.append(np.nan)\n",
    "                    else:\n",
    "                        try:\n",
    "                            values.append(float(x.replace(',', '')))\n",
    "                        except Exception:\n",
    "                            # Keep string for non‐numeric values (should not occur)\n",
    "                            values.append(x)\n",
    "                region_rows.append([region_name] + values)\n",
    "            i += 1\n",
    "        if region_rows:\n",
    "            # Normalize row lengths by padding with NaN\n",
    "            maxlen = max(len(r) for r in region_rows)\n",
    "            for r in region_rows:\n",
    "                while len(r) < maxlen:\n",
    "                    r.append(np.nan)\n",
    "            # Assign column names according to the maximum length detected\n",
    "            cols = ['Region', '#Locations', 'Army', 'Navy', 'Marine_Corps', 'Airforce', 'Total', 'Percent']\n",
    "            region_df = pd.DataFrame(region_rows, columns=cols[:maxlen])\n",
    "\n",
    "    # ----- Field office summary -----\n",
    "    field_rows: List[List] = []\n",
    "    start = False\n",
    "    current_region: Optional[str] = None\n",
    "    for raw in lines:\n",
    "        line = raw.rstrip(' ')\n",
    "        # Identify the start of the field office section\n",
    "        if 'Assets by Field Office' in line or 'EGMs by Field Office' in line:\n",
    "            start = True\n",
    "            continue\n",
    "        if not start:\n",
    "            continue\n",
    "        # Skip blank lines and header lines containing 'Slots'\n",
    "        if not line.strip() or line.strip().startswith('Slots'):\n",
    "            continue\n",
    "        # A line without any digits denotes the region name header\n",
    "        if line.strip() and not re.search(r'\\d', line.strip()):\n",
    "            current_region = line.strip()\n",
    "            continue\n",
    "        # Split into parts separated by two or more spaces\n",
    "        parts = [p for p in re.split(r'\\s{2,}', line.strip()) if p]\n",
    "        # Data rows end with five numeric tokens (Slots, ACM, ITC, FRS, Total)\n",
    "        if len(parts) >= 6 and all(re.match(r'^[()0-9,.-]+$', p) for p in parts[-5:]):\n",
    "            fo_number = parts[0]\n",
    "            location_name = ' '.join(parts[1:-5])\n",
    "            values: List[float] = []\n",
    "            for p in parts[-5:]:\n",
    "                if p == '-':\n",
    "                    values.append(np.nan)\n",
    "                else:\n",
    "                    v = p\n",
    "                    # Handle negative numbers enclosed in parentheses\n",
    "                    if v.startswith('(') and v.endswith(')'):\n",
    "                        v = '-' + v[1:-1]\n",
    "                    values.append(float(v.replace(',', '')))\n",
    "            field_rows.append([current_region, fo_number, location_name] + values)\n",
    "    if field_rows:\n",
    "        field_df = pd.DataFrame(field_rows, columns=[\n",
    "            'Region', 'FO#', 'Location', 'Slots', 'ACM_CountR', 'ITC', 'FRS', 'Total'\n",
    "        ])\n",
    "\n",
    "    return region_df, field_df\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Parser for installed assets\n",
    "###############################################################################\n",
    "\n",
    "def parse_installed_assets(lines: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parse the 'Installed Assets by Location, Manufacture' table from a list of lines.\n",
    "    Each row begins with a location name and may optionally include a field office\n",
    "    number.  Seven manufacturer columns (NOV, AIN, IGT, WMS, BAL, KON, ITE)\n",
    "    are followed by Tot_EGMs, FRS, ACM, ITC, Total_PDF, and a computed total.\n",
    "    \"\"\"\n",
    "    rows: List[List] = []\n",
    "    started = False\n",
    "    for line in lines:\n",
    "        if not started:\n",
    "            # Identify the beginning of the installed assets table by finding the\n",
    "            # section title or a header line containing 'FO #' and manufacturer names.\n",
    "            if ('Installed Assets by Location' in line) or (('FO #' in line) and ('IGT' in line)):\n",
    "                started = True\n",
    "            continue\n",
    "        # Skip lines without any digits (likely blank or notes)\n",
    "        if not line.strip() or not re.search(r'\\d', line):\n",
    "            continue\n",
    "        # Skip repeated header lines that contain manufacturer names or Tot/EGMs\n",
    "        if ('Tot/EGMs' in line) or (('NOV' in line) and ('AIN' in line)):\n",
    "            continue\n",
    "        parts = [p.strip() for p in re.split(r'\\s{2,}', line) if p.strip()]\n",
    "        if len(parts) < 5:\n",
    "            continue\n",
    "        name = parts[0]\n",
    "        idx = 1\n",
    "        fo_number: Optional[str] = None\n",
    "        # If the next token is all digits, treat it as the field office number\n",
    "        if idx < len(parts) and re.match(r'^\\d+$', parts[idx]):\n",
    "            fo_number = parts[idx]\n",
    "            idx += 1\n",
    "        loc = parts[idx] if idx < len(parts) else ''\n",
    "        idx += 1\n",
    "        svc = parts[idx] if idx < len(parts) else ''\n",
    "        idx += 1\n",
    "        # If the service column does not contain letters, skip this row\n",
    "        if not re.search(r'[A-Za-z]', svc):\n",
    "            continue\n",
    "        # Flatten the remaining tokens so that numbers separated by spaces are captured individually\n",
    "        metric_tokens: List[str] = []\n",
    "        for token in parts[idx:]:\n",
    "            metric_tokens += token.split()\n",
    "        if len(metric_tokens) < 8:\n",
    "            continue\n",
    "        # The first seven numeric values correspond to manufacturers\n",
    "        manuf_vals: List[Optional[float]] = []\n",
    "        for t in metric_tokens[:7]:\n",
    "            if t == '-':\n",
    "                manuf_vals.append(None)\n",
    "            else:\n",
    "                try:\n",
    "                    manuf_vals.append(float(t.replace(',', '')))\n",
    "                except Exception:\n",
    "                    manuf_vals.append(None)\n",
    "        # Helper for optional numeric fields\n",
    "        def _num(tok: Optional[str]) -> Optional[float]:\n",
    "            if tok is None or tok in ('-', ''):\n",
    "                return None\n",
    "            try:\n",
    "                return float(tok.replace(',', ''))\n",
    "            except Exception:\n",
    "                return None\n",
    "        tot_egms = _num(metric_tokens[7] if len(metric_tokens) > 7 else None)\n",
    "        frs = _num(metric_tokens[8] if len(metric_tokens) > 8 else None)\n",
    "        acm = _num(metric_tokens[9] if len(metric_tokens) > 9 else None)\n",
    "        # Remaining tokens may contain ITC and Total_PDF; the first number in\n",
    "        # the tail is treated as ITC, and the last token as Total_PDF\n",
    "        itc: Optional[float] = None\n",
    "        total_pdf: Optional[float] = None\n",
    "        remaining = metric_tokens[10:]\n",
    "        if remaining:\n",
    "            total_pdf = _num(remaining[-1])\n",
    "            for t in remaining[:-1]:\n",
    "                cand = _num(t)\n",
    "                if cand is not None:\n",
    "                    itc = cand\n",
    "                    break\n",
    "        total_computed = sum(x for x in [tot_egms, frs, acm, itc] if x is not None)\n",
    "        rows.append([\n",
    "            name, fo_number, loc, svc\n",
    "        ] + manuf_vals + [\n",
    "            tot_egms, frs, acm, itc, total_pdf, total_computed\n",
    "        ])\n",
    "    if rows:\n",
    "        columns = [\n",
    "            'LocationName', 'FO#', 'Loc', 'Svc',\n",
    "            'NOV', 'AIN', 'IGT', 'WMS', 'BAL', 'KON', 'ITE',\n",
    "            'Tot_EGMs', 'FRS', 'ACM', 'ITC', 'Total_PDF', 'Total_Computed'\n",
    "        ]\n",
    "        return pd.DataFrame(rows, columns=columns)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Helper functions to detect months and build month maps\n",
    "###############################################################################\n",
    "\n",
    "def get_month(page_text: str) -> Optional[str]:\n",
    "    \"\"\"Extract a month label (e.g., 'March 2021') from a page's text.\"\"\"\n",
    "    m = re.search(r'for month of\\s+([A-Za-z]+)\\s+(\\d{4})', page_text, re.IGNORECASE)\n",
    "    return f\"{m.group(1).capitalize()} {m.group(2)}\" if m else None\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Extraction for region/field/installed tables\n",
    "###############################################################################\n",
    "\n",
    "def extract_region_field_installed(pdf_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Convert the entire PDF to text and extract the region summaries, field office\n",
    "    tables and installed asset tables.  A context tracker maintains the current\n",
    "    month across pages; pages that do not contain a new month heading inherit\n",
    "    the month from the most recently encountered heading.\n",
    "    \"\"\"\n",
    "    # Convert all pages to text in a single call to pdftotext\n",
    "    text = subprocess.check_output(['pdftotext', '-layout', pdf_path, '-'], text=True)\n",
    "    # Split pages on the form feed character\n",
    "    pages = text.split('\\f')\n",
    "    region_frames: List[pd.DataFrame] = []\n",
    "    field_frames: List[pd.DataFrame] = []\n",
    "    installed_frames: List[pd.DataFrame] = []\n",
    "    current_month: Optional[str] = None\n",
    "    for page_text in pages:\n",
    "        if not page_text.strip():\n",
    "            continue\n",
    "        month = get_month(page_text)\n",
    "        if month:\n",
    "            current_month = month\n",
    "        # Split lines on newline\n",
    "        lines = page_text.split('\\n')\n",
    "        page_upper = page_text.upper()\n",
    "        has_region = ('ASSETS BY REGION' in page_upper) or ('EGMS BY REGION' in page_upper)\n",
    "        has_field = ('ASSETS BY FIELD OFFICE' in page_upper) or ('EGMS BY FIELD OFFICE' in page_upper)\n",
    "        has_inst = ('INSTALLED ASSETS BY LOCATION' in page_upper)\n",
    "        if has_region or has_field:\n",
    "            rdf, fdf = parse_region_and_field_office(lines)\n",
    "            if not rdf.empty:\n",
    "                rdf['Month'] = current_month\n",
    "                region_frames.append(rdf)\n",
    "            if not fdf.empty:\n",
    "                fdf['Month'] = current_month\n",
    "                field_frames.append(fdf)\n",
    "        if has_inst:\n",
    "            inst_df = parse_installed_assets(lines)\n",
    "            if not inst_df.empty:\n",
    "                inst_df['Month'] = current_month\n",
    "                installed_frames.append(inst_df)\n",
    "    region_df = pd.concat(region_frames, ignore_index=True) if region_frames else pd.DataFrame()\n",
    "    field_df = pd.concat(field_frames, ignore_index=True) if field_frames else pd.DataFrame()\n",
    "    installed_df = pd.concat(installed_frames, ignore_index=True) if installed_frames else pd.DataFrame()\n",
    "    return region_df, field_df, installed_df\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Detailed table parsers (asset details, floor details, site status)\n",
    "###############################################################################\n",
    "\n",
    "def detect_month_map(pages: List[str]) -> Dict[int, str]:\n",
    "    \"\"\"\n",
    "    Build a mapping from page numbers (1-indexed) to month strings by scanning\n",
    "    pages for the 'Assets by Region, Service for month of <Month Year>' header.\n",
    "    Returns a dictionary mapping each page index to its corresponding month.\n",
    "    \"\"\"\n",
    "    pattern = re.compile(r'Assets by Region, Service\\s+for month of\\s+([A-Za-z]+\\s+\\d{4})', re.I)\n",
    "    month_start_pages: List[Tuple[int, str]] = []\n",
    "    seen_months: List[str] = []\n",
    "    for i, page in enumerate(pages, start=1):\n",
    "        m = pattern.search(page)\n",
    "        if m:\n",
    "            month = m.group(1).strip()\n",
    "            if month not in seen_months:\n",
    "                month_start_pages.append((i, month))\n",
    "                seen_months.append(month)\n",
    "    # Now assign ranges between month start pages\n",
    "    month_ranges: List[Tuple[int, int, str]] = []\n",
    "    for idx, (start, month) in enumerate(month_start_pages):\n",
    "        end = month_start_pages[idx + 1][0] - 1 if idx + 1 < len(month_start_pages) else len(pages)\n",
    "        month_ranges.append((start, end, month))\n",
    "    month_map: Dict[int, str] = {}\n",
    "    for start, end, month in month_ranges:\n",
    "        for p in range(start, end + 1):\n",
    "            month_map[p] = month\n",
    "    return month_map\n",
    "\n",
    "\n",
    "def parse_asset_line(line: str) -> Optional[Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Parse a single line from the asset details table.  Each row contains\n",
    "    region, FONUM, FOSHORT, location number, location name, asset number,\n",
    "    class code, description, type, acquisition date, effective date, serial\n",
    "    number, age, and years in storage.  Returns a dictionary of values or\n",
    "    None if the line does not match the expected pattern.\n",
    "    \"\"\"\n",
    "    tokens = line.split()\n",
    "    if len(tokens) < 10:\n",
    "        return None\n",
    "    # Extract trailing numeric/date tokens\n",
    "    try:\n",
    "        years = tokens[-1]\n",
    "        age = tokens[-2]\n",
    "        serial = tokens[-3]\n",
    "        eff_date = tokens[-4]\n",
    "        acquire = tokens[-5]\n",
    "        _type = tokens[-6]\n",
    "    except IndexError:\n",
    "        return None\n",
    "    remaining = tokens[:-6]\n",
    "    if len(remaining) < 5:\n",
    "        return None\n",
    "    region = remaining[0]\n",
    "    fonum = remaining[1]\n",
    "    # Find the index of the location (first purely numeric token after FOSHORT)\n",
    "    loc_idx: Optional[int] = None\n",
    "    for i in range(2, len(remaining)):\n",
    "        if remaining[i].isdigit():\n",
    "            loc_idx = i\n",
    "            break\n",
    "    if loc_idx is None:\n",
    "        return None\n",
    "    foshort = ' '.join(remaining[2:loc_idx])\n",
    "    loc = remaining[loc_idx]\n",
    "    # Identify the asset number (a 4–6 digit number)\n",
    "    asset_idx: Optional[int] = None\n",
    "    for j in range(loc_idx + 1, len(remaining)):\n",
    "        if re.fullmatch(r'\\d{4,6}', remaining[j]):\n",
    "            asset_idx = j\n",
    "            break\n",
    "    if asset_idx is None or asset_idx + 1 >= len(remaining):\n",
    "        return None\n",
    "    lname = ' '.join(remaining[loc_idx + 1:asset_idx])\n",
    "    asset = remaining[asset_idx]\n",
    "    class_num = remaining[asset_idx + 1]\n",
    "    desc = ' '.join(remaining[asset_idx + 2:])\n",
    "    return {\n",
    "        'Region': region,\n",
    "        'FONUM': fonum,\n",
    "        'FOSHORT': foshort,\n",
    "        'Loc': loc,\n",
    "        'LNAME': lname,\n",
    "        'Asset': asset,\n",
    "        'Class': class_num,\n",
    "        'Desc': desc,\n",
    "        'Type': _type,\n",
    "        'Acquire': acquire,\n",
    "        'Effective': eff_date,\n",
    "        'SerialNum': serial,\n",
    "        'Age': age,\n",
    "        'Years_in_Storage': years,\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_asset_details_page(page_text: str) -> List[Dict[str, str]]:\n",
    "    data: List[Dict[str, str]] = []\n",
    "    # Iterate through lines and parse those beginning with region names\n",
    "    for line in page_text.split('\\n'):\n",
    "        line_strip = line.strip()\n",
    "        if not line_strip:\n",
    "            continue\n",
    "        if line_strip.startswith(('Europe', 'Korea', 'Japan', 'Okinawa', 'Pacific', 'United')):\n",
    "            parsed = parse_asset_line(line_strip)\n",
    "            if parsed:\n",
    "                data.append(parsed)\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_floor_line(line: str) -> Optional[Dict[str, str]]:\n",
    "    tokens = line.split()\n",
    "    if len(tokens) < 15:\n",
    "        return None\n",
    "    try:\n",
    "        age = tokens[-1]\n",
    "        year = tokens[-2]\n",
    "        cat = tokens[-3]\n",
    "        # Determine FONUM and FOSHORT by scanning tokens from the right\n",
    "        idx = len(tokens) - 3\n",
    "        foshort_tokens: List[str] = []\n",
    "        fonum: Optional[str] = None\n",
    "        while idx > 0:\n",
    "            token = tokens[idx - 1]\n",
    "            if token.isdigit():\n",
    "                fonum = token\n",
    "                idx -= 1\n",
    "                break\n",
    "            else:\n",
    "                foshort_tokens.insert(0, token)\n",
    "                idx -= 1\n",
    "        if fonum is None:\n",
    "            return None\n",
    "        foshort = ' '.join(foshort_tokens)\n",
    "        loc = tokens[0]\n",
    "        place = tokens[1]\n",
    "        region = tokens[2]\n",
    "        svc = tokens[3]\n",
    "        asset = tokens[4]\n",
    "        serial = tokens[5]\n",
    "        asset_type = tokens[6]\n",
    "        # Find the first date which marks the end of the description\n",
    "        date_pattern = re.compile(r'\\d{1,2}/\\d{1,2}/\\d{2,4}')\n",
    "        idx_desc_end: Optional[int] = None\n",
    "        for i in range(7, len(tokens)):\n",
    "            if date_pattern.fullmatch(tokens[i]):\n",
    "                idx_desc_end = i\n",
    "                break\n",
    "        if idx_desc_end is None or idx_desc_end + 6 >= len(tokens):\n",
    "            return None\n",
    "        desc = ' '.join(tokens[7:idx_desc_end])\n",
    "        acquire = tokens[idx_desc_end]\n",
    "        effective = tokens[idx_desc_end + 1]\n",
    "        disposed = tokens[idx_desc_end + 2]\n",
    "        class_num = tokens[idx_desc_end + 3]\n",
    "        mfg = tokens[idx_desc_end + 4]\n",
    "        # LNAME spans from after the MFG up to the location of FONUM\n",
    "        fonum_idx = tokens.index(fonum)\n",
    "        lname = ' '.join(tokens[idx_desc_end + 5:fonum_idx])\n",
    "    except Exception:\n",
    "        return None\n",
    "    return {\n",
    "        'Loc': loc,\n",
    "        'Place': place,\n",
    "        'Region': region,\n",
    "        'SVC': svc,\n",
    "        'Asset': asset,\n",
    "        'SerialNum': serial,\n",
    "        'Type': asset_type,\n",
    "        'Desc': desc,\n",
    "        'Acquire': acquire,\n",
    "        'Effective': effective,\n",
    "        'Disposed': disposed,\n",
    "        'Class': class_num,\n",
    "        'MFG': mfg,\n",
    "        'LNAME': lname,\n",
    "        'FONUM': fonum,\n",
    "        'FOSHORT': foshort,\n",
    "        'Cat': cat,\n",
    "        'Year': year,\n",
    "        'Age': age,\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_floor_details_page(page_text: str) -> List[Dict[str, str]]:\n",
    "    data: List[Dict[str, str]] = []\n",
    "    for line in page_text.split('\\n'):\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        # Rows start with a location number and include the literal 'Floor' as the second field\n",
    "        if re.match(r'\\s*\\d', line) and 'Floor' in line:\n",
    "            parsed = parse_floor_line(line.strip())\n",
    "            if parsed:\n",
    "                data.append(parsed)\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_site_line(row: str) -> Optional[Dict[str, str]]:\n",
    "    tokens = row.split()\n",
    "    if not tokens or not tokens[0].isdigit():\n",
    "        return None\n",
    "    # Find where the open date appears to delimit LNAME and Place\n",
    "    date_pattern = re.compile(r'\\d{1,2}/\\d{1,2}/\\d{2,4}')\n",
    "    open_idx: Optional[int] = None\n",
    "    for i in range(1, len(tokens)):\n",
    "        if date_pattern.fullmatch(tokens[i]):\n",
    "            open_idx = i\n",
    "            break\n",
    "    if open_idx is None or open_idx < 2:\n",
    "        return None\n",
    "    place = tokens[open_idx - 1]\n",
    "    lname = ' '.join(tokens[1:open_idx - 1])\n",
    "    try:\n",
    "        open_date = tokens[open_idx]\n",
    "        closed_date = tokens[open_idx + 1]\n",
    "        ksi = tokens[open_idx + 2]\n",
    "        cmty_num = tokens[open_idx + 3]\n",
    "    except IndexError:\n",
    "        return None\n",
    "    # Identify where the service field appears (Army, Navy, Air, Marine Corps)\n",
    "    services = {'Army', 'Navy', 'Air', 'Marine', 'Corps'}\n",
    "    svc_idx: Optional[int] = None\n",
    "    for j in range(open_idx + 4, len(tokens)):\n",
    "        tok = tokens[j]\n",
    "        if tok == 'Marine' and j + 1 < len(tokens) and tokens[j + 1] == 'Corps':\n",
    "            svc_idx = j\n",
    "            break\n",
    "        if tok in services:\n",
    "            svc_idx = j\n",
    "            break\n",
    "    if svc_idx is None:\n",
    "        return None\n",
    "    cmty = ' '.join(tokens[open_idx + 4:svc_idx])\n",
    "    if tokens[svc_idx] == 'Marine' and svc_idx + 1 < len(tokens) and tokens[svc_idx + 1] == 'Corps':\n",
    "        svc = 'Marine Corps'\n",
    "        fonum_index = svc_idx + 2\n",
    "    else:\n",
    "        svc = tokens[svc_idx]\n",
    "        fonum_index = svc_idx + 1\n",
    "    if fonum_index >= len(tokens):\n",
    "        return None\n",
    "    fonum = tokens[fonum_index]\n",
    "    foshort = ' '.join(tokens[fonum_index + 1:]) if fonum_index + 1 < len(tokens) else ''\n",
    "    return {\n",
    "        'Loc': tokens[0],\n",
    "        'LNAME': lname,\n",
    "        'Place': place,\n",
    "        'Open': open_date,\n",
    "        'Closed': closed_date,\n",
    "        'KSI': ksi,\n",
    "        'CmtyNum': cmty_num,\n",
    "        'Cmty': cmty,\n",
    "        'SVC': svc,\n",
    "        'FONUM': fonum,\n",
    "        'FOSHORT': foshort,\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_site_status_page(page_text: str) -> List[Dict[str, str]]:\n",
    "    # Combine lines into rows: a row begins with a number and continues until the next numeric line\n",
    "    lines = page_text.split('\\n')\n",
    "    rows: List[str] = []\n",
    "    current: str = ''\n",
    "    for line in lines:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        if re.match(r'\\s*\\d', line):\n",
    "            if current:\n",
    "                rows.append(current)\n",
    "                current = ''\n",
    "            current = line.strip()\n",
    "        else:\n",
    "            if current:\n",
    "                current += ' ' + line.strip()\n",
    "    if current:\n",
    "        rows.append(current)\n",
    "    data: List[Dict[str, str]] = []\n",
    "    for row in rows:\n",
    "        parsed = parse_site_line(row)\n",
    "        if parsed:\n",
    "            data.append(parsed)\n",
    "    return data\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Extraction for detailed tables (asset, floor, site) and years in storage\n",
    "###############################################################################\n",
    "\n",
    "_REGION_WORDS = r\"(Europe|Korea|Japan|Okinawa|Pacific|United)\"\n",
    "\n",
    "# Service can be ANY single letter + digit (E0/K0/J0…)\n",
    "_FOSHORT_HEAD = re.compile(\n",
    "    rf\"\"\"\n",
    "    ^\\s*\n",
    "    (?P<city>[A-Z0-9'&./\\- ]+?)\\s+     # KAISERSLAUTERN / STUTTGART / UIJEONGBU ...\n",
    "    (?P<locnum>\\d+)\\s+                 # 2 / 4 / 5 ...\n",
    "    (?P<Region>{_REGION_WORDS})\\s+     # Region\n",
    "    (?P<regcode>[A-Z]{{2}}\\d{{2}})\\s+  # KA02 / ST03 / UJ01 ...\n",
    "    (?P<service>[A-Z]\\d)\\s+            # E0 / K0 / J0\n",
    "    (?P<after>.+?)\\s*$                 # remainder\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE\n",
    ")\n",
    "\n",
    "UNPARSED_FOSHORT = []\n",
    "_RPTGRP_CODE = re.compile(r\"^[A-Z]/C$\", re.IGNORECASE)     # B/C, C/C, E/C...\n",
    "_BOOL_RX     = re.compile(r\"^(?:TRUE|FALSE)$\", re.IGNORECASE)\n",
    "_OPEN_RX     = re.compile(r\"^(?:OPEN|CLOSED|~CLOSED)$\", re.IGNORECASE)\n",
    "\n",
    "def _fallback_head_tokens(s: str):\n",
    "    \"\"\"\n",
    "    Fallback when _FOSHORT_HEAD doesn't match.\n",
    "    We look for: ... <locnum> <Region> <regcode> <service> <after...>\n",
    "    and derive <city> from the tokens before <locnum>.\n",
    "    \"\"\"\n",
    "    toks = s.split()\n",
    "    if len(toks) < 5:\n",
    "        return None\n",
    "    REGIONS = {\"EUROPE\",\"KOREA\",\"JAPAN\",\"OKINAWA\",\"PACIFIC\",\"UNITED\"}\n",
    "    ridxs = [i for i,t in enumerate(toks) if t.upper() in REGIONS]\n",
    "    if not ridxs:\n",
    "        return None\n",
    "    for ri in ridxs:\n",
    "        if ri == 0 or ri+3 >= len(toks):\n",
    "            continue\n",
    "        if not re.fullmatch(r\"\\d+\", toks[ri-1]):\n",
    "            continue\n",
    "        locnum  = toks[ri-1]\n",
    "        regcode = toks[ri+1] if ri+1 < len(toks) else None\n",
    "        # accept e.g. KA02 or a mild OCR like KAO2 (we'll normalize O→0 between digits)\n",
    "        if not regcode or not re.fullmatch(r\"[A-Za-z]{2}[0-9O][0-9]\", regcode):\n",
    "            continue\n",
    "        service = toks[ri+2] if ri+2 < len(toks) else None\n",
    "        if not service or not re.fullmatch(r\"[A-Za-z]\\d\", service, flags=re.I):\n",
    "            continue\n",
    "        # normalize regcode OCR: letter-letter-(O|0)-digit  -> letter-letter-0-digit\n",
    "        regcode = re.sub(r\"^([A-Za-z]{2})O(\\d)$\", r\"\\g<1>0\\g<2>\", regcode)\n",
    "        city = \" \".join(toks[:ri-1]).strip()\n",
    "        after = \" \".join(toks[ri+3:]).strip()\n",
    "        if city and after:\n",
    "            return {\n",
    "                \"city\": city, \"locnum\": locnum,\n",
    "                \"Region\": toks[ri], \"regcode\": regcode,\n",
    "                \"service\": service, \"after\": after\n",
    "            }\n",
    "    return None\n",
    "\n",
    "def _split_tail(after: str):\n",
    "    \"\"\"\n",
    "    Parses the part of FOSHORT after <Region> <REGCODE> <SERVICE>.\n",
    "    Handles BOTH formats:\n",
    "\n",
    "    OLD (pre–May 2021):\n",
    "      [MESSAGE…] <Banker> <Shortname…> [RptGrpCode?] <Split%> <TVLREST> <COVID19>\n",
    "\n",
    "    NEW (May 2021+):\n",
    "      [MESSAGE…] <Banker> <Shortname…> <Split%> <CMMTY words…>\n",
    "\n",
    "    Returns keys: MESSAGE, banker, shortname, grp_code, Split, TVLREST, COVID19, CMMTY\n",
    "    \"\"\"\n",
    "    # Normalize weird punctuation and compress spaces\n",
    "    after = (after.replace(\"’\",\"'\").replace(\"‘\",\"'\")\n",
    "                  .replace(\"–\",\"-\").replace(\"—\",\"-\"))\n",
    "    # Ensure a space after '%' if it glues to 'Open/Closed'\n",
    "    after = re.sub(r\"%\\s*(Open|Closed|~Closed)\\b\", r\"% \\1\", after, flags=re.I)\n",
    "    # Remove bracket chars; collapse whitespace\n",
    "    after = re.sub(r\"[\\[\\](){}]\", \" \", after)\n",
    "    after = re.sub(r\"\\s+\", \" \", after).strip()\n",
    "\n",
    "    toks = after.split()\n",
    "    if len(toks) < 2:\n",
    "        return None\n",
    "\n",
    "    # last token ending with % is our Split\n",
    "    split_i = None\n",
    "    for i in range(len(toks)-1, -1, -1):\n",
    "        if toks[i].endswith(\"%\"):\n",
    "            split_i = i\n",
    "            break\n",
    "    if split_i is None:\n",
    "        return None\n",
    "\n",
    "    # right side\n",
    "    right = toks[split_i+1:]\n",
    "    covid = None; tvl = None; cmty = None\n",
    "    if right:\n",
    "        if len(right) >= 2 and _OPEN_RX.match(right[0]) and _BOOL_RX.match(right[-1]):\n",
    "            tvl, covid = right[0].title(), right[-1].upper()\n",
    "        else:\n",
    "            cmty = \" \".join(right).strip() or None\n",
    "\n",
    "    # optional RptGrp code to the left of Split\n",
    "    grp_code = None\n",
    "    left_end = split_i\n",
    "    if split_i-1 >= 0 and _RPTGRP_CODE.match(toks[split_i-1]):\n",
    "        grp_code = toks[split_i-1]\n",
    "        left_end = split_i-1\n",
    "\n",
    "    # left side = [MESSAGE … Banker Shortname…]\n",
    "    left = toks[:left_end]\n",
    "    if len(left) < 2:\n",
    "        return None\n",
    "\n",
    "    STOP = {\"Open\", \"Closed\", \"~Closed\", \"Clsd\", \"SMS\"}\n",
    "    MONTHS = {\n",
    "        \"Jan\",\"Feb\",\"Mar\",\"Apr\",\"May\",\"Jun\",\"Jul\",\"Aug\",\"Sep\",\"Sept\",\"Oct\",\"Nov\",\"Dec\",\n",
    "        \"Jan.\",\"Feb.\",\"Mar.\",\"Apr.\",\"Jun.\",\"Jul.\",\"Aug.\",\"Sep.\",\"Oct.\",\"Nov.\",\"Dec.\"\n",
    "    }\n",
    "\n",
    "    def is_title(tok):     # Dave, Reno, Rick…\n",
    "        return bool(re.fullmatch(r\"[A-Z][a-z]+(?:'[A-Za-z]+)?\", tok))\n",
    "    def is_upperish(tok):  # KAZABRA, PATCH, K-16, ACE'S, HILLTOP, G/C …\n",
    "        return bool(re.fullmatch(r\"[A-Z0-9'&/()\\-]+\", tok))\n",
    "\n",
    "    banker_i = None\n",
    "    for i, tok in enumerate(left):\n",
    "        if is_title(tok) and tok not in STOP and tok not in MONTHS:\n",
    "            nxt = left[i+1] if i+1 < len(left) else \"\"\n",
    "            if is_upperish(nxt):\n",
    "                banker_i = i\n",
    "                break\n",
    "    if banker_i is None:\n",
    "        for i in range(len(left)-1, -1, -1):\n",
    "            if re.fullmatch(r\"[A-Za-z']+\", left[i]):\n",
    "                banker_i = i\n",
    "                break\n",
    "    if banker_i is None or banker_i == len(left)-1:\n",
    "        return None\n",
    "\n",
    "    message   = \" \".join(left[:banker_i]).strip() or None\n",
    "    banker    = left[banker_i]\n",
    "    shortname = \" \".join(left[banker_i+1:]).strip() or None\n",
    "\n",
    "    return {\n",
    "        \"MESSAGE\":   message,\n",
    "        \"banker\":    banker,\n",
    "        \"shortname\": shortname,\n",
    "        \"grp_code\":  grp_code,\n",
    "        \"Split\":     toks[split_i].upper(),\n",
    "        \"TVLREST\":   tvl,\n",
    "        \"COVID19\":   covid,\n",
    "        \"CMMTY\":     cmty,\n",
    "    }\n",
    "\n",
    "def _parse_foshort_row(s: str):\n",
    "    out = {\n",
    "        \"city\": None, \"locnum\": None, \"Region\": None,\n",
    "        \"regcode\": None, \"service\": None,\n",
    "        \"MESSAGE\": None, \"banker\": None, \"shortname\": None,\n",
    "        \"Split\": None, \"TVLREST\": None, \"COVID19\": None, \"CMMTY\": None\n",
    "    }\n",
    "    if not isinstance(s, str) or not s.strip():\n",
    "        return out\n",
    "\n",
    "    # CLEANUP (note: we REMOVED the bad \"([A-Z]{2})0(\\d{2}) -> \\1O\\2\" mutation)\n",
    "    s = (s.replace(\"’\",\"'\").replace(\"‘\",\"'\")\n",
    "           .replace(\"–\",\"-\").replace(\"—\",\"-\")\n",
    "           .replace(\"|\",\" \").replace(\"[\",\"\").replace(\"]\",\"\").replace(\";\",\" \")\n",
    "           .strip())\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "\n",
    "    m = _FOSHORT_HEAD.match(s)\n",
    "    if not m:\n",
    "        f = _fallback_head_tokens(s)\n",
    "        if not f:\n",
    "            UNPARSED_FOSHORT.append(s)\n",
    "            return out\n",
    "        city, locnum = f[\"city\"], f[\"locnum\"]\n",
    "        region, regcode, service = f[\"Region\"], f[\"regcode\"], f[\"service\"]\n",
    "        after = f[\"after\"]\n",
    "    else:\n",
    "        city     = m.group(\"city\").strip()\n",
    "        locnum   = m.group(\"locnum\")\n",
    "        region   = m.group(\"Region\")\n",
    "        regcode  = m.group(\"regcode\")\n",
    "        service  = m.group(\"service\")\n",
    "        after    = m.group(\"after\")\n",
    "\n",
    "    out[\"city\"]    = city\n",
    "    out[\"locnum\"]  = locnum\n",
    "    out[\"Region\"]  = region.title()\n",
    "    out[\"regcode\"] = regcode.upper()\n",
    "    out[\"service\"] = service.upper()\n",
    "\n",
    "    tail = _split_tail(after)\n",
    "    if tail:\n",
    "        out.update(tail)\n",
    "    else:\n",
    "        UNPARSED_FOSHORT.append(s)\n",
    "\n",
    "    return out\n",
    "\n",
    "def extract_detailed_tables(pdf_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Extract asset details, floor asset details, site operational status and a\n",
    "    derived years-in-storage pivot from the PDF.  Uses the month map to\n",
    "    assign pages to months, then applies parsers to each page.  Returns four\n",
    "    DataFrames: asset details, floor details, site status, and years storage.\n",
    "    \"\"\"\n",
    "    text = subprocess.check_output(['pdftotext', '-layout', pdf_path, '-']).decode('utf-8')\n",
    "    pages = text.split('\\f')\n",
    "    month_map = detect_month_map(pages)\n",
    "    asset_data: List[Dict[str, str]] = []\n",
    "    floor_data: List[Dict[str, str]] = []\n",
    "    site_data: List[Dict[str, str]] = []\n",
    "    for p_num, page_text in enumerate(pages, start=1):\n",
    "        month = month_map.get(p_num)\n",
    "        if not month:\n",
    "            continue\n",
    "        for rec in parse_asset_details_page(page_text):\n",
    "            rec['Month'] = month\n",
    "            asset_data.append(rec)\n",
    "        for rec in parse_floor_details_page(page_text):\n",
    "            rec['Month'] = month\n",
    "            floor_data.append(rec)\n",
    "        for rec in parse_site_status_page(page_text):\n",
    "            rec['Month'] = month\n",
    "            site_data.append(rec)\n",
    "    asset_df = pd.DataFrame(asset_data).drop_duplicates()\n",
    "    floor_df = pd.DataFrame(floor_data).drop_duplicates()\n",
    "    site_df  = pd.DataFrame(site_data).drop_duplicates()\n",
    "\n",
    "    if not site_df.empty and \"FOSHORT\" in site_df.columns:\n",
    "        # ---- parse FOSHORT into pieces ----\n",
    "        parsed = site_df[\"FOSHORT\"].apply(_parse_foshort_row).apply(pd.Series)\n",
    "\n",
    "        # A flag so you can see which rows parsed successfully\n",
    "        site_df[\"ParseOK\"] = ~parsed.isna().all(axis=1)\n",
    "\n",
    "        # Normalize empties to NA aggressively\n",
    "        site_df = (site_df\n",
    "                .replace(r\"^\\s*$\", pd.NA, regex=True)\n",
    "                .applymap(lambda v: pd.NA if isinstance(v, str) and not v.strip() else v))\n",
    "\n",
    "        # helpers\n",
    "        def _up(v):    return v.upper() if isinstance(v, str) and v.strip() else pd.NA\n",
    "        def _title(v): return v.title() if isinstance(v, str) and v.strip() else pd.NA\n",
    "\n",
    "        # parsed series\n",
    "        p_city   = parsed.get(\"city\"     , pd.Series(dtype=object)).apply(_up)\n",
    "        p_locnum = pd.to_numeric(parsed.get(\"locnum\", pd.Series(dtype=object)), errors=\"coerce\").astype(\"Int64\")\n",
    "        p_region = parsed.get(\"Region\"   , pd.Series(dtype=object))\n",
    "        p_code   = parsed.get(\"regcode\"  , pd.Series(dtype=object)).apply(_up)\n",
    "        p_bank   = parsed.get(\"service\"  , pd.Series(dtype=object)).apply(_up)\n",
    "        p_msg    = parsed.get(\"MESSAGE\"  , pd.Series(dtype=object))\n",
    "        p_banker = parsed.get(\"banker\"   , pd.Series(dtype=object)).apply(_title)\n",
    "        p_short  = parsed.get(\"shortname\", pd.Series(dtype=object))\n",
    "        p_split  = parsed.get(\"Split\"    , pd.Series(dtype=object)).apply(_up)\n",
    "        p_covid  = parsed.get(\"COVID19\"  , pd.Series(dtype=object)).apply(_up)\n",
    "        p_tvl    = parsed.get(\"TVLREST\"  , pd.Series(dtype=object)).apply(lambda v: v.title() if isinstance(v, str) else pd.NA)\n",
    "        p_cmty   = parsed.get(\"CMMTY\"    , pd.Series(dtype=object))\n",
    "\n",
    "        # left-series helper\n",
    "        def _col(df, name, dtype=\"object\"):\n",
    "            if name in df.columns:\n",
    "                return df[name].reindex(df.index)\n",
    "            return pd.Series(pd.NA, index=df.index, dtype=dtype)\n",
    "\n",
    "        # overwrite/merge\n",
    "        site_df[\"FOSHORT\"]    = p_city\n",
    "        site_df[\"REGNUM\"]     = _col(site_df, \"REGNUM\", \"Int64\").combine_first(p_locnum)\n",
    "        site_df[\"Region\"]     = _col(site_df, \"Region\").combine_first(p_region)\n",
    "        site_df[\"SMSSection\"] = _col(site_df, \"SMSSection\").combine_first(p_code)\n",
    "        site_df[\"SMSBank\"]    = _col(site_df, \"SMSBank\").combine_first(p_bank)\n",
    "        site_df[\"MESSAGE\"]    = _col(site_df, \"MESSAGE\").combine_first(p_msg)\n",
    "        site_df[\"RptGrp\"]     = _col(site_df, \"RptGrp\").combine_first(p_banker)\n",
    "        site_df[\"Shortname\"]  = _col(site_df, \"Shortname\").combine_first(p_short)\n",
    "        site_df[\"Split\"]      = _col(site_df, \"Split\").combine_first(p_split)\n",
    "        site_df[\"COVID19\"]    = _col(site_df, \"COVID19\").combine_first(p_covid)\n",
    "        site_df[\"TVLREST\"]    = _col(site_df, \"TVLREST\").combine_first(p_tvl)\n",
    "        site_df[\"CMMTY\"]      = _col(site_df, \"CMMTY\").combine_first(p_cmty)\n",
    "\n",
    "        # ---- Handle layout shift in May 2021 (CMMTY replaces COVID19/TVLREST) ----\n",
    "        if any(site_df[\"Month\"].astype(str).str.contains(r\"May\\s*2021\", case=False, na=False)):\n",
    "            cols_needed = [\n",
    "                \"Loc\",\"LNAME\",\"Open\",\"Closed\",\"KSI\",\"CmtyNum\",\"SVC\",\"FONUM\",\n",
    "                \"FOSHORT\",\"REGNUM\",\"Region\",\"SMSSection\",\"SMSBank\",\n",
    "                \"MESSAGE\",\"RptGrp\",\"Shortname\",\"Split\",\"CMMTY\"\n",
    "            ]\n",
    "        else:\n",
    "            cols_needed = [\n",
    "                \"Loc\",\"LNAME\",\"Open\",\"Closed\",\"KSI\",\"CmtyNum\",\"SVC\",\"FONUM\",\n",
    "                \"FOSHORT\",\"REGNUM\",\"Region\",\"SMSSection\",\"SMSBank\",\n",
    "                \"MESSAGE\",\"RptGrp\",\"Shortname\",\"Split\",\"COVID19\",\"TVLREST\"\n",
    "            ]\n",
    "        \n",
    "        # Not needed in final output\n",
    "        site_df.drop(columns=[\"PLACE\"], errors=\"ignore\", inplace=True)\n",
    "\n",
    "        # Guarantee all columns exist and reorder them\n",
    "        for c in cols_needed:\n",
    "            if c not in site_df.columns:\n",
    "                site_df[c] = pd.NA\n",
    "\n",
    "        site_df = site_df[[c for c in cols_needed if c in site_df.columns] +\n",
    "                        [c for c in site_df.columns if c not in cols_needed]]\n",
    "\n",
    "\n",
    "        # Fallbacks from original site parser if present\n",
    "        if \"Cmty\" in site_df.columns:\n",
    "            site_df[\"CMMTY\"] = site_df[\"CMMTY\"].combine_first(site_df[\"Cmty\"])\n",
    "        if \"CmtyNum\" in site_df.columns:\n",
    "            site_df[\"CmtyNum\"] = pd.to_numeric(site_df[\"CmtyNum\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        \n",
    "    # ---- end of site_df processing ----\n",
    "\n",
    "    # Derive years in storage pivot: bucket Years_in_Storage to integers and count by age\n",
    "    if not asset_df.empty:\n",
    "        asset_df['Age_int'] = pd.to_numeric(asset_df['Age'], errors='coerce').astype('Int64')\n",
    "        asset_df['Years_in_Storage_float'] = pd.to_numeric(asset_df['Years_in_Storage'], errors='coerce')\n",
    "        asset_df['Years_in_Storage_int'] = asset_df['Years_in_Storage_float'].fillna(0).astype(int)\n",
    "        pivot_tables: List[pd.DataFrame] = []\n",
    "        for month, group in asset_df[(asset_df['Age_int'].notna()) & (asset_df['Age_int'] <= 24)].groupby('Month'):\n",
    "            pt = pd.pivot_table(\n",
    "                group,\n",
    "                index='Age_int',\n",
    "                columns='Years_in_Storage_int',\n",
    "                values='Asset',\n",
    "                aggfunc='count',\n",
    "                fill_value=0\n",
    "            )\n",
    "            pt = pt.reset_index().rename(columns={'Age_int': 'Age'})\n",
    "            pt.columns = ['Age'] + [f'Storage_{col}' for col in pt.columns[1:]]\n",
    "            pt.insert(0, 'Month', month)\n",
    "            pivot_tables.append(pt)\n",
    "        years_storage_df = pd.concat(pivot_tables, ignore_index=True) if pivot_tables else pd.DataFrame()\n",
    "    else:\n",
    "        years_storage_df = pd.DataFrame()\n",
    "    return asset_df, floor_df, site_df, years_storage_df\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Main entry point\n",
    "###############################################################################\n",
    "\n",
    "def run_all(pdf_path: str,\n",
    "            outdir: str = 'csv_output',\n",
    "            excel_path: str = None,\n",
    "            also_write_csvs: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Execute the full extraction pipeline, optionally writing CSVs AND/OR\n",
    "    a single Excel workbook with one sheet per table.\n",
    "    \"\"\"\n",
    "    # Extract three high-level tables\n",
    "    region_df, field_df, installed_df = extract_region_field_installed(pdf_path)\n",
    "    # Extract detailed tables\n",
    "    asset_df, floor_df, site_df, years_storage_df = extract_detailed_tables(pdf_path)\n",
    "\n",
    "    # ---- CSVs (optional) ----\n",
    "    if also_write_csvs:\n",
    "        os.makedirs(outdir, exist_ok=True)\n",
    "        if not region_df.empty:\n",
    "            region_df.to_csv(os.path.join(outdir, 'assets_by_region_service.csv'), index=False)\n",
    "        if not field_df.empty:\n",
    "            field_df.to_csv(os.path.join(outdir, 'assets_by_field_office.csv'), index=False)\n",
    "        if not installed_df.empty:\n",
    "            installed_df.to_csv(os.path.join(outdir, 'installed_assets_location_manufacture.csv'), index=False)\n",
    "        if not asset_df.empty:\n",
    "            asset_df.to_csv(os.path.join(outdir, 'asset_details.csv'), index=False)\n",
    "        if not floor_df.empty:\n",
    "            floor_df.to_csv(os.path.join(outdir, 'floor_asset_details.csv'), index=False)\n",
    "        if not site_df.empty:\n",
    "            site_df.to_csv(os.path.join(outdir, 'site_operational_status.csv'), index=False)\n",
    "        if not years_storage_df.empty:\n",
    "            years_storage_df.to_csv(os.path.join(outdir, 'years_in_storage.csv'), index=False)\n",
    "\n",
    "    # ---- Single Excel workbook ----\n",
    "    # Default Excel path if not provided\n",
    "    if excel_path is None:\n",
    "        excel_path = os.path.join(outdir, 'FY2021_Asset_Report.xlsx')\n",
    "\n",
    "    # Ensure destination directory exists\n",
    "    Path(excel_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Sheet names must be <= 31 chars and unique\n",
    "    sheets = [\n",
    "        (\"Region_Service\", region_df),\n",
    "        (\"Field_Office\", field_df),\n",
    "        (\"Installed_Assets\", installed_df),\n",
    "        (\"Asset_Details\", asset_df),\n",
    "        (\"Floor_Details\", floor_df),\n",
    "        (\"Site_Status\", site_df),\n",
    "        (\"Years_Storage\", years_storage_df),\n",
    "    ]\n",
    "\n",
    "    # Keep only non-empty frames\n",
    "    sheets = [(name, df) for name, df in sheets if not df.empty]\n",
    "\n",
    "    if sheets:\n",
    "        # Deduplicate sheet names if any collision (defensive)\n",
    "        used = set()\n",
    "        final = []\n",
    "        for name, df in sheets:\n",
    "            base = name[:31]\n",
    "            nm = base\n",
    "            i = 1\n",
    "            while nm in used:\n",
    "                suffix = f\"_{i}\"\n",
    "                nm = (base[:31 - len(suffix)]) + suffix\n",
    "                i += 1\n",
    "            used.add(nm)\n",
    "            final.append((nm, df))\n",
    "\n",
    "        with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:\n",
    "            for sheet_name, df in final:\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "        print(f'Excel workbook written to: {excel_path}')\n",
    "    else:\n",
    "        print('No tables to write to Excel (all DataFrames were empty).')\n",
    "\n",
    "    # Print summary\n",
    "    print('Extraction complete.')\n",
    "    print(f'Region summary rows: {len(region_df)}')\n",
    "    print(f'Field office rows: {len(field_df)}')\n",
    "    print(f'Installed assets rows: {len(installed_df)}')\n",
    "    print(f'Asset details rows: {len(asset_df)}')\n",
    "    print(f'Floor asset details rows: {len(floor_df)}')\n",
    "    print(f'Site operational rows: {len(site_df)}')\n",
    "    print(f'Years in storage rows: {len(years_storage_df)}')\n",
    "\n",
    "# --- update your __main__ to point to the Excel destination you want ---\n",
    "if __name__ == '__main__':\n",
    "    pdf_path = r\"C:\\Users\\dawid\\Desktop\\MUCKROCK\\data\\FY2021 Asset Report.pdf\"\n",
    "    # keep CSVs (if you want) but also write a single Excel file here:\n",
    "    excel_out = r\"C:\\Users\\dawid\\Desktop\\MUCKROCK\\database\\FY2021_CSVs\\FY2021_Asset_Report.xlsx\"\n",
    "    output_dir = r\"C:\\Users\\dawid\\Desktop\\MUCKROCK\\database\\FY2021_CSVs\\csv_output\"\n",
    "\n",
    "    if os.path.exists(pdf_path):\n",
    "        run_all(pdf_path, outdir=output_dir, excel_path=excel_out, also_write_csvs=True)\n",
    "    else:\n",
    "        print(f'PDF file not found: {pdf_path}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
