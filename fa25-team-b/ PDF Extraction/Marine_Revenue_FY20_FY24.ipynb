{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Preliminary Setup"
      ],
      "metadata": {
        "id": "ddGzOun1yRfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber tabula-py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iH6EUeiPO7K",
        "outputId": "ad881492-700a-445c-dcf8-ce3916ce733a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tabula-py\n",
            "  Downloading tabula_py-2.10.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting pdfminer.six==20251107 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: pandas>=0.25.3 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.2.2)\n",
            "Requirement already satisfied: numpy>1.24.4 in /usr/local/lib/python3.12/dist-packages (from tabula-py) (2.0.2)\n",
            "Requirement already satisfied: distro in /usr/local/lib/python3.12/dist-packages (from tabula-py) (1.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.25.3->tabula-py) (2025.2)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.17.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tabula_py-2.10.0-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, tabula-py, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20251107 pdfplumber-0.11.8 pypdfium2-5.0.0 tabula-py-2.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "import pickle\n",
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pdfplumber\n",
        "\n",
        "url = 'https://drive.google.com/uc?export=download&id=1jKoXzzd5bqpcJeT_TtchmoGza3WRY8Xc'\n",
        "output = 'Marine_Revenue_FY20-FY24.pdf'\n",
        "gdown.download(url, output, quiet=False)"
      ],
      "metadata": {
        "id": "DAybzsS7IIKP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "d3ed6eea-30ae-4cd8-fc03-7baa563b466f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1jKoXzzd5bqpcJeT_TtchmoGza3WRY8Xc\n",
            "To: /content/Marine_Revenue_FY20-FY24.pdf\n",
            "100%|██████████| 1.64M/1.64M [00:00<00:00, 47.8MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Marine_Revenue_FY20-FY24.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary Table"
      ],
      "metadata": {
        "id": "WMi4My_3x2t9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from pathlib import Path\n",
        "import pdfplumber\n",
        "import pandas as pd\n",
        "\n",
        "# ---- Paths and output ----\n",
        "PDF = Path(\"/content/Marine_Revenue_FY20-FY24.pdf\")\n",
        "OUT = PDF.parent / \"Marine_Revenue_FY20-FY24_summary_table.csv\"\n",
        "\n",
        "# ---- Columns ----\n",
        "COLS = [\"Country\", \"Installation\", \"FY16\", \"FY17\", \"FY18\", \"FY19\", \"FY20 thru SEP\", \"Annualized FY20\"]\n",
        "\n",
        "# ---- Normalization maps ----\n",
        "COUNTRY_MAP = {\"japan\": \"Japan\", \"korea\": \"Korea\"}\n",
        "INSTALL_MAP = {\n",
        "    \"campfuji\": \"Camp Fuji\", \"campschwab\": \"Camp Schwab\", \"camphansen\": \"Camp Hansen\",\n",
        "    \"campcourtney\": \"Camp Courtney\", \"campbutler/foster\": \"Camp Butler/Foster\",\n",
        "    \"campfutenma\": \"Camp Futenma\", \"campkinser\": \"Camp Kinser\",\n",
        "    \"iwakuni\": \"Iwakuni\", \"campmujuk\": \"Camp Mujuk\"\n",
        "}\n",
        "\n",
        "TOTAL_RX = re.compile(r\"^totalrevenue$\", re.IGNORECASE)\n",
        "\n",
        "def compact(s):\n",
        "    return re.sub(r\"\\s+\", \"\", s or \"\").strip()\n",
        "\n",
        "def normalize_country(s):\n",
        "    key = compact(s).lower()\n",
        "    return COUNTRY_MAP.get(key, (s or \"\").strip())\n",
        "\n",
        "def normalize_installation(s):\n",
        "    s2 = re.sub(r\"(?<=\\w)\\s(?=\\w)\", \"\", s or \"\")\n",
        "    key = compact(s2).lower()\n",
        "    return INSTALL_MAP.get(key, s2.strip())\n",
        "\n",
        "def money_to_float(s):\n",
        "    if not s:\n",
        "        return None\n",
        "    s = str(s).replace(\",\", \"\").replace(\"$\", \"\").strip()\n",
        "    if s.startswith(\"(\") and s.endswith(\")\"):\n",
        "        s = \"-\" + s[1:-1]\n",
        "    try:\n",
        "        return float(s)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# ---------- Basic line/character grouping ----------\n",
        "def chars_to_lines(chars, y_tol=3):\n",
        "    chars = sorted(chars, key=lambda c: (c[\"top\"], c[\"x0\"]))\n",
        "    lines, cur, last_y = [], [], None\n",
        "    for ch in chars:\n",
        "        if last_y is None or abs(ch[\"top\"] - last_y) <= y_tol:\n",
        "            cur.append(ch)\n",
        "            last_y = ch[\"top\"] if last_y is None else (last_y + ch[\"top\"]) / 2\n",
        "        else:\n",
        "            if cur:\n",
        "                lines.append(cur)\n",
        "            cur = [ch]\n",
        "            last_y = ch[\"top\"]\n",
        "    if cur:\n",
        "        lines.append(cur)\n",
        "    return lines\n",
        "\n",
        "def _merge_chars(chars):\n",
        "    txt = \"\".join(c[\"text\"] for c in chars)\n",
        "    return {\"text\": txt, \"x0\": min(c[\"x0\"] for c in chars), \"x1\": max(c[\"x1\"] for c in chars)}\n",
        "\n",
        "def line_to_words(line_chars, gap=3):\n",
        "    if not line_chars:\n",
        "        return []\n",
        "    line_chars = sorted(line_chars, key=lambda c: c[\"x0\"])\n",
        "    words, cur = [], [line_chars[0]]\n",
        "    for ch in line_chars[1:]:\n",
        "        if ch[\"x0\"] - cur[-1][\"x1\"] <= gap:\n",
        "            cur.append(ch)\n",
        "        else:\n",
        "            words.append(_merge_chars(cur))\n",
        "            cur = [ch]\n",
        "    if cur:\n",
        "        words.append(_merge_chars(cur))\n",
        "    return words\n",
        "\n",
        "# ---------- Header detection and column splitting ----------\n",
        "def detect_cuts_from_header(wline):\n",
        "    \"\"\"\n",
        "    Estimate column split lines based on approximate positions of header texts;\n",
        "    if not found, fallback to using average step width.\n",
        "    \"\"\"\n",
        "    fy_pattern = re.compile(r\"^(fy)?\\d+(\\.\\d+)?$|annualized\", re.IGNORECASE)\n",
        "    boxes = []\n",
        "    for w in wline:\n",
        "        key = compact(w[\"text\"]).lower()\n",
        "        mid = (w[\"x0\"] + w[\"x1\"]) / 2\n",
        "        width = w[\"x1\"] - w[\"x0\"]\n",
        "        if key == \"countryinstallation\":\n",
        "            # Special handling for merged header\n",
        "            total_len = len(\"countryinstallation\")\n",
        "            country_len = len(\"country\")\n",
        "            proportion = country_len / total_len\n",
        "            mid1 = w[\"x0\"] + proportion * width * 0.5\n",
        "            mid2 = w[\"x1\"] - (1 - proportion) * width * 0.5\n",
        "            boxes.append((mid1, \"country\"))\n",
        "            boxes.append((mid2, \"installation\"))\n",
        "        elif \"country\" in key:\n",
        "            boxes.append((mid, \"country\"))\n",
        "        elif \"installation\" in key:\n",
        "            boxes.append((mid, \"installation\"))\n",
        "        elif fy_pattern.match(key):\n",
        "            boxes.append((mid, \"fy\"))\n",
        "    if len(boxes) < len(COLS):\n",
        "        left = min(w[\"x0\"] for w in wline)\n",
        "        right = max(w[\"x1\"] for w in wline)\n",
        "        step = (right - left) / len(COLS)\n",
        "        return [left + i * step for i in range(len(COLS) + 1)]\n",
        "    boxes.sort()\n",
        "    xs = [x for x, _ in boxes]\n",
        "    cuts = [-1e9] + [(xs[i] + xs[i + 1]) / 2 for i in range(len(xs) - 1)] + [1e9]\n",
        "    return cuts\n",
        "\n",
        "def find_header_index(lines_words):\n",
        "    \"\"\"\n",
        "    Automatically find the header row that contains keywords like Country / Installation / FYxx.\n",
        "    Return the index of that row; return None if not found.\n",
        "    \"\"\"\n",
        "    for idx, wline in enumerate(lines_words):\n",
        "        texts = [compact(w.get(\"text\",\"\")).lower() for w in wline]\n",
        "        joined = \" \".join(texts)\n",
        "        # Must contain both \"country\" and \"installation\", and also \"fy\"\n",
        "        if (\"country\" in joined and \"installation\" in joined and \"fy\" in joined):\n",
        "            return idx\n",
        "    return None\n",
        "\n",
        "# ---------- Data row splitting and fixing ----------\n",
        "def assign_row(wline, cuts):\n",
        "    buckets = [[] for _ in range(len(COLS))]\n",
        "    for w in sorted(wline, key=lambda d: d[\"x0\"]):\n",
        "        xm = (w[\"x0\"] + w[\"x1\"]) / 2\n",
        "        for i in range(len(COLS)):\n",
        "            if cuts[i] <= xm < cuts[i + 1]:\n",
        "                buckets[i].append(w[\"text\"])\n",
        "                break\n",
        "\n",
        "    def join_txt(t):\n",
        "        return re.sub(r\"(?<=\\w)\\s(?=\\w)\", \"\", \" \".join(t)).strip()\n",
        "    def join_money(t):\n",
        "        return re.sub(r\"[^\\d\\-\\.\\$,\\(\\)]\", \"\", \"\".join(t).replace(\" \", \"\"))\n",
        "\n",
        "    row = []\n",
        "    for i, toks in enumerate(buckets):\n",
        "        row.append(join_txt(toks) if i <= 1 else join_money(toks))\n",
        "    return row\n",
        "\n",
        "def fix_country_installation(row):\n",
        "    c, i = row[0], row[1]\n",
        "    if i:\n",
        "        return [normalize_country(c), normalize_installation(i)] + row[2:]\n",
        "    # Handle merged cases like \"KoreaCampMujuk\" or \"JapanCampFuji\"\n",
        "    c_compact = compact(c)\n",
        "    known_countries = [\"Korea\", \"Japan\"]\n",
        "    for country in known_countries:\n",
        "        if c_compact.startswith(country):\n",
        "            install_part = c[len(country):]  # Preserve original formatting\n",
        "            return [normalize_country(country), normalize_installation(install_part)] + row[2:]\n",
        "    # Fallback to searching for \"Camp\"\n",
        "    m = re.search(r\"Camp\", c, re.IGNORECASE)\n",
        "    if m:\n",
        "        pos = m.start()\n",
        "        country_part = c[:pos]\n",
        "        install_part = c[pos:]\n",
        "        return [normalize_country(country_part), normalize_installation(install_part)] + row[2:]\n",
        "    return [normalize_country(c), normalize_installation(i)] + row[2:]\n",
        "\n",
        "# ---------- Extract a single page ----------\n",
        "def extract_page_slot_revenue(page):\n",
        "    # Extract text as characters → lines → words\n",
        "    lines_chars = chars_to_lines(page.chars)\n",
        "    lines_words = [line_to_words(ln) for ln in lines_chars if ln]\n",
        "\n",
        "    # Find header\n",
        "    hdr_idx = find_header_index(lines_words)\n",
        "    if hdr_idx is None:\n",
        "        return pd.DataFrame(columns=COLS)  # Return empty DataFrame if not found\n",
        "\n",
        "    hdr = lines_words[hdr_idx]\n",
        "    cuts = detect_cuts_from_header(hdr)\n",
        "\n",
        "    rows = []\n",
        "    for i in range(hdr_idx + 1, len(lines_words)):\n",
        "        txt = compact(\"\".join(w[\"text\"] for w in lines_words[i]))\n",
        "        if TOTAL_RX.match(txt):\n",
        "            break\n",
        "        row = assign_row(lines_words[i], cuts)\n",
        "        row = fix_country_installation(row)\n",
        "        # Keep rows where either Country/Installation or any numeric column has data\n",
        "        if any(row[2:]) or row[0] or row[1]:\n",
        "            rows.append(row)\n",
        "\n",
        "    df = pd.DataFrame(rows, columns=COLS)\n",
        "    # Clean numeric values\n",
        "    for c in COLS[2:]:\n",
        "        df[c] = df[c].apply(lambda x: None if not x else x)\n",
        "        df[c] = df[c].apply(money_to_float)\n",
        "    return df\n",
        "\n",
        "# ---------- Main program: extract pages 1, 35, 115 ----------\n",
        "def main():\n",
        "    target_pages_1based = [1, 35, 73, 115, 157]\n",
        "    target_indices = [p - 1 for p in target_pages_1based]  # Convert to 0-based index\n",
        "\n",
        "    all_dfs = []\n",
        "    with pdfplumber.open(PDF) as pdf:\n",
        "        for i, pidx in enumerate(target_indices):\n",
        "            if pidx < 0 or pidx >= len(pdf.pages):\n",
        "                print(f\"⚠️ Specified page number out of range: {pidx+1}\")\n",
        "                continue\n",
        "            page = pdf.pages[pidx]\n",
        "            df = extract_page_slot_revenue(page)\n",
        "            if not df.empty:\n",
        "                df.insert(0, \"Page\", pidx + 1)  # Preserve actual 1-based page number\n",
        "                all_dfs.append(df)\n",
        "            else:\n",
        "                print(f\"⚠️ Page {pidx+1} did not detect Slot Revenue table.\")\n",
        "\n",
        "    if all_dfs:\n",
        "        out_df = pd.concat(all_dfs, ignore_index=True)\n",
        "        out_df.to_csv(OUT, index=False, encoding=\"utf-8-sig\")\n",
        "        print(f\"✅ Slot Revenue: {len(out_df)} rows exported → {OUT}\")\n",
        "    else:\n",
        "        print(\"⚠️ No table data retrieved from any page.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UF3DWKiupH2",
        "outputId": "0218e8f8-694b-428c-87be-7d80f39886fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Slot Revenue: 97 rows exported → /content/Marine_Revenue_FY20-FY24_summary_table.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detail Table"
      ],
      "metadata": {
        "id": "697M6so7xzoW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Path configuration ----\n",
        "PDF = Path(\"Marine_Revenue_FY20-FY24.pdf\")  # your PDF filename\n",
        "OUT = PDF.parent / \"Marine_Revenue_FY20-FY24_detail.csv\"\n",
        "\n",
        "# ---- Columns and extraction parameters ----\n",
        "COLS = [\"Loc #\", \"Location\", \"Month\", \"Revenue\", \"NAFI Amt\", \"Annual Revenue\", \"Annual NAFI\"]\n",
        "\n",
        "# Baseline tuning parameters. These values work for the majority of pages.\n",
        "# See SPECIAL_PARAMS below for overrides used on specific page ranges.\n",
        "BASE_Y_TOL = 3.0        # vertical merge tolerance\n",
        "BASE_X_JOIN = 4.0       # horizontal character join tolerance\n",
        "BASE_GAP_RATIO = 0.8    # smart join whitespace threshold\n",
        "BASE_DROP_MIN = 2       # minimum number of non-empty columns to keep a row\n",
        "BASE_EDGE_PAD = 15      # extra padding around column cut lines\n",
        "BASE_LEFT_SHIFT_MONTH = 18   # slight left shift for the Month column\n",
        "BASE_LEFT_SHIFT_REVENUE = 8  # slight left shift for the Revenue column\n",
        "\n",
        "# Page ranges with customised parameter values. The keys are range objects\n",
        "# inclusive of the start and end page numbers. For example, range(163, 167)\n",
        "# covers pages 163, 164, 165 and 166. Values that are not specified in a\n",
        "# particular override fall back to the baseline values defined above.\n",
        "SPECIAL_PARAMS = {\n",
        "    # Pages 163–166 require slightly larger tolerances and shifts to account\n",
        "    # for the layout on those pages.\n",
        "    range(163, 167): {\n",
        "        \"Y_TOL\": 6.5,\n",
        "        \"X_JOIN\": 2.6,\n",
        "        \"GAP_RATIO\": 0.52,\n",
        "        \"DROP_MIN\": 1,\n",
        "        \"EDGE_PAD\": 8,\n",
        "        \"LEFT_SHIFT_MONTH\": 14,\n",
        "        \"LEFT_SHIFT_REVENUE\": 5,\n",
        "    },\n",
        "    # Pages 172–177 have tighter spacing horizontally but require a larger\n",
        "    # vertical tolerance; adjust the shifts modestly.\n",
        "    range(172, 178): {\n",
        "        \"Y_TOL\": 4.0,\n",
        "        \"X_JOIN\": 4.5,\n",
        "        \"GAP_RATIO\": 0.8,\n",
        "        \"DROP_MIN\": 2,\n",
        "        \"EDGE_PAD\": 15,\n",
        "        \"LEFT_SHIFT_MONTH\": 20,\n",
        "        \"LEFT_SHIFT_REVENUE\": 10,\n",
        "    },\n",
        "    # Pages 178–190 contain very dense tables with wider spacing between rows\n",
        "    # and larger numbers; increase tolerances and edge padding accordingly.\n",
        "    range(178, 191): {\n",
        "        \"Y_TOL\": 5.0,\n",
        "        \"X_JOIN\": 5.0,\n",
        "        \"GAP_RATIO\": 0.8,\n",
        "        \"DROP_MIN\": 2,\n",
        "        \"EDGE_PAD\": 20,\n",
        "        \"LEFT_SHIFT_MONTH\": 22,\n",
        "        \"LEFT_SHIFT_REVENUE\": 12,\n",
        "    },\n",
        "}\n",
        "\n",
        "# ---- Regular expressions ----\n",
        "MONTH_FULL = re.compile(r\"^(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[-/]?\\d{2}$\", re.I)\n",
        "MONTH_REV = re.compile(r\"^\\d{2}[-/](?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)$\", re.I)\n",
        "MONTH_ONLY = re.compile(r\"^(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)$\", re.I)\n",
        "NUM_RE = re.compile(r\"^-?\\(?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\)?$\")\n",
        "INT_RE = re.compile(r\"^\\d+$\")\n",
        "YEAR_FRAG = {-17, -18, -19, -20}\n",
        "NUM_RE_STRICT = re.compile(r\"^-?\\(?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?\\)?$\")\n",
        "money_re = re.compile(r'^-?\\(?\\d+(?:\\.\\d+)?\\)?$')\n",
        "\n",
        "# ---- Header patterns ----\n",
        "HEADER_PATTERNS = {\n",
        "    \"Loc #\": re.compile(r\"^Loc\\s*#?$\", re.I),\n",
        "    \"Location\": re.compile(r\"^Location$\", re.I),\n",
        "    \"Month\": re.compile(r\"^Month$\", re.I),\n",
        "    \"Revenue\": re.compile(r\"^Revenue$\", re.I),\n",
        "    \"NAFI Amt\": re.compile(r\"^NAFI\\s*Amt$\", re.I),\n",
        "    \"Annual Revenue\": re.compile(r\"^Annual\\s+Revenue$\", re.I),\n",
        "    \"Annual NAFI\": re.compile(r\"^Annual\\s+NAFI$\", re.I),\n",
        "}\n",
        "HEADER_ORDER = [\"Loc #\", \"Location\", \"Month\", \"Revenue\", \"NAFI Amt\", \"Annual Revenue\", \"Annual NAFI\"]\n",
        "\n",
        "# ==== Noise keywords (exclude tokens not part of data rows) ====\n",
        "EXCLUDE_LINE_PATTERNS = [\n",
        "    re.compile(r\"^ARMP\\s+Marine\\s+Slot\\s+Report$\", re.I),\n",
        "    re.compile(r\"^Monthly\\s+Summary\\s+by\\s+Location$\", re.I),\n",
        "    re.compile(r\"^Slot\\s+Revenue$\", re.I),\n",
        "    re.compile(r\"^NAFI\\s+Reimbursement\\s+from\\s+ARMP$\", re.I),\n",
        "    re.compile(r\"^Region$\", re.I),\n",
        "    re.compile(r\"^Loc\\s*#?$\", re.I),\n",
        "    re.compile(r\"^Location$\", re.I),\n",
        "    re.compile(r\"^Month$\", re.I),\n",
        "    re.compile(r\"^Revenue$\", re.I),\n",
        "    re.compile(r\"^NAFI\\s*Amt$\", re.I),\n",
        "    re.compile(r\"^Annual\\s+Revenue$\", re.I),\n",
        "    re.compile(r\"^Annual\\s+NAFI$\", re.I),\n",
        "]\n",
        "\n",
        "def _is_exclude_token(t):\n",
        "    txt = t[\"text\"].strip()\n",
        "    return any(p.match(txt) for p in EXCLUDE_LINE_PATTERNS)\n",
        "\n",
        "# ---------------- Basic helpers ----------------\n",
        "def chars_to_words(page, x_tol, y_tol):\n",
        "    \"\"\"Group characters from a pdfplumber page into word tokens.\n",
        "    Parameters ``x_tol`` and ``y_tol`` control the tolerance for horizontal\n",
        "    character joins and vertical line grouping, respectively. These values\n",
        "    will be supplied from the current page's parameter set.\n",
        "    \"\"\"\n",
        "    chars = sorted(page.chars, key=lambda c: (round(c[\"top\"], 1), c[\"x0\"]))\n",
        "    lines, words = [], []\n",
        "    for ch in chars:\n",
        "        if not lines or abs(ch[\"top\"] - lines[-1][\"y\"]) > y_tol:\n",
        "            lines.append({\"y\": ch[\"top\"], \"chars\": [ch]})\n",
        "        else:\n",
        "            lines[-1][\"chars\"].append(ch)\n",
        "    for line in lines:\n",
        "        row = sorted(line[\"chars\"], key=lambda c: c[\"x0\"])\n",
        "        cur = [row[0]]\n",
        "        for c in row[1:]:\n",
        "            if c[\"x0\"] - cur[-1][\"x1\"] <= x_tol:\n",
        "                cur.append(c)\n",
        "            else:\n",
        "                words.append({\n",
        "                    \"text\": \"\".join(x[\"text\"] for x in cur),\n",
        "                    \"x0\": cur[0][\"x0\"],\n",
        "                    \"x1\": cur[-1][\"x1\"],\n",
        "                    \"top\": line[\"y\"],\n",
        "                })\n",
        "                cur = [c]\n",
        "        if cur:\n",
        "            words.append({\n",
        "                \"text\": \"\".join(x[\"text\"] for x in cur),\n",
        "                \"x0\": cur[0][\"x0\"],\n",
        "                \"x1\": cur[-1][\"x1\"],\n",
        "                \"top\": line[\"y\"],\n",
        "            })\n",
        "    return words\n",
        "\n",
        "def smart_join(tokens, x_tol, gap_ratio):\n",
        "    \"\"\"Join tokens horizontally, inserting spaces when appropriate.\n",
        "    The ``gap_ratio`` parameter determines how aggressive the join is when\n",
        "    encountering whitespace between tokens. Higher values will insert\n",
        "    spaces more frequently.\n",
        "    \"\"\"\n",
        "    if not tokens:\n",
        "        return \"\"\n",
        "    tokens = sorted(tokens, key=lambda w: w[\"x0\"])\n",
        "    s = tokens[0][\"text\"]\n",
        "    for i in range(1, len(tokens)):\n",
        "        prev = tokens[i - 1]\n",
        "        curr = tokens[i]\n",
        "        gap = curr[\"x0\"] - prev[\"x1\"]\n",
        "        prev_txt = prev[\"text\"].strip()\n",
        "        curr_txt = curr[\"text\"].strip()\n",
        "        # handle commas and numbers specially\n",
        "        if (\n",
        "            gap <= x_tol or\n",
        "            (prev_txt.isdigit() and curr_txt == \",\") or\n",
        "            (prev_txt == \",\" and curr_txt.isdigit()) or\n",
        "            (prev_txt.isdigit() and curr_txt.isdigit()) or\n",
        "            (\".\" in prev_txt and curr_txt.isdigit())\n",
        "        ):\n",
        "            s += curr[\"text\"]\n",
        "        else:\n",
        "            s += \" \" + curr[\"text\"]\n",
        "    return s.strip()\n",
        "\n",
        "def assign_bin(xmid, cuts, edge_pad):\n",
        "    \"\"\"Return the index of the bucket into which the x-coordinate ``xmid`` falls.\n",
        "    ``edge_pad`` expands each cut line slightly on either side to catch\n",
        "    borderline tokens.\n",
        "    \"\"\"\n",
        "    for i in range(len(cuts) - 1):\n",
        "        if cuts[i] - edge_pad <= xmid <= cuts[i + 1] + edge_pad:\n",
        "            return i\n",
        "    return None\n",
        "\n",
        "def header_y_of(words):\n",
        "    ys = [w[\"top\"] for w in words if w[\"text\"] in (\"Location\", \"Month\", \"Revenue\")]\n",
        "    return min(ys) if ys else 0\n",
        "\n",
        "def group_by_y(words, y_tol):\n",
        "    lines = []\n",
        "    for w in sorted(words, key=lambda w: (round(w[\"top\"], 1), w[\"x0\"])):\n",
        "        if not lines or abs(w[\"top\"] - lines[-1][\"y\"]) > y_tol:\n",
        "            lines.append({\"y\": w[\"top\"], \"tokens\": [w]})\n",
        "        else:\n",
        "            lines[-1][\"tokens\"].append(w)\n",
        "    return lines\n",
        "\n",
        "# ---------------- Detect header and cuts ----------------\n",
        "def detect_header_line(words):\n",
        "    candidates = []\n",
        "    for line in group_by_y(words, BASE_Y_TOL):\n",
        "        texts = [t[\"text\"].strip() for t in line[\"tokens\"]]\n",
        "        hit = sum(1 for pat in HEADER_PATTERNS.values() if any(pat.match(tx) for tx in texts))\n",
        "        if hit >= 3:\n",
        "            candidates.append((line[\"y\"], line[\"tokens\"], hit))\n",
        "    if not candidates:\n",
        "        return 0, None\n",
        "    candidates.sort(key=lambda x: (x[2], x[0]))\n",
        "    y, toks, _ = candidates[-1]\n",
        "    return y, toks\n",
        "\n",
        "def cuts_from_header(words, base_cuts, prev_cuts, left_shift_month, left_shift_revenue):\n",
        "    \"\"\"\n",
        "    Determine each column's cut lines (cuts) based on detected header tokens.\n",
        "    Return (cuts, header_y). If detection fails for any reason, return\n",
        "    (prev_cuts or base_cuts, an estimated header_y).\n",
        "    \"\"\"\n",
        "    # First try: detect the header line by finding a line containing ≥3 header titles\n",
        "    header_y, header_tokens = detect_header_line(words)\n",
        "\n",
        "    # Provide a fallback header_y (use the minimum y among common columns)\n",
        "    fallback_hy = header_y_of(words)\n",
        "\n",
        "    # If a header line is found, derive cuts directly from that line's token x0 values\n",
        "    if header_tokens:\n",
        "        # Map each token text on that line to a header field name\n",
        "        name_to_x0 = {}\n",
        "        for t in header_tokens:\n",
        "            tx = t[\"text\"].strip()\n",
        "            for name, pat in HEADER_PATTERNS.items():\n",
        "                if pat.match(tx):\n",
        "                    # Use only the leftmost x0 for that field\n",
        "                    name_to_x0[name] = min(name_to_x0.get(name, t[\"x0\"]), t[\"x0\"])\n",
        "        if name_to_x0:\n",
        "            # Generate each column's left boundary (starts) according to HEADER_ORDER\n",
        "            # If a field is missing, use the previous page's or base position as an interpolation\n",
        "            ref = prev_cuts or base_cuts\n",
        "            starts = []\n",
        "            for name in HEADER_ORDER:\n",
        "                if name in name_to_x0:\n",
        "                    starts.append(float(name_to_x0[name]))\n",
        "                else:\n",
        "                    # Estimate a reasonable position using the previous/base column boundaries\n",
        "                    col_idx = HEADER_ORDER.index(name)\n",
        "                    left_edge = ref[col_idx]\n",
        "                    right_edge = ref[col_idx + 1]\n",
        "                    starts.append((left_edge * 0.65 + right_edge * 0.35))\n",
        "            # Produce cuts from starts: use the midpoint between adjacent column left edges\n",
        "            cuts = [-1e9]\n",
        "            for i in range(1, len(starts)):\n",
        "                cuts.append((starts[i - 1] + starts[i]) / 2.0)\n",
        "            cuts.append(1e9)\n",
        "            # Slightly adjust the Month/Revenue boundaries (consistent with the original left-shift)\n",
        "            idx_month = HEADER_ORDER.index(\"Month\")\n",
        "            idx_revenue = HEADER_ORDER.index(\"Revenue\")\n",
        "            # Note: cuts has one fewer index than columns; the boundary to the right of Month is cuts[idx_month+1]\n",
        "            # The original logic shifts the boundary left after generating starts; keep that simple behavior here:\n",
        "            cuts[idx_month + 1] -= left_shift_month\n",
        "            cuts[idx_revenue + 1] -= left_shift_revenue\n",
        "            return cuts, (header_y or fallback_hy)\n",
        "        # Header line found but no field matched → fall back\n",
        "        return (prev_cuts or base_cuts), (header_y or fallback_hy)\n",
        "\n",
        "    # ---- No header line found: traverse the page and use the first x0 where each header appears as reference ----\n",
        "    name_to_x0 = {}\n",
        "    for name, pat in HEADER_PATTERNS.items():\n",
        "        xs = [w[\"x0\"] for w in words if pat.match(w[\"text\"].strip())]\n",
        "        if xs:\n",
        "            name_to_x0[name] = float(min(xs))\n",
        "    if name_to_x0:\n",
        "        ref = prev_cuts or base_cuts\n",
        "        starts = []\n",
        "        for name in HEADER_ORDER:\n",
        "            if name in name_to_x0:\n",
        "                starts.append(name_to_x0[name])\n",
        "            else:\n",
        "                # If missing, interpolate using previous/base boundaries\n",
        "                col_idx = HEADER_ORDER.index(name)\n",
        "                left_edge = ref[col_idx]\n",
        "                right_edge = ref[col_idx + 1]\n",
        "                starts.append((left_edge * 0.65 + right_edge * 0.35))\n",
        "        cuts = [-1e9]\n",
        "        for i in range(1, len(starts)):\n",
        "            cuts.append((starts[i - 1] + starts[i]) / 2.0)\n",
        "        cuts.append(1e9)\n",
        "        idx_month = HEADER_ORDER.index(\"Month\")\n",
        "        idx_revenue = HEADER_ORDER.index(\"Revenue\")\n",
        "        cuts[idx_month + 1] -= left_shift_month\n",
        "        cuts[idx_revenue + 1] -= left_shift_revenue\n",
        "        return cuts, fallback_hy\n",
        "\n",
        "    # ---- Still nothing found → final fallback ----\n",
        "    return (prev_cuts or base_cuts), fallback_hy\n",
        "\n",
        "def _is_money_token(s: str) -> bool:\n",
        "    \"\"\"Return True if a string looks like a numeric or currency value.\"\"\"\n",
        "    if s is None:\n",
        "        return False\n",
        "    s = str(s).strip().replace(',', '').replace('$', '')\n",
        "    return bool(re.match(r\"^-?\\(?\\d+(\\.\\d+)?\\)?$\", s))\n",
        "\n",
        "\n",
        "# ---------------- Type-aware refinement of cuts ----------------\n",
        "def _is_month_token(s: str) -> bool:\n",
        "    if s is None:\n",
        "        return False\n",
        "    s = str(s).strip()\n",
        "    return bool(\n",
        "        MONTH_FULL.match(s) or\n",
        "        MONTH_REV.match(s) or\n",
        "        MONTH_ONLY.match(s)\n",
        "    )\n",
        "\n",
        "def refine_cuts_typeaware(page, cuts, hy):\n",
        "    words = chars_to_words(page, x_tol=BASE_X_JOIN, y_tol=BASE_Y_TOL)\n",
        "    body = [w for w in words if w[\"top\"] > hy + 1 and not _is_exclude_token(w)]\n",
        "\n",
        "    rows, cur_y, cur = [], None, []\n",
        "    for w in sorted(body, key=lambda w: (round(w[\"top\"], 1), w[\"x0\"])):\n",
        "        y = round(w[\"top\"], 1)\n",
        "        if cur_y is None or abs(y - cur_y) <= BASE_Y_TOL:\n",
        "            cur.append(w)\n",
        "            cur_y = y if cur_y is None else (cur_y + y) / 2\n",
        "        else:\n",
        "            rows.append(cur)\n",
        "            cur = [w]\n",
        "            cur_y = y\n",
        "    if cur:\n",
        "        rows.append(cur)\n",
        "\n",
        "    month_right, revenue_left = [], []\n",
        "    revenue_right, nafi_left = [], []\n",
        "    nafi_right, annual_rev_left = [], []\n",
        "    annual_rev_right, annual_nafi_left = [], []\n",
        "\n",
        "    for r in rows:\n",
        "        for w in r:\n",
        "            xmid = (w[\"x0\"] + w[\"x1\"]) / 2\n",
        "            bi = assign_bin(xmid, cuts, BASE_EDGE_PAD)\n",
        "            if bi is None:\n",
        "                continue\n",
        "            txt = w[\"text\"]\n",
        "            if bi == 2 and _is_month_token(txt):  # Month bucket\n",
        "                month_right.append(w[\"x1\"])\n",
        "            if bi == 3 and _is_money_token(txt):  # Revenue bucket\n",
        "                revenue_left.append(w[\"x0\"])\n",
        "                revenue_right.append(w[\"x1\"])\n",
        "            if bi == 4 and _is_money_token(txt):  # NAFI Amt bucket\n",
        "                nafi_left.append(w[\"x0\"])\n",
        "                nafi_right.append(w[\"x1\"])\n",
        "            if bi == 5 and _is_money_token(txt):  # Annual Revenue bucket\n",
        "                annual_rev_left.append(w[\"x0\"])\n",
        "                annual_rev_right.append(w[\"x1\"])\n",
        "            if bi == 6 and _is_money_token(txt):  # Annual NAFI bucket\n",
        "                annual_nafi_left.append(w[\"x0\"])\n",
        "\n",
        "    def pct(a, p):\n",
        "        return float(np.percentile(a, p)) if a else None\n",
        "\n",
        "    # Month↔Revenue boundary (cuts[3])\n",
        "    left_p = pct(month_right, 100)\n",
        "    right_p = pct(revenue_left, 0)\n",
        "    if left_p is not None and right_p is not None and right_p > left_p:\n",
        "        target = (left_p + right_p) / 2\n",
        "        base = BASE_CUTS[3]\n",
        "        delta = CLAMP_DELTA[3]\n",
        "        cuts[3] = max(base - delta, min(base + delta, target))\n",
        "    if left_p is not None and right_p is None:\n",
        "        cuts[3] = max(cuts[3], left_p + 1)\n",
        "\n",
        "    # Revenue↔NAFI boundary (cuts[4])\n",
        "    left_p = pct(revenue_right, 100)\n",
        "    right_p = pct(nafi_left, 0)\n",
        "    if left_p is not None and right_p is not None and right_p > left_p:\n",
        "        target = (left_p + right_p) / 2\n",
        "        base = BASE_CUTS[4]\n",
        "        delta = CLAMP_DELTA[4]\n",
        "        cuts[4] = max(base - delta, min(base + delta, target))\n",
        "    if left_p is not None and right_p is None:\n",
        "        cuts[4] = max(cuts[4], left_p + 1)\n",
        "\n",
        "    # NAFI Amt↔Annual Revenue boundary (cuts[5])\n",
        "    left_p = pct(nafi_right, 100)\n",
        "    right_p = pct(annual_rev_left, 0)\n",
        "    if left_p is not None and right_p is not None and right_p > left_p:\n",
        "        target = (left_p + right_p) / 2\n",
        "        base = BASE_CUTS[5]\n",
        "        delta = CLAMP_DELTA[5]\n",
        "        cuts[5] = max(base - delta, min(base + delta, target))\n",
        "    if left_p is not None and right_p is None:\n",
        "        cuts[5] = max(cuts[5], left_p + 1)\n",
        "\n",
        "    # Annual Revenue↔Annual NAFI boundary (cuts[6])\n",
        "    left_p = pct(annual_rev_right, 100)\n",
        "    right_p = pct(annual_nafi_left, 0)\n",
        "    if left_p is not None and right_p is not None and right_p > left_p:\n",
        "        target = (left_p + right_p) / 2\n",
        "        base = BASE_CUTS[6]\n",
        "        delta = CLAMP_DELTA[6]\n",
        "        cuts[6] = max(base - delta, min(base + delta, target))\n",
        "    if left_p is not None and right_p is None:\n",
        "        cuts[6] = max(cuts[6], left_p + 1)\n",
        "\n",
        "    return cuts\n",
        "\n",
        "# ---------------- Row extraction ----------------\n",
        "def rows_from_page(page, cuts, hy, x_tol, y_tol, edge_pad, gap_ratio, drop_min):\n",
        "    words = chars_to_words(page, x_tol=x_tol, y_tol=y_tol)\n",
        "    body = [w for w in words if w[\"top\"] > hy + 1 and not _is_exclude_token(w)]\n",
        "\n",
        "    rows, cur_y, cur = [], None, []\n",
        "    for w in sorted(body, key=lambda w: (round(w[\"top\"], 1), w[\"x0\"])):\n",
        "        y = round(w[\"top\"], 1)\n",
        "        if cur_y is None or abs(y - cur_y) <= y_tol:\n",
        "            cur.append(w)\n",
        "            cur_y = y if cur_y is None else (cur_y + y) / 2\n",
        "        else:\n",
        "            rows.append(cur)\n",
        "            cur = [w]\n",
        "            cur_y = y\n",
        "    if cur:\n",
        "        rows.append(cur)\n",
        "\n",
        "    out = []\n",
        "    for r in rows:\n",
        "        buckets = {i: [] for i in range(len(cuts) - 1)}\n",
        "        for w in r:\n",
        "            xmid = (w[\"x0\"] + w[\"x1\"]) / 2\n",
        "            bi = assign_bin(xmid, cuts, edge_pad)\n",
        "            if bi is not None:\n",
        "                buckets[bi].append(w)\n",
        "\n",
        "        vals = [smart_join(buckets[i], x_tol, gap_ratio) for i in range(len(cuts) - 1)]\n",
        "\n",
        "        # If the third column (Month) looks like money and the fourth\n",
        "        # column (Revenue) looks like a month, swap them\n",
        "        if len(vals) >= 4:\n",
        "            m, rv = vals[2].strip(), vals[3].strip()\n",
        "            if (\n",
        "                m and NUM_RE_STRICT.match(m)\n",
        "                and rv and (_is_month_token(rv))\n",
        "            ):\n",
        "                vals[2], vals[3] = rv, m\n",
        "\n",
        "        if sum(1 for v in vals if v not in (\"\", \"-\")) >= drop_min:\n",
        "            out.append(vals)\n",
        "\n",
        "    return pd.DataFrame(out, columns=COLS)\n",
        "\n",
        "# ---------------- Repair helpers ----------------\n",
        "def repair_loc_and_location(df):\n",
        "    mask = df[\"Loc #\"].astype(str).str.match(r\"^\\d+\\s+\\S+\", na=False)\n",
        "    if mask.any():\n",
        "        ex = df.loc[mask, \"Loc #\"].astype(str).str.extract(r\"^(\\d+)\\s+(.*)$\")\n",
        "        df.loc[mask, \"Loc #\"] = ex[0]\n",
        "        df.loc[mask, \"Location\"] = (\n",
        "            ex[1].fillna(\"\").str.strip() + \" \" + df.loc[mask, \"Location\"].fillna(\"\")\n",
        "        ).str.strip().replace({\"\": None})\n",
        "\n",
        "    # Append pure 6-digit codes onto the previous row's Location\n",
        "    for i in range(len(df) - 1, 0, -1):\n",
        "        cur_loc = str(df.at[i, \"Location\"]).strip()\n",
        "        if re.fullmatch(r\"\\d{6}\", cur_loc):\n",
        "            prev_loc = str(df.at[i - 1, \"Location\"]).strip()\n",
        "            if prev_loc:\n",
        "                df.at[i - 1, \"Location\"] = (prev_loc + \" \" + cur_loc).strip()\n",
        "                df.at[i, \"Location\"] = np.nan\n",
        "\n",
        "    for i in range(1, len(df)):\n",
        "        cur_locnum = str(df.at[i, \"Loc #\"]) if pd.notna(df.at[i, \"Loc #\"]) else \"\"\n",
        "        cur_loc = str(df.at[i, \"Location\"]) if pd.notna(df.at[i, \"Location\"]) else \"\"\n",
        "        prev_loc = (\n",
        "            str(df.at[i - 1, \"Location\"]) if pd.notna(df.at[i - 1, \"Location\"]) else \"\"\n",
        "        )\n",
        "        if re.fullmatch(r\"\\d{6}\", cur_locnum) and (cur_loc == \"\" or cur_loc.lower() == \"nan\") and prev_loc not in (\"\", \"nan\"):\n",
        "            df.at[i - 1, \"Location\"] = (prev_loc + \" \" + cur_locnum).strip()\n",
        "            df.at[i, \"Loc #\"] = df.at[i - 1, \"Loc #\"]\n",
        "\n",
        "    return df\n",
        "\n",
        "def split_and_swap_month_revenue(df):\n",
        "    def split_cell(mon, rev):\n",
        "        ms = None if pd.isna(mon) else str(mon).strip()\n",
        "        rs = None if pd.isna(rev) else str(rev).strip()\n",
        "        if ms:\n",
        "            cleaned = ms.replace(\",\", \"\")\n",
        "            toks = re.split(r\"\\s+\", cleaned)\n",
        "            mon_tok = next((t for t in toks if _is_month_token(t)), None)\n",
        "            money_toks = [t for t in toks if not _is_month_token(t)]\n",
        "            money = \"\".join(money_toks)\n",
        "            if mon_tok:\n",
        "                ms = mon_tok\n",
        "            if money and not (rs and NUM_RE_STRICT.match(rs)):\n",
        "                rs = money\n",
        "        if rs and _is_month_token(rs):\n",
        "            if ms and (NUM_RE_STRICT.match(ms) or NUM_RE_STRICT.match(ms.replace(\"$\", \"\"))):\n",
        "                ms, rs = rs, ms\n",
        "        ms = ms if (ms and ms.strip(\"-\")) else None\n",
        "        rs = rs if (rs and rs.strip(\"-\")) else None\n",
        "        return ms, rs\n",
        "\n",
        "    tmp = df.apply(lambda r: split_cell(r[\"Month\"], r[\"Revenue\"]), axis=1, result_type=\"expand\")\n",
        "    df[\"Month\"], df[\"Revenue\"] = tmp[0], tmp[1]\n",
        "    return df\n",
        "\n",
        "def normalize_months(df):\n",
        "    def norm(m):\n",
        "        if pd.isna(m):\n",
        "            return m\n",
        "        s = str(m).strip()\n",
        "        if MONTH_REV.match(s):\n",
        "            parts = re.split(r'[-/]', s)\n",
        "            if len(parts) == 2:\n",
        "                year, mon = parts\n",
        "                mon = mon[:3].capitalize()\n",
        "                return mon + '-' + year\n",
        "        return s\n",
        "\n",
        "    df[\"Month\"] = df[\"Month\"].apply(norm)\n",
        "    return df\n",
        "\n",
        "def _seeded_ffill(series: pd.Series, seed):\n",
        "    s = series.replace({\"\": None}).copy()\n",
        "    if s.empty:\n",
        "        return s\n",
        "    if pd.isna(s.iloc[0]) and seed is not None:\n",
        "        s.iloc[0] = seed\n",
        "    return s.ffill()\n",
        "\n",
        "def pagewise_seeded_ffill(dfp: pd.DataFrame, prev_locnum, prev_loc):\n",
        "    for col in [\"Loc #\", \"Location\"]:\n",
        "        dfp[col] = dfp[col].replace({\"\": None})\n",
        "    dfp[\"Loc #\"] = _seeded_ffill(dfp[\"Loc #\"], prev_locnum)\n",
        "    dfp[\"Location\"] = _seeded_ffill(dfp[\"Location\"], prev_loc)\n",
        "    return dfp\n",
        "\n",
        "def finalize(df):\n",
        "    mask = df[\"Month\"].astype(str).str.match(MONTH_ONLY, na=False) & df[\"Revenue\"].astype(str).isin({str(x) for x in YEAR_FRAG})\n",
        "    df.loc[mask, \"Month\"] = df.loc[mask, \"Month\"] + \"-\" + df.loc[mask, \"Revenue\"].astype(str).str[-2:]\n",
        "    df.loc[mask, \"Revenue\"] = np.nan\n",
        "\n",
        "    df[\"Loc #\"] = df[\"Loc #\"].replace({\"\": None}).ffill()\n",
        "    df[\"Location\"] = df[\"Location\"].replace({\"\": None}).ffill()\n",
        "\n",
        "    for c in [\"Revenue\", \"NAFI Amt\", \"Annual Revenue\", \"Annual NAFI\"]:\n",
        "        df[c] = (\n",
        "            df[c].astype(str)\n",
        "            .str.replace(\" \", \"\", regex=False)\n",
        "            .str.replace(\",\", \"\", regex=False)\n",
        "            .str.replace(\"$\", \"\", regex=False)\n",
        "            .str.replace(\"(\", \"-\", regex=False)\n",
        "            .str.replace(\")\", \"\", regex=False)\n",
        "        )\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    return df.dropna(how=\"all\").reset_index(drop=True)\n",
        "\n",
        "def monthly_numeric_left_pack(df):\n",
        "    num_cols = [\"Revenue\", \"NAFI Amt\", \"Annual Revenue\", \"Annual NAFI\"]\n",
        "\n",
        "    def is_monthlike(x):\n",
        "        if pd.isna(x):\n",
        "            return False\n",
        "        s = str(x).strip()\n",
        "        return bool(MONTH_FULL.match(s) or MONTH_REV.match(s) or MONTH_ONLY.match(s))\n",
        "\n",
        "    def pack_row(r):\n",
        "        if not is_monthlike(r.get(\"Month\")):\n",
        "            return r\n",
        "        vals = [r[c] for c in num_cols]\n",
        "        avail = [v for v in vals if pd.notna(v)]\n",
        "        if avail:\n",
        "            r[\"Revenue\"] = avail[0]\n",
        "            r[\"NAFI Amt\"] = avail[1] if len(avail) > 1 else np.nan\n",
        "            r[\"Annual Revenue\"] = avail[2] if len(avail) > 2 else np.nan\n",
        "            r[\"Annual NAFI\"] = avail[3] if len(avail) > 3 else np.nan\n",
        "        return r\n",
        "\n",
        "    return df.apply(pack_row, axis=1)\n",
        "\n",
        "def fill_missing_annual(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "\n",
        "    month_map = {\n",
        "        m: i\n",
        "        for i, m in enumerate(\n",
        "            [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"], start=1\n",
        "        )\n",
        "    }\n",
        "\n",
        "    def parse_date(m):\n",
        "        if pd.isna(m):\n",
        "            return None\n",
        "        m = str(m)\n",
        "        if re.match(r\"^[A-Za-z]{3}-\\d{2}$\", m):\n",
        "            mon, yr = m.split('-')\n",
        "            return (2000 + int(yr), month_map.get(mon[:3], 0))\n",
        "        return None\n",
        "\n",
        "    df[\"_ym\"] = df[\"Month\"].apply(parse_date)\n",
        "\n",
        "    for (locnum, loc), idxs in df.groupby([\"Loc #\", \"Location\"], dropna=False).groups.items():\n",
        "        idxs = list(idxs)\n",
        "        sub = df.loc[idxs].copy()\n",
        "        sub[\"_ym_key\"] = sub[\"_ym\"].apply(lambda t: t[0] * 100 + t[1] if t else None)\n",
        "        sub = sub.sort_values(\"_ym_key\")\n",
        "\n",
        "        revenues = sub[\"Revenue\"].fillna(0).to_list()\n",
        "        nafis = sub[\"NAFI Amt\"].fillna(0).to_list()\n",
        "\n",
        "        for i, (idx, row) in enumerate(sub.iterrows()):\n",
        "            ym = row[\"_ym\"]\n",
        "            if (\n",
        "                (pd.isna(row[\"Annual Revenue\"]) or pd.isna(row[\"Annual NAFI\"]))\n",
        "                and ym\n",
        "                and ym[1] == 9\n",
        "            ):\n",
        "                start = max(0, i - 11)\n",
        "                s_rev = sum(revenues[start : i + 1])\n",
        "                s_naf = sum(nafis[start : i + 1])\n",
        "                if pd.isna(row[\"Annual Revenue\"]):\n",
        "                    df.at[idx, \"Annual Revenue\"] = s_rev\n",
        "                if pd.isna(row[\"Annual NAFI\"]):\n",
        "                    df.at[idx, \"Annual NAFI\"] = s_naf\n",
        "\n",
        "    df.drop(columns=[\"_ym\"], inplace=True, errors=\"ignore\")\n",
        "    df.drop(columns=[\"_ym_key\"], inplace=True, errors=\"ignore\")\n",
        "    return df\n",
        "\n",
        "# ---- Default base cut lines and allowed clamp deltas ----\n",
        "BASE_CUTS = [-1e9, 156.21, 214.20, 269.19, 327.93, 393.67, 460.17, 1e9]\n",
        "CLAMP_DELTA = [0, 30, 40, 40, 30, 30, 30, 0]\n",
        "\n",
        "def apply_page_params(page_num):\n",
        "    \"\"\"Determine parameter overrides for a given page.\n",
        "    Returns a dictionary of parameters to use when processing ``page_num``.\n",
        "    Values not specified in an override fall back to the base values.\n",
        "    \"\"\"\n",
        "    params = {\n",
        "        \"Y_TOL\": BASE_Y_TOL,\n",
        "        \"X_JOIN\": BASE_X_JOIN,\n",
        "        \"GAP_RATIO\": BASE_GAP_RATIO,\n",
        "        \"DROP_MIN\": BASE_DROP_MIN,\n",
        "        \"EDGE_PAD\": BASE_EDGE_PAD,\n",
        "        \"LEFT_SHIFT_MONTH\": BASE_LEFT_SHIFT_MONTH,\n",
        "        \"LEFT_SHIFT_REVENUE\": BASE_LEFT_SHIFT_REVENUE,\n",
        "    }\n",
        "    # find any matching override range\n",
        "    for rng, overrides in SPECIAL_PARAMS.items():\n",
        "        if page_num in rng:\n",
        "            params.update(overrides)\n",
        "            break\n",
        "    return params\n",
        "\n",
        "# ---------------- Main processing routine ----------------\n",
        "def main():\n",
        "    all_pages = []\n",
        "    prev_loc = None\n",
        "    prev_locnum = None\n",
        "    prev_cuts = None\n",
        "\n",
        "    with pdfplumber.open(PDF) as pdf:\n",
        "        last = min(202, len(pdf.pages))\n",
        "\n",
        "        # reference header y from page 2 (index 1) if needed\n",
        "        words2 = chars_to_words(pdf.pages[1], x_tol=BASE_X_JOIN, y_tol=BASE_Y_TOL)\n",
        "        header2_y = header_y_of(words2)\n",
        "\n",
        "        # skip pages used for slot revenue tables\n",
        "        skip_pages = {1, 35, 73, 115, 157}\n",
        "\n",
        "        # iterate pages starting from 2\n",
        "        for page_num in range(2, last + 1):\n",
        "            if page_num in skip_pages:\n",
        "                print(f\"[p{page_num}] SKIPPED.\")\n",
        "                continue\n",
        "\n",
        "            # Determine parameter set for this page\n",
        "            params = apply_page_params(page_num)\n",
        "            y_tol = params[\"Y_TOL\"]\n",
        "            x_tol = params[\"X_JOIN\"]\n",
        "            gap_ratio = params[\"GAP_RATIO\"]\n",
        "            drop_min = params[\"DROP_MIN\"]\n",
        "            edge_pad = params[\"EDGE_PAD\"]\n",
        "            left_shift_month = params[\"LEFT_SHIFT_MONTH\"]\n",
        "            left_shift_revenue = params[\"LEFT_SHIFT_REVENUE\"]\n",
        "\n",
        "            page = pdf.pages[page_num - 1]\n",
        "            words = chars_to_words(page, x_tol=x_tol, y_tol=y_tol)\n",
        "\n",
        "            # derive cuts from header; if no header found use previous or base\n",
        "            cuts, hy = cuts_from_header(\n",
        "                words,\n",
        "                BASE_CUTS,\n",
        "                prev_cuts=prev_cuts,\n",
        "                left_shift_month=left_shift_month,\n",
        "                left_shift_revenue=left_shift_revenue,\n",
        "            )\n",
        "            hy = hy or header2_y\n",
        "\n",
        "            # refine cuts to account for numbers crossing boundaries\n",
        "            cuts = refine_cuts_typeaware(page, cuts, hy)\n",
        "\n",
        "            # split page into rows\n",
        "            dfp = rows_from_page(\n",
        "                page,\n",
        "                cuts,\n",
        "                hy,\n",
        "                x_tol=x_tol,\n",
        "                y_tol=y_tol,\n",
        "                edge_pad=edge_pad,\n",
        "                gap_ratio=gap_ratio,\n",
        "                drop_min=drop_min,\n",
        "            )\n",
        "\n",
        "            if dfp.empty:\n",
        "                print(f\"[p{page_num}] rows=0\")\n",
        "                continue\n",
        "\n",
        "            # repair fields and normalize months\n",
        "            dfp = repair_loc_and_location(dfp)\n",
        "            dfp = split_and_swap_month_revenue(dfp)\n",
        "            dfp = normalize_months(dfp)\n",
        "            dfp = pagewise_seeded_ffill(dfp, prev_locnum, prev_loc)\n",
        "            dfp = finalize(dfp)\n",
        "            dfp = monthly_numeric_left_pack(dfp)\n",
        "\n",
        "            # update seeds for cross-page continuity\n",
        "            if dfp[\"Location\"].notna().any():\n",
        "                prev_loc = dfp[\"Location\"].dropna().iloc[-1]\n",
        "            if dfp[\"Loc #\"].notna().any():\n",
        "                prev_locnum = dfp[\"Loc #\"].dropna().iloc[-1]\n",
        "            prev_cuts = cuts\n",
        "\n",
        "            # annotate with original page number\n",
        "            dfp.insert(0, \"Page\", page_num)\n",
        "            all_pages.append(dfp)\n",
        "            print(f\"[p{page_num}] rows={len(dfp)}\")\n",
        "\n",
        "    if all_pages:\n",
        "        out = pd.concat(all_pages, ignore_index=True)\n",
        "        # Do not automatically fill annual totals; respect original numbers\n",
        "        out.to_csv(OUT, index=False)\n",
        "        print(f\"✅ Done. rows={len(out)} → {OUT}\")\n",
        "    else:\n",
        "        print(\"No data\")\n",
        "\n",
        "# 1) Set the PDF path (if the file is under /content)\n",
        "PDF = Path('/content/Marine_Revenue_FY20-FY24.pdf')\n",
        "OUT = PDF.parent / 'Marine_Revenue_FY20-FY24_detail.csv'\n",
        "\n",
        "# 2) Override global variables, then run main()\n",
        "globals()['PDF'] = PDF\n",
        "globals()['OUT'] = OUT\n",
        "print('PDF =', PDF)\n",
        "print('OUT =', OUT)\n",
        "main()\n"
      ],
      "metadata": {
        "id": "VWmA04jVrb6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a077c802-725f-47b7-aec3-c04caf7dea03"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PDF = /content/Marine_Revenue_FY20-FY24.pdf\n",
            "OUT = /content/Marine_Revenue_FY20-FY24_detail.csv\n",
            "[p2] rows=14\n",
            "[p3] rows=38\n",
            "[p4] rows=20\n",
            "[p5] rows=41\n",
            "[p6] rows=43\n",
            "[p7] rows=29\n",
            "[p8] rows=41\n",
            "[p9] rows=43\n",
            "[p10] rows=8\n",
            "[p11] rows=40\n",
            "[p12] rows=42\n",
            "[p13] rows=47\n",
            "[p14] rows=39\n",
            "[p15] rows=44\n",
            "[p16] rows=41\n",
            "[p17] rows=44\n",
            "[p18] rows=41\n",
            "[p19] rows=44\n",
            "[p20] rows=38\n",
            "[p21] rows=40\n",
            "[p22] rows=44\n",
            "[p23] rows=33\n",
            "[p24] rows=39\n",
            "[p25] rows=4\n",
            "[p26] rows=26\n",
            "[p27] rows=41\n",
            "[p28] rows=42\n",
            "[p29] rows=39\n",
            "[p30] rows=17\n",
            "[p31] rows=39\n",
            "[p32] rows=38\n",
            "[p33] rows=36\n",
            "[p34] rows=10\n",
            "[p35] SKIPPED.\n",
            "[p36] rows=14\n",
            "[p37] rows=40\n",
            "[p38] rows=30\n",
            "[p39] rows=36\n",
            "[p40] rows=38\n",
            "[p41] rows=37\n",
            "[p42] rows=15\n",
            "[p43] rows=44\n",
            "[p44] rows=47\n",
            "[p45] rows=21\n",
            "[p46] rows=38\n",
            "[p47] rows=42\n",
            "[p48] rows=41\n",
            "[p49] rows=44\n",
            "[p50] rows=25\n",
            "[p51] rows=44\n",
            "[p52] rows=43\n",
            "[p53] rows=42\n",
            "[p54] rows=44\n",
            "[p55] rows=40\n",
            "[p56] rows=44\n",
            "[p57] rows=40\n",
            "[p58] rows=40\n",
            "[p59] rows=44\n",
            "[p60] rows=32\n",
            "[p61] rows=41\n",
            "[p62] rows=16\n",
            "[p63] rows=26\n",
            "[p64] rows=40\n",
            "[p65] rows=44\n",
            "[p66] rows=11\n",
            "[p67] rows=41\n",
            "[p68] rows=31\n",
            "[p69] rows=41\n",
            "[p70] rows=38\n",
            "[p71] rows=40\n",
            "[p72] rows=13\n",
            "[p73] SKIPPED.\n",
            "[p74] rows=14\n",
            "[p75] rows=38\n",
            "[p76] rows=35\n",
            "[p77] rows=8\n",
            "[p78] rows=36\n",
            "[p79] rows=38\n",
            "[p80] rows=37\n",
            "[p81] rows=27\n",
            "[p82] rows=41\n",
            "[p83] rows=44\n",
            "[p84] rows=30\n",
            "[p85] rows=22\n",
            "[p86] rows=41\n",
            "[p87] rows=45\n",
            "[p88] rows=44\n",
            "[p89] rows=47\n",
            "[p90] rows=38\n",
            "[p91] rows=44\n",
            "[p92] rows=43\n",
            "[p93] rows=42\n",
            "[p94] rows=44\n",
            "[p95] rows=40\n",
            "[p96] rows=44\n",
            "[p97] rows=42\n",
            "[p98] rows=42\n",
            "[p99] rows=40\n",
            "[p100] rows=43\n",
            "[p101] rows=33\n",
            "[p102] rows=42\n",
            "[p103] rows=32\n",
            "[p104] rows=26\n",
            "[p105] rows=40\n",
            "[p106] rows=44\n",
            "[p107] rows=23\n",
            "[p108] rows=39\n",
            "[p109] rows=29\n",
            "[p110] rows=39\n",
            "[p111] rows=38\n",
            "[p112] rows=37\n",
            "[p113] rows=36\n",
            "[p114] rows=11\n",
            "[p115] SKIPPED.\n",
            "[p116] rows=14\n",
            "[p117] rows=39\n",
            "[p118] rows=36\n",
            "[p119] rows=20\n",
            "[p120] rows=40\n",
            "[p121] rows=42\n",
            "[p122] rows=41\n",
            "[p123] rows=28\n",
            "[p124] rows=44\n",
            "[p125] rows=47\n",
            "[p126] rows=34\n",
            "[p127] rows=31\n",
            "[p128] rows=38\n",
            "[p129] rows=42\n",
            "[p130] rows=37\n",
            "[p131] rows=45\n",
            "[p132] rows=42\n",
            "[p133] rows=32\n",
            "[p134] rows=53\n",
            "[p135] rows=52\n",
            "[p136] rows=51\n",
            "[p137] rows=52\n",
            "[p138] rows=50\n",
            "[p139] rows=53\n",
            "[p140] rows=47\n",
            "[p141] rows=49\n",
            "[p142] rows=52\n",
            "[p143] rows=30\n",
            "[p144] rows=50\n",
            "[p145] rows=41\n",
            "[p146] rows=26\n",
            "[p147] rows=40\n",
            "[p148] rows=44\n",
            "[p149] rows=35\n",
            "[p150] rows=41\n",
            "[p151] rows=31\n",
            "[p152] rows=41\n",
            "[p153] rows=40\n",
            "[p154] rows=39\n",
            "[p155] rows=40\n",
            "[p156] rows=22\n",
            "[p157] SKIPPED.\n",
            "[p158] rows=14\n",
            "[p159] rows=38\n",
            "[p160] rows=35\n",
            "[p161] rows=33\n",
            "[p162] rows=5\n",
            "[p163] rows=44\n",
            "[p164] rows=45\n",
            "[p165] rows=45\n",
            "[p166] rows=33\n",
            "[p167] rows=41\n",
            "[p168] rows=44\n",
            "[p169] rows=41\n",
            "[p170] rows=24\n",
            "[p171] rows=19\n",
            "[p172] rows=41\n",
            "[p173] rows=45\n",
            "[p174] rows=31\n",
            "[p175] rows=48\n",
            "[p176] rows=45\n",
            "[p177] rows=38\n",
            "[p178] rows=53\n",
            "[p179] rows=52\n",
            "[p180] rows=46\n",
            "[p181] rows=53\n",
            "[p182] rows=48\n",
            "[p183] rows=49\n",
            "[p184] rows=51\n",
            "[p185] rows=48\n",
            "[p186] rows=49\n",
            "[p187] rows=50\n",
            "[p188] rows=33\n",
            "[p189] rows=50\n",
            "[p190] rows=33\n",
            "[p191] rows=26\n",
            "[p192] rows=41\n",
            "[p193] rows=46\n",
            "[p194] rows=40\n",
            "[p195] rows=39\n",
            "[p196] rows=29\n",
            "[p197] rows=39\n",
            "[p198] rows=38\n",
            "[p199] rows=34\n",
            "[p200] rows=37\n",
            "[p201] rows=39\n",
            "[p202] rows=16\n",
            "✅ Done. rows=7309 → /content/Marine_Revenue_FY20-FY24_detail.csv\n"
          ]
        }
      ]
    }
  ]
}