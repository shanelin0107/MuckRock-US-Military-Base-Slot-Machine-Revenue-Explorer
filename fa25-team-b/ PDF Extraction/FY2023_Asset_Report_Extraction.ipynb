{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -y\n",
        "!apt-get install -y --fix-missing poppler-utils\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq poppler-utils\n",
        "!pip install docling-parse docling-core rapidfuzz docling\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEVP1rPSl0G7",
        "outputId": "64ea569c-97c0-4ed1-95f4-d7959cc8d6d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.2 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,411 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,963 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,855 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,161 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,520 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,822 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Fetched 35.1 MB in 8s (4,143 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 47 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "0% [Working]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "roEY5jLFYXoh"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# I’ve uploaded the file to Google Drive, and this code is used to read it.\n",
        "url = 'https://drive.google.com/uc?export=download&id=1QtmW3UhIMXDLQoI3wI7zH88SACFurjcR'\n",
        "output = 'FY2023_Asset_Report.pdf'\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =============================================================================\n",
        "# Common Regular Expressions\n",
        "# =============================================================================\n",
        "DATE_RE = re.compile(r'^(?:\\d{1,2}/\\d{1,2}/\\d{2,4}|\\d{4}-\\d{1,2}-\\d{1,2})$')\n",
        "NUM_RE  = re.compile(r'^[()\\-]?\\d{1,3}(?:,\\d{3})*(?:\\.\\d+)?$')\n",
        "\n",
        "# Allowed keywords for page-start Region (common in Asset Details sections)\n",
        "REGION_PREFIX = re.compile(\n",
        "    r'^(Europe|Japan|Korea|Okinawa|Pacific|United(?:\\s+States)?)\\b',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "# Serial pattern: letters/numbers/-\n",
        "SERIAL_RE = re.compile(r'(?i)^[a-z0-9-]+$')\n",
        "\n",
        "# =============================================================================\n",
        "# Helper: safe concat (filter out empty/all-NA DataFrames to avoid dtype warnings/behavior differences)\n",
        "# =============================================================================\n",
        "def _safe_concat(frames: List[pd.DataFrame]) -> pd.DataFrame:\n",
        "    if not frames:\n",
        "        return pd.DataFrame()\n",
        "    kept: List[pd.DataFrame] = []\n",
        "    for df in frames:\n",
        "        if df is None or df.empty:\n",
        "            continue\n",
        "        if df.dropna(how='all').empty:\n",
        "            continue\n",
        "        kept.append(df)\n",
        "    return pd.concat(kept, ignore_index=True) if kept else pd.DataFrame()\n",
        "\n",
        "# =============================================================================\n",
        "# A) —— Three Summary Tables (reuse code 1 logic; renamed to avoid conflicts)\n",
        "# =============================================================================\n",
        "def get_month_v1(page_text: str) -> Optional[str]:\n",
        "    m = re.search(r'for month of\\s+([A-Za-z]+)\\s+(\\d{4})', page_text, re.IGNORECASE)\n",
        "    return f\"{m.group(1).capitalize()} {m.group(2)}\" if m else None\n",
        "\n",
        "\n",
        "def parse_region_and_field_office_v1(lines: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Code 1 version: Region and Field Office (more complete, used with v1 flow)\"\"\"\n",
        "    region_df = pd.DataFrame()\n",
        "    field_df = pd.DataFrame()\n",
        "\n",
        "    # ----- Region summary -----\n",
        "    idx_slots = next((i for i, l in enumerate(lines) if l.strip().startswith('Slots Only')), None)\n",
        "    if idx_slots is not None:\n",
        "        region_rows: List[List] = []\n",
        "        i = idx_slots + 1\n",
        "        while i < len(lines):\n",
        "            line = lines[i].strip()\n",
        "            if not line:\n",
        "                i += 1\n",
        "                continue\n",
        "            if (\n",
        "                line.startswith('Locations by Service') or\n",
        "                'Assets by Field Office' in line or\n",
        "                'EGMs by Field Office' in line or\n",
        "                'Installed Assets by Location' in line\n",
        "            ):\n",
        "                break\n",
        "            parts = [p for p in re.split(r'\\s{2,}', line) if p]\n",
        "            if len(parts) >= 5:\n",
        "                region_name = parts[0]\n",
        "                values: List = []\n",
        "                for p in parts[1:]:\n",
        "                    x = p.replace('%', '')\n",
        "                    if x in ('-', ''):\n",
        "                        values.append(np.nan)\n",
        "                    else:\n",
        "                        try:\n",
        "                            values.append(float(x.replace(',', '')))\n",
        "                        except Exception:\n",
        "                            values.append(x)\n",
        "                region_rows.append([region_name] + values)\n",
        "            i += 1\n",
        "        if region_rows:\n",
        "            maxlen = max(len(r) for r in region_rows)\n",
        "            for r in region_rows:\n",
        "                while len(r) < maxlen:\n",
        "                    r.append(np.nan)\n",
        "            cols = ['Region', '#Locations', 'Army', 'Navy', 'Marine_Corps', 'Airforce', 'Total', 'Percent']\n",
        "            region_df = pd.DataFrame(region_rows, columns=cols[:maxlen])\n",
        "\n",
        "    # ----- Field office summary -----\n",
        "    field_rows: List[List] = []\n",
        "    start = False\n",
        "    current_region: Optional[str] = None\n",
        "    for raw in lines:\n",
        "        line = raw.rstrip(' ')\n",
        "        if 'Assets by Field Office' in line or 'EGMs by Field Office' in line:\n",
        "            start = True\n",
        "            continue\n",
        "        if not start:\n",
        "            continue\n",
        "        if not line.strip() or line.strip().startswith('Slots'):\n",
        "            continue\n",
        "        if line.strip() and not re.search(r'\\d', line.strip()):\n",
        "            current_region = line.strip()\n",
        "            continue\n",
        "        parts = [p for p in re.split(r'\\s{2,}', line.strip()) if p]\n",
        "        if len(parts) >= 6 and all(re.match(r'^[()0-9,.-]+$', p) for p in parts[-5:]):\n",
        "            fo_number = parts[0]\n",
        "            location_name = ' '.join(parts[1:-5])\n",
        "            values: List[float] = []\n",
        "            for p in parts[-5:]:\n",
        "                if p == '-':\n",
        "                    values.append(np.nan)\n",
        "                else:\n",
        "                    v = p\n",
        "                    if v.startswith('(') and v.endswith(')'):\n",
        "                        v = '-' + v[1:-1]\n",
        "                    values.append(float(v.replace(',', '')))\n",
        "            field_rows.append([current_region, fo_number, location_name] + values)\n",
        "    if field_rows:\n",
        "        field_df = pd.DataFrame(field_rows, columns=[\n",
        "            'Region', 'FO#', 'Location', 'Slots', 'ACM_CountR', 'ITC', 'FRS', 'Total'\n",
        "        ])\n",
        "    return region_df, field_df\n",
        "\n",
        "\n",
        "def parse_installed_assets_v1(lines: List[str]) -> pd.DataFrame:\n",
        "    \"\"\"Code 1 version: Installed Assets (outputs both Total_PDF and Total_Computed)\"\"\"\n",
        "    rows: List[List] = []\n",
        "    started = False\n",
        "    for line in lines:\n",
        "        if not started:\n",
        "            if ('Installed Assets by Location' in line) or (('FO #' in line) and ('IGT' in line)):\n",
        "                started = True\n",
        "            continue\n",
        "        if not line.strip() or not re.search(r'\\d', line):\n",
        "            continue\n",
        "        if ('Tot/EGMs' in line) or (('NOV' in line) and ('AIN' in line)):\n",
        "            continue\n",
        "        parts = [p.strip() for p in re.split(r'\\s{2,}', line) if p.strip()]\n",
        "        if len(parts) < 5:\n",
        "            continue\n",
        "        name = parts[0]\n",
        "        idx = 1\n",
        "        fo_number: Optional[str] = None\n",
        "        if idx < len(parts) and re.match(r'^\\d+$', parts[idx]):\n",
        "            fo_number = parts[idx]\n",
        "            idx += 1\n",
        "        loc = parts[idx] if idx < len(parts) else ''\n",
        "        idx += 1\n",
        "        svc = parts[idx] if idx < len(parts) else ''\n",
        "        idx += 1\n",
        "        if not re.search(r'[A-Za-z]', svc):\n",
        "            continue\n",
        "        metric_tokens: List[str] = []\n",
        "        for token in parts[idx:]:\n",
        "            metric_tokens += token.split()\n",
        "        if len(metric_tokens) < 8:\n",
        "            continue\n",
        "        manuf_vals: List[Optional[float]] = []\n",
        "        for t in metric_tokens[:7]:\n",
        "            if t == '-':\n",
        "                manuf_vals.append(None)\n",
        "            else:\n",
        "                try:\n",
        "                    manuf_vals.append(float(t.replace(',', '')))\n",
        "                except Exception:\n",
        "                    manuf_vals.append(None)\n",
        "\n",
        "        def _num(tok: Optional[str]) -> Optional[float]:\n",
        "            if tok is None or tok in ('-', ''):\n",
        "                return None\n",
        "            try:\n",
        "                return float(tok.replace(',', ''))\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "        tot_egms = _num(metric_tokens[7] if len(metric_tokens) > 7 else None)\n",
        "        frs = _num(metric_tokens[8] if len(metric_tokens) > 8 else None)\n",
        "        acm = _num(metric_tokens[9] if len(metric_tokens) > 9 else None)\n",
        "\n",
        "        itc: Optional[float] = None\n",
        "        total_pdf: Optional[float] = None\n",
        "        remaining = metric_tokens[10:]\n",
        "        if remaining:\n",
        "            total_pdf = _num(remaining[-1])\n",
        "            for t in remaining[:-1]:\n",
        "                cand = _num(t)\n",
        "                if cand is not None:\n",
        "                    itc = cand\n",
        "                    break\n",
        "\n",
        "        total_computed = sum(x for x in [tot_egms, frs, acm, itc] if x is not None)\n",
        "        rows.append([\n",
        "            name, fo_number, loc, svc\n",
        "        ] + manuf_vals + [\n",
        "            tot_egms, frs, acm, itc, total_pdf, total_computed\n",
        "        ])\n",
        "    if rows:\n",
        "        columns = [\n",
        "            'LocationName', 'FO#', 'Loc', 'Svc',\n",
        "            'NOV', 'AIN', 'IGT', 'WMS', 'BAL', 'KON', 'ITE',\n",
        "            'Tot_EGMs', 'FRS', 'ACM', 'ITC', 'Total_PDF', 'Total_Computed'\n",
        "        ]\n",
        "        return pd.DataFrame(rows, columns=columns)\n",
        "    return pd.DataFrame()\n",
        "\n",
        "\n",
        "def extract_region_field_installed_v1(pdf_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Code 1 version extraction of three tables: one pass to text, split by page, carry forward Month, then concat.\n",
        "    Returns: region_df, field_df, installed_df (all with Month column when possible)\n",
        "    \"\"\"\n",
        "    text = subprocess.check_output(['pdftotext', '-layout', pdf_path, '-'], text=True)\n",
        "    pages = text.split('\\f')\n",
        "    region_frames: List[pd.DataFrame] = []\n",
        "    field_frames: List[pd.DataFrame] = []\n",
        "    installed_frames: List[pd.DataFrame] = []\n",
        "    current_month: Optional[str] = None\n",
        "\n",
        "    for page_text in pages:\n",
        "        if not page_text.strip():\n",
        "            continue\n",
        "        month = get_month_v1(page_text)\n",
        "        if month:\n",
        "            current_month = month\n",
        "\n",
        "        lines = page_text.split('\\n')\n",
        "        page_upper = page_text.upper()\n",
        "        has_region = ('ASSETS BY REGION' in page_upper) or ('EGMS BY REGION' in page_upper)\n",
        "        has_field  = ('ASSETS BY FIELD OFFICE' in page_upper) or ('EGMS BY FIELD OFFICE' in page_upper)\n",
        "        has_inst   = ('INSTALLED ASSETS BY LOCATION' in page_upper)\n",
        "\n",
        "        if has_region or has_field:\n",
        "            rdf, fdf = parse_region_and_field_office_v1(lines)\n",
        "            if not rdf.empty:\n",
        "                if current_month:\n",
        "                    rdf = rdf.assign(Month=current_month)\n",
        "                region_frames.append(rdf)\n",
        "            if not fdf.empty:\n",
        "                if current_month:\n",
        "                    fdf = fdf.assign(Month=current_month)\n",
        "                field_frames.append(fdf)\n",
        "\n",
        "        if has_inst:\n",
        "            inst_df = parse_installed_assets_v1(lines)\n",
        "            if not inst_df.empty:\n",
        "                if current_month:\n",
        "                    inst_df = inst_df.assign(Month=current_month)\n",
        "                installed_frames.append(inst_df)\n",
        "\n",
        "    # Use safe concat to avoid FutureWarning and dtype inference ambiguity\n",
        "    region_df    = _safe_concat(region_frames)\n",
        "    field_df     = _safe_concat(field_frames)\n",
        "    installed_df = _safe_concat(installed_frames)\n",
        "    return region_df, field_df, installed_df\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# B) —— Month Detection (enhanced)\n",
        "# =============================================================================\n",
        "def detect_month_map(pages: List[str]) -> Dict[int, str]:\n",
        "    patterns = [\n",
        "        re.compile(r'Assets by Region,\\s*Service\\s+for month of\\s+([A-Za-z]+\\s+\\d{4})', re.I),\n",
        "        re.compile(r'EGMs by Region,\\s*Service\\s+for month of\\s+([A-Za-z]+\\s+\\d{4})', re.I),\n",
        "        re.compile(r'for month of\\s+([A-Za-z]+\\s+\\d{4})', re.I),  # fallback\n",
        "    ]\n",
        "    month_start_pages: List[Tuple[int, str]] = []\n",
        "    seen: set = set()\n",
        "    for i, page in enumerate(pages, start=1):\n",
        "        for pat in patterns:\n",
        "            m = pat.search(page)\n",
        "            if m:\n",
        "                month = m.group(1).strip()\n",
        "                if month not in seen:\n",
        "                    month_start_pages.append((i, month))\n",
        "                    seen.add(month)\n",
        "                break\n",
        "\n",
        "    if not month_start_pages:\n",
        "        return {}\n",
        "\n",
        "    month_ranges: List[Tuple[int, int, str]] = []\n",
        "    for idx, (start, month) in enumerate(month_start_pages):\n",
        "        end = month_start_pages[idx + 1][0] - 1 if idx + 1 < len(month_start_pages) else len(pages)\n",
        "        month_ranges.append((start, end, month))\n",
        "\n",
        "    month_map: Dict[int, str] = {}\n",
        "    for s, e, mth in month_ranges:\n",
        "        for p in range(s, e + 1):\n",
        "            month_map[p] = mth\n",
        "    return month_map\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# C) —— Detailed Tables (fixing date-format issues)\n",
        "# =============================================================================\n",
        "def parse_asset_details_page(page_text: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"\n",
        "    Fixed version: handle non-standard date formats\n",
        "    - Adds stricter date validation to exclude Excel serial-like values such as 39681\n",
        "    - For values that are not valid dates, leave Acquire and Effective empty\n",
        "    - Other fields are extracted as usual\n",
        "    \"\"\"\n",
        "    out: List[Dict[str, str]] = []\n",
        "\n",
        "    # Stricter date validation\n",
        "    def is_valid_date(s: str) -> bool:\n",
        "        \"\"\"Check whether a string is a valid date format\"\"\"\n",
        "        if not s:\n",
        "            return False\n",
        "        # Basic format check\n",
        "        if not DATE_RE.match(s):\n",
        "            return False\n",
        "        # Exclude pure digits (e.g., Excel serial-like 39681)\n",
        "        if re.match(r'^\\d{5,6}$', s):\n",
        "            return False\n",
        "        # Require separator '/' or '-'\n",
        "        if '/' not in s and '-' not in s:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    for raw in page_text.split('\\n'):\n",
        "        line = raw.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if not REGION_PREFIX.match(line):\n",
        "            continue\n",
        "        toks = line.split()\n",
        "        if len(toks) < 8:\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            # Fixed parsing on the left side\n",
        "            region = toks[0]\n",
        "            fonum = toks[1]\n",
        "            i = 2\n",
        "            while i < len(toks) and not toks[i].isdigit():\n",
        "                i += 1\n",
        "            foshort = ' '.join(toks[2:i]).strip()\n",
        "            if i >= len(toks) or not toks[i].isdigit():\n",
        "                continue\n",
        "            loc = toks[i]\n",
        "            i += 1\n",
        "            j = i\n",
        "            while j < len(toks) and not re.fullmatch(r'\\d{4,6}', toks[j]):\n",
        "                j += 1\n",
        "            if j >= len(toks):\n",
        "                continue\n",
        "            lname = ' '.join(toks[i:j]).strip()\n",
        "            asset = toks[j]\n",
        "            j += 1\n",
        "            if j >= len(toks):\n",
        "                continue\n",
        "            clazz = toks[j]\n",
        "            j += 1\n",
        "\n",
        "            remaining = toks[j:]\n",
        "\n",
        "            # Find relative indices of valid dates within remaining\n",
        "            date_rel_idx = [k for k, t in enumerate(remaining) if is_valid_date(t)]\n",
        "\n",
        "            desc = ''\n",
        "            type_tok = ''\n",
        "            acquire = ''\n",
        "            effective = ''\n",
        "            disposed = ''\n",
        "            serial = ''\n",
        "            age = ''\n",
        "            years_in_storage = ''\n",
        "\n",
        "            if date_rel_idx:\n",
        "                # With valid dates: use the first date to anchor Type\n",
        "                first_rel = date_rel_idx[0]\n",
        "                if first_rel > 0:\n",
        "                    type_tok = remaining[first_rel - 1]\n",
        "                # Collect consecutive valid dates\n",
        "                rel_i = first_rel\n",
        "                dates: List[str] = []\n",
        "                while rel_i < len(remaining) and is_valid_date(remaining[rel_i]):\n",
        "                    dates.append(remaining[rel_i])\n",
        "                    rel_i += 1\n",
        "                # Fill only valid dates\n",
        "                acquire = dates[0] if len(dates) > 0 else ''\n",
        "                effective = dates[1] if len(dates) > 1 else ''\n",
        "                disposed = dates[2] if len(dates) > 2 else ''\n",
        "                # Serial: right after the date cluster if it matches pattern\n",
        "                if rel_i < len(remaining) and SERIAL_RE.match(remaining[rel_i]):\n",
        "                    serial = remaining[rel_i]\n",
        "                    rel_i += 1\n",
        "                # Desc: up to Type if available\n",
        "                desc_end = first_rel - 1 if type_tok else first_rel\n",
        "                desc = ' '.join(remaining[:desc_end]).strip()\n",
        "                # Tail\n",
        "                tail = remaining[rel_i:]\n",
        "            else:\n",
        "                # No valid date present\n",
        "                # Check for pseudo dates like 39681\n",
        "                pseudo_date_idx = [k for k, t in enumerate(remaining) if re.match(r'^\\d{5,6}$', t)]\n",
        "\n",
        "                if pseudo_date_idx:\n",
        "                    # Found pseudo dates; follow similar logic but do not fill date fields\n",
        "                    first_pseudo = pseudo_date_idx[0]\n",
        "                    if first_pseudo > 0:\n",
        "                        type_tok = remaining[first_pseudo - 1]\n",
        "                    # Skip all consecutive pseudo-date tokens\n",
        "                    rel_i = first_pseudo\n",
        "                    while rel_i < len(remaining) and re.match(r'^\\d{5,6}$', remaining[rel_i]):\n",
        "                        rel_i += 1\n",
        "                    # Serial after pseudo dates\n",
        "                    if rel_i < len(remaining) and SERIAL_RE.match(remaining[rel_i]):\n",
        "                        serial = remaining[rel_i]\n",
        "                        rel_i += 1\n",
        "                    # Desc\n",
        "                    desc_end = first_pseudo - 1 if type_tok else first_pseudo\n",
        "                    desc = ' '.join(remaining[:desc_end]).strip()\n",
        "                    # Tail\n",
        "                    tail = remaining[rel_i:]\n",
        "                else:\n",
        "                    # No date-like numbers at all\n",
        "                    desc = ' '.join(remaining).strip()\n",
        "                    # Check if ends with 'Store'\n",
        "                    if remaining and remaining[-1].lower() == 'store':\n",
        "                        type_tok = remaining[-1]\n",
        "                        if len(remaining) > 1 and SERIAL_RE.match(remaining[-2]):\n",
        "                            serial = remaining[-2]\n",
        "                            desc = ' '.join(remaining[:-2]).strip()\n",
        "                        else:\n",
        "                            desc = ' '.join(remaining[:-1]).strip()\n",
        "                    tail = []\n",
        "\n",
        "            # Parse tail: pure numbers; take from right as Years_in_Storage and Age\n",
        "            # Limit token length to avoid misidentifying date-like numbers\n",
        "            if tail:\n",
        "                ints = [t for t in tail if re.match(r'^\\d{1,3}$', t)]\n",
        "                if len(ints) >= 1:\n",
        "                    years_in_storage = ints[-1]\n",
        "                if len(ints) >= 2:\n",
        "                    age = ints[-2]\n",
        "\n",
        "            rec = {\n",
        "                'Region': region, 'FONUM': fonum, 'FOSHORT': foshort, 'Loc': loc, 'LNAME': lname,\n",
        "                'Asset': asset, 'Class': clazz, 'Desc': desc, 'Type': type_tok,\n",
        "                'Acquire': acquire, 'Effective': effective, 'SerialNum': serial,\n",
        "            }\n",
        "            if disposed:\n",
        "                rec['Disposed'] = disposed\n",
        "            if age:\n",
        "                rec['Age'] = age\n",
        "            if years_in_storage:\n",
        "                rec['Years_in_Storage'] = years_in_storage\n",
        "            out.append(rec)\n",
        "        except Exception:\n",
        "            continue  # Skip only extreme anomalies\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def parse_floor_details_page(page_text: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"Parse 'Floor Asset Details' from full lines (allowing cross-line tokens)\"\"\"\n",
        "    rows: List[Dict[str, str]] = []\n",
        "    for line in page_text.split('\\n'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "        if not (re.match(r'^\\s*\\d', line) and 'Floor' in line):\n",
        "            continue\n",
        "        parts = [p.strip() for p in re.split(r'\\s{2,}', line) if p.strip()]\n",
        "        if len(parts) < 6:\n",
        "            continue\n",
        "\n",
        "        first = parts[0]\n",
        "        toks = first.split(None, 1)\n",
        "        loc = toks[0]\n",
        "        place = toks[1] if len(toks) > 1 else ''\n",
        "        region = parts[1] if len(parts) > 1 else ''\n",
        "        svc = parts[2] if len(parts) > 2 else ''\n",
        "\n",
        "        asset = ''\n",
        "        serial = ''\n",
        "        if len(parts) > 3:\n",
        "            asset_serial = parts[3].split(None, 1)\n",
        "            asset = asset_serial[0]\n",
        "            serial = asset_serial[1] if len(asset_serial) > 1 else ''\n",
        "\n",
        "        type_ = ''\n",
        "        desc = ''\n",
        "        i = 4\n",
        "        if i < len(parts):\n",
        "            td = parts[i].split()\n",
        "            type_ = td[0] if td else ''\n",
        "            desc = ' '.join(td[1:]) if len(td) > 1 else ''\n",
        "            i += 1\n",
        "\n",
        "        acquire = ''\n",
        "        effective = ''\n",
        "        disposed = ''\n",
        "        dates: List[str] = []\n",
        "        leftover: List[str] = []\n",
        "        while i < len(parts) and len(dates) < 3:\n",
        "            token = parts[i]\n",
        "            for st in token.split():\n",
        "                if DATE_RE.match(st):\n",
        "                    dates.append(st)\n",
        "                else:\n",
        "                    leftover.append(st)\n",
        "            i += 1\n",
        "        if len(dates) > 0: acquire = dates[0]\n",
        "        if len(dates) > 1: effective = dates[1]\n",
        "        if len(dates) > 2: disposed = dates[2]\n",
        "\n",
        "        clazz = ''\n",
        "        mfg = ''\n",
        "        class_mfg_parts: List[str] = leftover.copy()\n",
        "        while i < len(parts) and len(' '.join(class_mfg_parts).split()) < 2:\n",
        "            class_mfg_parts.append(parts[i]); i += 1\n",
        "        cm = ' '.join(class_mfg_parts).split()\n",
        "        if cm: clazz = cm[0]\n",
        "        if len(cm) > 1: mfg = cm[1]\n",
        "\n",
        "        lname = parts[i] if i < len(parts) else ''\n",
        "        i += 1\n",
        "\n",
        "        fonum = ''\n",
        "        foshort = ''\n",
        "        if i < len(parts):\n",
        "            fo_parts = parts[i].split(None, 1)\n",
        "            fonum = fo_parts[0]\n",
        "            foshort = fo_parts[1] if len(fo_parts) > 1 else ''\n",
        "            i += 1\n",
        "\n",
        "        cat = parts[i] if i < len(parts) else ''\n",
        "        i += 1\n",
        "        year = parts[i] if i < len(parts) else ''\n",
        "        i += 1\n",
        "\n",
        "        age = ''\n",
        "        report_date = ''\n",
        "        if i < len(parts):\n",
        "            remaining = parts[i:]\n",
        "            if len(remaining) == 1:\n",
        "                token = remaining[0]\n",
        "                if DATE_RE.match(token):\n",
        "                    report_date = token\n",
        "                else:\n",
        "                    age = token\n",
        "            else:\n",
        "                age_candidate = remaining[0]\n",
        "                last_token = remaining[-1]\n",
        "                if DATE_RE.match(last_token):\n",
        "                    report_date = last_token\n",
        "                    age = age_candidate\n",
        "                else:\n",
        "                    age = ' '.join(remaining)\n",
        "\n",
        "        rows.append({\n",
        "            'Loc': loc, 'Place': place, 'Region': region, 'Service': svc, 'Asset': asset,\n",
        "            'SerialNum': serial, 'Type': type_, 'Desc': desc, 'Acquire': effective if False else acquire,  # keep exact structure; no logic change\n",
        "            'Effective': effective, 'Disposed': disposed, 'Class': clazz, 'MFG': mfg,\n",
        "            'LNAME': lname, 'FONUM': fonum, 'FOSHORT': foshort, 'Cat': cat,\n",
        "            'Year': year, 'Age': age, 'ReportDate': report_date\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "\n",
        "def parse_site_line(row: str) -> Optional[Dict[str, str]]:\n",
        "    \"\"\"A stitched row -> Site operational fields\"\"\"\n",
        "    toks = row.split()\n",
        "    if not toks or not toks[0].isdigit():\n",
        "        return None\n",
        "\n",
        "    open_idx = None\n",
        "    for i in range(1, len(toks)):\n",
        "        if DATE_RE.match(toks[i]):\n",
        "            open_idx = i\n",
        "            break\n",
        "    if open_idx is None or open_idx < 2:\n",
        "        return None\n",
        "\n",
        "    loc = toks[0]\n",
        "    place = toks[open_idx - 1]\n",
        "    lname = ' '.join(toks[1:open_idx - 1])\n",
        "\n",
        "    open_date = toks[open_idx]\n",
        "    closed_date = toks[open_idx + 1] if open_idx + 1 < len(toks) and DATE_RE.match(toks[open_idx + 1]) else ''\n",
        "    base = open_idx + (2 if closed_date else 1)\n",
        "    ksi = toks[base] if base < len(toks) else ''\n",
        "    cmty_num = toks[base + 1] if base + 1 < len(toks) else ''\n",
        "\n",
        "    svc_idx = None\n",
        "    j = base + 2\n",
        "    while j < len(toks):\n",
        "        if toks[j] in {'Army', 'Navy', 'Air'}:\n",
        "            svc_idx = j; break\n",
        "        if toks[j] == 'Marine' and j + 1 < len(toks) and toks[j + 1] == 'Corps':\n",
        "            svc_idx = j; break\n",
        "        j += 1\n",
        "    if svc_idx is None:\n",
        "        return None\n",
        "\n",
        "    cmty = ' '.join(toks[base + 2:svc_idx])\n",
        "    if toks[svc_idx] == 'Marine' and svc_idx + 1 < len(toks) and toks[svc_idx + 1] == 'Corps':\n",
        "        svc = 'Marine Corps'\n",
        "        fonum_start = svc_idx + 2\n",
        "    else:\n",
        "        svc = toks[svc_idx]\n",
        "        fonum_start = svc_idx + 1\n",
        "\n",
        "    if fonum_start >= len(toks):\n",
        "        return None\n",
        "    fonum = toks[fonum_start]\n",
        "    foshort = ' '.join(toks[fonum_start + 1:]) if fonum_start + 1 < len(toks) else ''\n",
        "\n",
        "    return {\n",
        "        'Loc': loc, 'LNAME': lname, 'Place': place,\n",
        "        'Open': open_date, 'Closed': closed_date, 'KSI': ksi,\n",
        "        'CmtyNum': cmty_num, 'Cmty': cmty, 'SVC': svc,\n",
        "        'FONUM': fonum, 'FOSHORT': foshort,\n",
        "    }\n",
        "\n",
        "\n",
        "def parse_site_status_page(page_text: str) -> List[Dict[str, str]]:\n",
        "    \"\"\"Stitch multi-lines into rows: a line that starts with digits marks the start of a new row; other lines are appended to the current row.\"\"\"\n",
        "    lines = [ln.rstrip() for ln in page_text.split('\\n') if ln.strip()]\n",
        "    rows: List[str] = []\n",
        "    buf: List[str] = []\n",
        "\n",
        "    def flush():\n",
        "        if buf:\n",
        "            rows.append(' '.join(buf))\n",
        "            buf.clear()\n",
        "\n",
        "    for ln in lines:\n",
        "        if re.match(r'^\\s*\\d+\\b', ln):  # new row\n",
        "            flush()\n",
        "            buf.append(ln.strip())\n",
        "        else:\n",
        "            if buf:\n",
        "                buf.append(ln.strip())\n",
        "            else:\n",
        "                continue\n",
        "    flush()\n",
        "\n",
        "    out: List[Dict[str, str]] = []\n",
        "    for r in rows:\n",
        "        rec = parse_site_line(r)\n",
        "        if rec:\n",
        "            out.append(rec)\n",
        "    return out\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# D) —— Detailed Extraction (includes Site/Years; fixes empty tables + dtype-compatible casting)\n",
        "# =============================================================================\n",
        "def extract_detailed_tables(pdf_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Returns: asset_df, floor_df, site_df, years_storage_df\n",
        "    - If a page has no detected month, carry forward the most recent month\n",
        "    - Years in storage: even if years column is missing, default 0 will still appear in the pivot\n",
        "    \"\"\"\n",
        "    text = subprocess.check_output(['pdftotext', '-layout', pdf_path, '-']).decode('utf-8')\n",
        "    pages = text.split('\\f')\n",
        "    month_map = detect_month_map(pages)\n",
        "\n",
        "    asset_data: List[Dict[str, str]] = []\n",
        "    floor_data: List[Dict[str, str]] = []\n",
        "    site_data: List[Dict[str, str]] = []\n",
        "\n",
        "    current_month: Optional[str] = None\n",
        "    for p_num, page_text in enumerate(pages, start=1):\n",
        "        if not page_text.strip():\n",
        "            continue\n",
        "        month = month_map.get(p_num)\n",
        "        if month:\n",
        "            current_month = month\n",
        "\n",
        "        for rec in parse_asset_details_page(page_text):\n",
        "            if current_month:\n",
        "                rec['Month'] = current_month\n",
        "            asset_data.append(rec)\n",
        "\n",
        "        for rec in parse_floor_details_page(page_text):\n",
        "            if current_month:\n",
        "                rec['Month'] = current_month\n",
        "            floor_data.append(rec)\n",
        "\n",
        "        for rec in parse_site_status_page(page_text):\n",
        "            if current_month:\n",
        "                rec['Month'] = current_month\n",
        "            site_data.append(rec)\n",
        "\n",
        "    asset_df = pd.DataFrame(asset_data).drop_duplicates()\n",
        "    floor_df = pd.DataFrame(floor_data).drop_duplicates()\n",
        "    site_df  = pd.DataFrame(site_data).drop_duplicates()\n",
        "\n",
        "    # ---- Years in storage pivot (more robust) ----\n",
        "    if not asset_df.empty:\n",
        "        # Find years column\n",
        "        yrs_col = None\n",
        "        for c in asset_df.columns:\n",
        "            if c.lower() in {'years_in_storage', 'years', 'yrs', 'yrs_in_storage'}:\n",
        "                yrs_col = c\n",
        "                break\n",
        "        if yrs_col is None:\n",
        "            asset_df['Years_in_Storage'] = 0\n",
        "            yrs_col = 'Years_in_Storage'\n",
        "\n",
        "        # Compatible casting: prefer nullable int, otherwise fall back to float/int\n",
        "        age_num = pd.to_numeric(asset_df.get('Age'), errors='coerce')\n",
        "        yis_num = pd.to_numeric(asset_df[yrs_col], errors='coerce').fillna(0)\n",
        "\n",
        "        try:\n",
        "            INT64_NULLABLE = pd.Int64Dtype()\n",
        "            asset_df['Age_int'] = age_num.astype(INT64_NULLABLE)\n",
        "            asset_df['Years_in_Storage_int'] = yis_num.astype(INT64_NULLABLE)\n",
        "        except Exception:\n",
        "            asset_df['Age_int'] = age_num.astype('float64')\n",
        "            asset_df['Years_in_Storage_int'] = yis_num.astype('int64')\n",
        "\n",
        "        filt = asset_df['Age_int'].notna() & (asset_df['Age_int'] <= 50)\n",
        "        pivot_tables: List[pd.DataFrame] = []\n",
        "\n",
        "        # Check whether Month column exists\n",
        "        if 'Month' in asset_df.columns:\n",
        "            for mth, grp in asset_df[filt].groupby('Month', dropna=False):\n",
        "                pt = pd.pivot_table(\n",
        "                    grp,\n",
        "                    index='Age_int',\n",
        "                    columns='Years_in_Storage_int',\n",
        "                    values='Asset',\n",
        "                    aggfunc='count',\n",
        "                    fill_value=0\n",
        "                )\n",
        "                if isinstance(pt, pd.Series):\n",
        "                    pt = pt.to_frame()\n",
        "                pt = pt.reset_index().rename(columns={'Age_int': 'Age'})\n",
        "                pt.columns = ['Age'] + [f'Storage_{c}' for c in pt.columns[1:]]\n",
        "                if mth is not None:\n",
        "                    pt.insert(0, 'Month', mth)\n",
        "                pivot_tables.append(pt)\n",
        "        else:\n",
        "            # Without Month column, handle in aggregate\n",
        "            pt = pd.pivot_table(\n",
        "                asset_df[filt],\n",
        "                index='Age_int',\n",
        "                columns='Years_in_Storage_int',\n",
        "                values='Asset',\n",
        "                aggfunc='count',\n",
        "                fill_value=0\n",
        "            )\n",
        "            if isinstance(pt, pd.Series):\n",
        "                pt = pt.to_frame()\n",
        "            pt = pt.reset_index().rename(columns={'Age_int': 'Age'})\n",
        "            pt.columns = ['Age'] + [f'Storage_{c}' for c in pt.columns[1:]]\n",
        "            pivot_tables.append(pt)\n",
        "\n",
        "        years_storage_df = pd.concat(pivot_tables, ignore_index=True) if pivot_tables else pd.DataFrame()\n",
        "    else:\n",
        "        years_storage_df = pd.DataFrame()\n",
        "\n",
        "    return asset_df, floor_df, site_df, years_storage_df\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# E) —— Output / Runner\n",
        "# =============================================================================\n",
        "def save_outputs(outdir: str,\n",
        "                 region_df: pd.DataFrame,\n",
        "                 field_df: pd.DataFrame,\n",
        "                 installed_df: pd.DataFrame,\n",
        "                 asset_df: pd.DataFrame,\n",
        "                 floor_df: pd.DataFrame,\n",
        "                 site_df: pd.DataFrame,\n",
        "                 years_storage_df: pd.DataFrame) -> None:\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    if not region_df.empty:\n",
        "        region_df.to_csv(os.path.join(outdir, 'assets_by_region_service.csv'), index=False)\n",
        "    if not field_df.empty:\n",
        "        field_df.to_csv(os.path.join(outdir, 'assets_by_field_office.csv'), index=False)\n",
        "    if not installed_df.empty:\n",
        "        installed_df.to_csv(os.path.join(outdir, 'installed_assets_location_manufacture.csv'), index=False)\n",
        "    if not asset_df.empty:\n",
        "        asset_df.to_csv(os.path.join(outdir, 'asset_details.csv'), index=False)\n",
        "    if not floor_df.empty:\n",
        "        floor_df.to_csv(os.path.join(outdir, 'floor_asset_details.csv'), index=False)\n",
        "    if not site_df.empty:\n",
        "        site_df.to_csv(os.path.join(outdir, 'site_operational_status.csv'), index=False)\n",
        "    if not years_storage_df.empty:\n",
        "        years_storage_df.to_csv(os.path.join(outdir, 'years_in_storage.csv'), index=False)\n",
        "\n",
        "\n",
        "def run_in_notebook(pdf_path: str, outdir: str = '.'):\n",
        "    \"\"\"\n",
        "    First use code 1 flow to extract three summary tables (more complete, with Month);\n",
        "    then extract four detailed tables (Asset/Floor/Site/Years); finally save all outputs.\n",
        "    \"\"\"\n",
        "    region_df, field_df, installed_df = extract_region_field_installed_v1(pdf_path)\n",
        "    asset_df, floor_df, site_df, years_storage_df = extract_detailed_tables(pdf_path)\n",
        "\n",
        "    save_outputs(outdir, region_df, field_df, installed_df, asset_df, floor_df, site_df, years_storage_df)\n",
        "    print('Extraction complete.')\n",
        "    for name, df in [('Region', region_df), ('FieldOffice', field_df),\n",
        "                     ('Installed', installed_df), ('AssetDetails', asset_df),\n",
        "                     ('FloorDetails', floor_df), ('SiteOperational', site_df),\n",
        "                     ('YearsInStorage', years_storage_df)]:\n",
        "        if not df.empty:\n",
        "            print(f'[{name}] rows={len(df):,}, cols={len(df.columns)}')\n",
        "\n",
        "run_in_notebook('/content/FY2023_Asset_Report.pdf', '/content')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_r4AKXV48Kg7",
        "outputId": "ef15f0a7-08f8-4b02-dcf1-fcb5ab0dee37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1938747456.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  return pd.concat(kept, ignore_index=True) if kept else pd.DataFrame()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n",
            "[Region] rows=48, cols=9\n",
            "[FieldOffice] rows=300, cols=9\n",
            "[Installed] rows=888, cols=18\n",
            "[AssetDetails] rows=4,911, cols=17\n",
            "[FloorDetails] rows=23,303, cols=21\n",
            "[SiteOperational] rows=994, cols=12\n",
            "[YearsInStorage] rows=125, cols=56\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}