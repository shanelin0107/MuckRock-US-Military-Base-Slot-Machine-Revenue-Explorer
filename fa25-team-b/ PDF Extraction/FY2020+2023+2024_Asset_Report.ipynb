{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!apt-get update -y\n",
        "!apt-get install -y --fix-missing poppler-utils\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq poppler-utils\n",
        "!pip install docling-parse docling-core rapidfuzz docling\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEVP1rPSl0G7",
        "outputId": "40700e04-c1cb-4f50-fe47-f293ccd3541e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,123 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,825 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,168 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:14 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:15 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,434 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,526 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,594 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,856 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.5 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [5,969 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Fetched 37.2 MB in 5s (7,273 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 46 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.12 [186 kB]\n",
            "Fetched 186 kB in 0s (457 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 125082 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.12_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.12) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Collecting docling-parse\n",
            "  Downloading docling_parse-4.7.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting docling-core\n",
            "  Downloading docling_core-2.50.1-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting rapidfuzz\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (12 kB)\n",
            "Collecting docling\n",
            "  Downloading docling-2.61.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tabulate<1.0.0,>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from docling-parse) (0.9.0)\n",
            "Requirement already satisfied: pillow<13.0.0,>=10.0.0 in /usr/local/lib/python3.12/dist-packages (from docling-parse) (11.3.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from docling-parse) (2.11.10)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.16.0 in /usr/local/lib/python3.12/dist-packages (from docling-core) (4.25.1)\n",
            "Collecting jsonref<2.0.0,>=1.1.0 (from docling-core)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: pandas<3.0.0,>=2.1.4 in /usr/local/lib/python3.12/dist-packages (from docling-core) (2.2.2)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.1 in /usr/local/lib/python3.12/dist-packages (from docling-core) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from docling-core) (4.15.0)\n",
            "Collecting typer<0.20.0,>=0.12.5 (from docling-core)\n",
            "  Downloading typer-0.19.2-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting latex2mathml<4.0.0,>=3.77.0 (from docling-core)\n",
            "  Downloading latex2mathml-3.78.1-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting docling-ibm-models<4,>=3.9.1 (from docling)\n",
            "  Downloading docling_ibm_models-3.10.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from docling)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting pypdfium2!=4.30.1,<5.0.0,>=4.30.0 (from docling)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic-settings<3.0.0,>=2.3.0 in /usr/local/lib/python3.12/dist-packages (from docling) (2.11.0)\n",
            "Requirement already satisfied: huggingface_hub<1,>=0.23 in /usr/local/lib/python3.12/dist-packages (from docling) (0.36.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from docling) (2.32.4)\n",
            "Collecting rapidocr<4.0.0,>=3.3 (from docling)\n",
            "  Downloading rapidocr-3.4.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: certifi>=2024.7.4 in /usr/local/lib/python3.12/dist-packages (from docling) (2025.10.5)\n",
            "Collecting rtree<2.0.0,>=1.3.0 (from docling)\n",
            "  Downloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting python-docx<2.0.0,>=1.1.2 (from docling)\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx<2.0.0,>=1.0.2 (from docling)\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.12/dist-packages (from docling) (4.13.5)\n",
            "Collecting marko<3.0.0,>=2.1.2 (from docling)\n",
            "  Downloading marko-2.2.1-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: openpyxl<4.0.0,>=3.1.5 in /usr/local/lib/python3.12/dist-packages (from docling) (3.1.5)\n",
            "Requirement already satisfied: lxml<7.0.0,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (5.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.65.0 in /usr/local/lib/python3.12/dist-packages (from docling) (4.67.1)\n",
            "Requirement already satisfied: pluggy<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.6.0)\n",
            "Collecting pylatexenc<3.0,>=2.10 (from docling)\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.16.3)\n",
            "Requirement already satisfied: accelerate<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from docling) (1.11.0)\n",
            "Collecting polyfactory>=2.22.2 (from docling)\n",
            "  Downloading polyfactory-2.22.3-py3-none-any.whl.metadata (27 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (25.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (5.9.5)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (2.8.0+cu126)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from accelerate<2,>=1.0.0->docling) (0.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->docling) (2.8)\n",
            "Collecting semchunk<3.0.0,>=2.2.0 (from docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading semchunk-2.2.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.12/dist-packages (from docling-core[chunking]<3.0.0,>=2.50.1->docling) (4.57.1)\n",
            "Requirement already satisfied: torchvision<1,>=0 in /usr/local/lib/python3.12/dist-packages (from docling-ibm-models<4,>=3.9.1->docling) (0.23.0+cu126)\n",
            "Collecting jsonlines<5.0.0,>=3.1.0 (from docling-ibm-models<4,>=3.9.1->docling)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub<1,>=0.23->docling) (1.2.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.16.0->docling-core) (0.28.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl<4.0.0,>=3.1.5->docling) (2.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling-core) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling-core) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=2.1.4->docling-core) (2025.2)\n",
            "Collecting faker>=5.0.0 (from polyfactory>=2.22.2->docling)\n",
            "  Downloading faker-37.12.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->docling-parse) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->docling-parse) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->docling-parse) (0.4.2)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.12/dist-packages (from pydantic-settings<3.0.0,>=2.3.0->docling) (1.2.1)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx<2.0.0,>=1.0.2->docling)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting pyclipper>=1.2.0 (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: opencv-python>=4.5.1.48 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (4.12.0.88)\n",
            "Requirement already satisfied: six>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (1.17.0)\n",
            "Requirement already satisfied: Shapely!=2.0.4,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.1.2)\n",
            "Requirement already satisfied: omegaconf in /usr/local/lib/python3.12/dist-packages (from rapidocr<4.0.0,>=3.3->docling) (2.3.0)\n",
            "Collecting colorlog (from rapidocr<4.0.0,>=3.3->docling)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.32.2->docling) (2.5.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling-core) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling-core) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.20.0,>=0.12.5->docling-core) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling-core) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<0.20.0,>=0.12.5->docling-core) (2.19.2)\n",
            "Collecting mpire[dill] (from semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling)\n",
            "  Downloading mpire-2.10.2-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.34.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.22.1)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.12/dist-packages (from omegaconf->rapidocr<4.0.0,>=3.3->docling) (4.9.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<0.20.0,>=0.12.5->docling-core) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate<2,>=1.0.0->docling) (3.0.3)\n",
            "Requirement already satisfied: multiprocess>=0.70.15 in /usr/local/lib/python3.12/dist-packages (from mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.70.16)\n",
            "Requirement already satisfied: dill>=0.3.8 in /usr/local/lib/python3.12/dist-packages (from multiprocess>=0.70.15->mpire[dill]->semchunk<3.0.0,>=2.2.0->docling-core[chunking]<3.0.0,>=2.50.1->docling) (0.3.8)\n",
            "Downloading docling_parse-4.7.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_core-2.50.1-py3-none-any.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.3/169.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling-2.61.1-py3-none-any.whl (254 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m254.7/254.7 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docling_ibm_models-3.10.2-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.4/87.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading latex2mathml-3.78.1-py3-none-any.whl (73 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.9/73.9 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marko-2.2.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.7/42.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading polyfactory-2.22.3-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m47.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rapidocr-3.4.2-py3-none-any.whl (15.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.1/15.1 MB\u001b[0m \u001b[31m72.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtree-1.4.1-py3-none-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (507 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.6/507.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.19.2-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faker-37.12.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading pyclipper-1.3.0.post6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (963 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m963.8/963.8 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading semchunk-2.2.2-py3-none-any.whl (10 kB)\n",
            "Downloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Downloading mpire-2.10.2-py3-none-any.whl (272 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.8/272.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pylatexenc\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136817 sha256=9f533c8fb4cc6b10fa12f749c2993928fe4595002b23ba8bbe904e8f2233d9bb\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/3e/78/fa1588c1ae991bbfd814af2bcac6cef7a178beee1939180d46\n",
            "Successfully built pylatexenc\n",
            "Installing collected packages: pylatexenc, pyclipper, filetype, XlsxWriter, rtree, rapidfuzz, python-docx, pypdfium2, mpire, marko, latex2mathml, jsonref, jsonlines, faker, colorlog, rapidocr, python-pptx, polyfactory, typer, semchunk, docling-core, docling-parse, docling-ibm-models, docling\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.20.0\n",
            "    Uninstalling typer-0.20.0:\n",
            "      Successfully uninstalled typer-0.20.0\n",
            "Successfully installed XlsxWriter-3.2.9 colorlog-6.10.1 docling-2.61.1 docling-core-2.50.1 docling-ibm-models-3.10.2 docling-parse-4.7.1 faker-37.12.0 filetype-1.2.0 jsonlines-4.0.0 jsonref-1.1.0 latex2mathml-3.78.1 marko-2.2.1 mpire-2.10.2 polyfactory-2.22.3 pyclipper-1.3.0.post6 pylatexenc-2.10 pypdfium2-4.30.0 python-docx-1.2.0 python-pptx-1.0.2 rapidfuzz-3.14.3 rapidocr-3.4.2 rtree-1.4.1 semchunk-2.2.2 typer-0.19.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "roEY5jLFYXoh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "9789dd4c-c4a4-4c47-b79a-22fd3a0ff7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1pAkcjieicAWNrMH5nSjWT8ak3P23vcIp\n",
            "To: /content/FY2020_Asset_Report.pdf\n",
            "100%|██████████| 12.4M/12.4M [00:00<00:00, 39.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1KM8JnW5gaNNYn6eTYI1pRXDtQsKbFJHo\n",
            "To: /content/FY2022_Asset_Report.pdf\n",
            "100%|██████████| 16.1M/16.1M [00:00<00:00, 106MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=1QtmW3UhIMXDLQoI3wI7zH88SACFurjcR\n",
            "To: /content/FY2023_Asset_Report.pdf\n",
            "100%|██████████| 13.5M/13.5M [00:00<00:00, 88.0MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=10J0fzRZMbZfO7v0huyD0s7d5kQ8garfQ\n",
            "To: /content/FY2024_Asset_Report.pdf\n",
            "100%|██████████| 10.4M/10.4M [00:00<00:00, 48.0MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'FY2024_Asset_Report.pdf'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import gdown\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# I’ve uploaded the file to Google Drive, and this code is used to read it.\n",
        "url = 'https://drive.google.com/uc?export=download&id=1pAkcjieicAWNrMH5nSjWT8ak3P23vcIp'\n",
        "output = 'FY2020_Asset_Report.pdf'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "url = 'https://drive.google.com/uc?export=download&id=1KM8JnW5gaNNYn6eTYI1pRXDtQsKbFJHo'\n",
        "output = 'FY2022_Asset_Report.pdf'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "url = 'https://drive.google.com/uc?export=download&id=1QtmW3UhIMXDLQoI3wI7zH88SACFurjcR'\n",
        "output = 'FY2023_Asset_Report.pdf'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "url = 'https://drive.google.com/uc?export=download&id=10J0fzRZMbZfO7v0huyD0s7d5kQ8garfQ'\n",
        "output = 'FY2024_Asset_Report.pdf'\n",
        "gdown.download(url, output, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import subprocess\n",
        "from typing import List, Tuple, Optional, Dict, Any\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# =============================================================================\n",
        "# Common regular expressions and helper functions\n",
        "# =============================================================================\n",
        "DATE_RE = re.compile(r'^(?:\\d{1,2}/\\d{1,2}/\\d{2,4}|\\d{4}-\\d{1,2}-\\d{1,2})$')\n",
        "REGION_PREFIX = re.compile(\n",
        "    r'^(Europe|Japan|Korea|Okinawa|Pacific|United(?:\\s+States)?)\\b',\n",
        "    re.IGNORECASE\n",
        ")\n",
        "SERIAL_RE = re.compile(r'(?i)^[a-z0-9-]+$')\n",
        "\n",
        "# Acceptable Floor regions, used to determine whether a row is a summary\n",
        "ALLOWED_FLOOR_REGIONS = {\"Europe\", \"Japan\", \"Korea\"}\n",
        "\n",
        "def _safe_concat(frames: List[pd.DataFrame]) -> pd.DataFrame:\n",
        "    \"\"\"Safely concatenate DataFrames, ignoring empty DataFrames.\"\"\"\n",
        "    if not frames:\n",
        "        return pd.DataFrame()\n",
        "    kept: List[pd.DataFrame] = []\n",
        "    for df in frames:\n",
        "        if df is None or df.empty:\n",
        "            continue\n",
        "        if df.dropna(how='all').empty:\n",
        "            continue\n",
        "        kept.append(df)\n",
        "    return pd.concat(kept, ignore_index=True) if kept else pd.DataFrame()\n",
        "\n",
        "# =============================================================================\n",
        "# PDF → Text (New: try pdftotext first; if it fails, fallback to pdfminer.six)\n",
        "# =============================================================================\n",
        "def read_pdf_text_layout(pdf_path: str) -> str:\n",
        "    \"\"\"Read PDF text layout using pdftotext only (no fallback).\"\"\"\n",
        "    if not os.path.exists(pdf_path):\n",
        "        raise FileNotFoundError(f\"PDF not found: {pdf_path}\")\n",
        "    if shutil.which(\"pdftotext\") is None:\n",
        "        raise EnvironmentError(\"pdftotext is not installed or not found in PATH.\")\n",
        "    out = subprocess.check_output(\n",
        "        [\"pdftotext\", \"-layout\", pdf_path, \"-\"],\n",
        "        stderr=subprocess.STDOUT\n",
        "    )\n",
        "    text = out.decode(\"utf-8\", errors=\"ignore\")\n",
        "    if not text.strip():\n",
        "        raise ValueError(\"Empty text extracted from PDF. Check the file or pdftotext output.\")\n",
        "    return text\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# A) Region and Field Office summary parsing\n",
        "# =============================================================================\n",
        "def get_month_v1(page_text: str) -> Optional[str]:\n",
        "    m = re.search(r'for month of\\s+([A-Za-z]+)\\s+(\\d{4})', page_text, re.IGNORECASE)\n",
        "    return f\"{m.group(1).capitalize()} {m.group(2)}\" if m else None\n",
        "\n",
        "def parse_region_and_field_office_v1(lines: List[str]) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"Parse summary tables such as Assets/EGMs by Region/Field Office.\"\"\"\n",
        "    region_df = pd.DataFrame()\n",
        "    field_df = pd.DataFrame()\n",
        "\n",
        "    # Region summary\n",
        "    idx_slots = next((i for i, l in enumerate(lines) if l.strip().startswith('Slots Only')), None)\n",
        "    if idx_slots is not None:\n",
        "        region_rows: List[List[Any]] = []\n",
        "        i = idx_slots + 1\n",
        "        while i < len(lines):\n",
        "            line = lines[i].strip()\n",
        "            if not line:\n",
        "                i += 1\n",
        "                continue\n",
        "            if (\n",
        "                line.startswith('Locations by Service') or\n",
        "                'Assets by Field Office' in line or\n",
        "                'EGMs by Field Office' in line or\n",
        "                'Installed Assets by Location' in line\n",
        "            ):\n",
        "                break\n",
        "            parts = [p for p in re.split(r'\\s{2,}', line) if p]\n",
        "            if len(parts) >= 5:\n",
        "                region_name = parts[0]\n",
        "                values: List[Any] = []\n",
        "                for p in parts[1:]:\n",
        "                    x = p.replace('%', '')\n",
        "                    if x in ('-', ''):\n",
        "                        values.append(np.nan)\n",
        "                    else:\n",
        "                        try:\n",
        "                            values.append(float(x.replace(',', '')))\n",
        "                        except Exception:\n",
        "                            values.append(x)\n",
        "                region_rows.append([region_name] + values)\n",
        "            i += 1\n",
        "        if region_rows:\n",
        "            maxlen = max(len(r) for r in region_rows)\n",
        "            for r in region_rows:\n",
        "                while len(r) < maxlen:\n",
        "                    r.append(np.nan)\n",
        "            cols = ['Region', '#Locations', 'Army', 'Navy', 'Marine_Corps', 'Airforce', 'Total', 'Percent']\n",
        "            region_df = pd.DataFrame(region_rows, columns=cols[:maxlen])\n",
        "\n",
        "    # Field office summary\n",
        "    field_rows: List[List[Any]] = []\n",
        "    start = False\n",
        "    current_region: Optional[str] = None\n",
        "    for raw in lines:\n",
        "        line = raw.rstrip(' ')\n",
        "        if 'Assets by Field Office' in line or 'EGMs by Field Office' in line:\n",
        "            start = True\n",
        "            continue\n",
        "        if not start:\n",
        "            continue\n",
        "        if not line.strip() or line.strip().startswith('Slots'):\n",
        "            continue\n",
        "        if line.strip() and not re.search(r'\\d', line.strip()):\n",
        "            current_region = line.strip()\n",
        "            continue\n",
        "        parts = [p for p in re.split(r'\\s{2,}', line.strip()) if p]\n",
        "        if len(parts) >= 6 and all(re.match(r'^[()0-9,.-]+$', p) for p in parts[-5:]):\n",
        "            first_part = parts[0]\n",
        "            fo_split = first_part.split()\n",
        "            if len(fo_split) < 2 or not fo_split[0].isdigit():\n",
        "                continue\n",
        "            fo_number = fo_split[0]\n",
        "            location_parts = fo_split[1:] + parts[1:-5]\n",
        "            location_name = ' '.join(location_parts)\n",
        "            values: List[float] = []\n",
        "            for p in parts[-5:]:\n",
        "                if p == '-':\n",
        "                    values.append(np.nan)\n",
        "                else:\n",
        "                    v = p\n",
        "                    if v.startswith('(') and v.endswith(')'):\n",
        "                        v = '-' + v[1:-1]\n",
        "                    values.append(float(v.replace(',', '')))\n",
        "            field_rows.append([current_region, fo_number, location_name] + values)\n",
        "    if field_rows:\n",
        "        field_df = pd.DataFrame(field_rows, columns=[\n",
        "            'Region', 'FO#', 'Location', 'Slots', 'ACM_CountR', 'ITC', 'FRS', 'Total'\n",
        "        ])\n",
        "\n",
        "    return region_df, field_df\n",
        "\n",
        "# =============================================================================\n",
        "# B) Installed Assets by Location parsing (original v2)\n",
        "# =============================================================================\n",
        "def parse_installed_assets_v2(lines: List[str]) -> pd.DataFrame:\n",
        "    rows: List[List[Any]] = []\n",
        "    started = False\n",
        "    current_region: Optional[str] = None\n",
        "\n",
        "    for raw in lines:\n",
        "        line = raw.rstrip()\n",
        "\n",
        "        if 'Installed Assets by Location' in line or ((\"FO #\" in line) and (\"IGT\" in line)):\n",
        "            started = True\n",
        "            m = REGION_PREFIX.match(line.strip())\n",
        "            if m:\n",
        "                current_region = m.group(1).strip()\n",
        "            continue\n",
        "\n",
        "        if not started:\n",
        "            continue\n",
        "\n",
        "        if not line.strip():\n",
        "            continue\n",
        "\n",
        "        if not re.search(r'\\d', line):\n",
        "            m = REGION_PREFIX.match(line.strip())\n",
        "            if m:\n",
        "                current_region = m.group(1).strip()\n",
        "            continue\n",
        "\n",
        "        if ('Tot/EGMs' in line) or ((\"NOV\" in line) and (\"AIN\" in line)):\n",
        "            parts_hdr = [p.strip() for p in re.split(r'\\s{2,}', line) if p.strip()]\n",
        "            if parts_hdr:\n",
        "                m = REGION_PREFIX.match(parts_hdr[0])\n",
        "                if m:\n",
        "                    current_region = m.group(1).strip()\n",
        "            continue\n",
        "\n",
        "        parts = [p.strip() for p in re.split(r'\\s{2,}', line) if p.strip()]\n",
        "\n",
        "        if parts and REGION_PREFIX.match(parts[0]):\n",
        "            current_region = REGION_PREFIX.match(parts[0]).group(1).strip()\n",
        "            continue\n",
        "\n",
        "        if len(parts) < 5:\n",
        "            continue\n",
        "\n",
        "        name = parts[0]\n",
        "        idx = 1\n",
        "\n",
        "        fo_number = None\n",
        "        if idx < len(parts) and re.match(r'^\\d+$', parts[idx]):\n",
        "            fo_number = parts[idx]\n",
        "            idx += 1\n",
        "\n",
        "        if idx >= len(parts):\n",
        "            continue\n",
        "        loc = parts[idx]\n",
        "        idx += 1\n",
        "\n",
        "        if idx >= len(parts):\n",
        "            continue\n",
        "        svc = parts[idx]\n",
        "        idx += 1\n",
        "\n",
        "        if not re.search(r'[A-Za-z]', svc):\n",
        "            continue\n",
        "\n",
        "        metric_tokens: List[str] = []\n",
        "        for token in parts[idx:]:\n",
        "            metric_tokens += token.split()\n",
        "\n",
        "        if len(metric_tokens) < 6:\n",
        "            continue\n",
        "\n",
        "        manuf_vals: List[Optional[float]] = []\n",
        "        for t in metric_tokens[:7]:\n",
        "            if t in {'-', ''}:\n",
        "                manuf_vals.append(None)\n",
        "            else:\n",
        "                try:\n",
        "                    manuf_vals.append(float(t.replace(',', '')))\n",
        "                except Exception:\n",
        "                    manuf_vals.append(None)\n",
        "        while len(manuf_vals) < 7:\n",
        "            manuf_vals.append(None)\n",
        "\n",
        "        remaining = metric_tokens[7:]\n",
        "        if not remaining:\n",
        "            continue\n",
        "\n",
        "        def _num(tok: Optional[str]) -> Optional[float]:\n",
        "            if tok is None or tok in ('-', ''):\n",
        "                return None\n",
        "            try:\n",
        "                return float(tok.replace(',', ''))\n",
        "            except Exception:\n",
        "                return None\n",
        "\n",
        "        total_pdf = _num(remaining[-1])\n",
        "\n",
        "        tot_egms = frs = acm = None\n",
        "        for tok in remaining[:-1]:\n",
        "            n = _num(tok)\n",
        "            if n is not None:\n",
        "                if tot_egms is None:\n",
        "                    tot_egms = n\n",
        "                    continue\n",
        "                if frs is None:\n",
        "                    frs = n\n",
        "                    continue\n",
        "                if acm is None:\n",
        "                    acm = n\n",
        "                    continue\n",
        "            if tot_egms is not None and frs is not None and acm is not None:\n",
        "                break\n",
        "\n",
        "        itc = None\n",
        "        for tok in reversed(remaining[:-1]):\n",
        "            n = _num(tok)\n",
        "            if n is not None:\n",
        "                itc = n\n",
        "                break\n",
        "\n",
        "        total_computed = sum(x for x in [tot_egms, frs, acm, itc] if x is not None)\n",
        "\n",
        "        rows.append([\n",
        "            current_region, name, fo_number, loc, svc,\n",
        "            *manuf_vals, tot_egms, frs, acm, itc, total_pdf, total_computed\n",
        "        ])\n",
        "\n",
        "    if not rows:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    columns = [\n",
        "        'Region', 'LocationName', 'FO#', 'Loc', 'Svc',\n",
        "        'NOV', 'AIN', 'IGT', 'WMS', 'BAL', 'KON', 'ITE',\n",
        "        'Tot_EGMs', 'FRS', 'ACM', 'ITC', 'Total_PDF', 'Total_Computed'\n",
        "    ]\n",
        "    return pd.DataFrame(rows, columns=columns)\n",
        "\n",
        "def extract_region_field_installed_v2(pdf_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    # Here we switch to the shared PDF→Text function (keep the remaining logic unchanged)\n",
        "    text = read_pdf_text_layout(pdf_path)\n",
        "    pages = text.split('\\f')\n",
        "\n",
        "    region_frames: List[pd.DataFrame] = []\n",
        "    field_frames: List[pd.DataFrame] = []\n",
        "    installed_frames: List[pd.DataFrame] = []\n",
        "\n",
        "    current_month: Optional[str] = None\n",
        "    for page_text in pages:\n",
        "        if not page_text.strip():\n",
        "            continue\n",
        "\n",
        "        month = get_month_v1(page_text)\n",
        "        if month:\n",
        "            current_month = month\n",
        "\n",
        "        lines = page_text.split('\\n')\n",
        "        page_upper = page_text.upper()\n",
        "\n",
        "        has_region = ('ASSETS BY REGION' in page_upper) or ('EGMS BY REGION' in page_upper)\n",
        "        has_field = ('ASSETS BY FIELD OFFICE' in page_upper) or ('EGMS BY FIELD OFFICE' in page_upper)\n",
        "        if has_region or has_field:\n",
        "            rdf, fdf = parse_region_and_field_office_v1(lines)\n",
        "            if not rdf.empty:\n",
        "                if current_month:\n",
        "                    rdf = rdf.assign(Month=current_month)\n",
        "                region_frames.append(rdf)\n",
        "            if not fdf.empty:\n",
        "                if current_month:\n",
        "                    fdf = fdf.assign(Month=current_month)\n",
        "                field_frames.append(fdf)\n",
        "\n",
        "        if 'INSTALLED ASSETS BY LOCATION' in page_upper:\n",
        "            inst_df = parse_installed_assets_v2(lines)\n",
        "            if not inst_df.empty:\n",
        "                if current_month:\n",
        "                    inst_df = inst_df.assign(Month=current_month)\n",
        "                installed_frames.append(inst_df)\n",
        "\n",
        "    region_df = _safe_concat(region_frames)\n",
        "    field_df = _safe_concat(field_frames)\n",
        "    installed_df = _safe_concat(installed_frames)\n",
        "    return region_df, field_df, installed_df\n",
        "\n",
        "# =============================================================================\n",
        "# C) Other detailed table parsing\n",
        "# =============================================================================\n",
        "def detect_month_map(pages: List[str]) -> Dict[int, str]:\n",
        "    patterns = [\n",
        "        re.compile(r'Assets by Region,\\s*Service\\s+for month of\\s+([A-Za-z]+\\s+\\d{4})', re.I),\n",
        "        re.compile(r'EGMs by Region,\\s*Service\\s+for month of\\s+([A-Za-z]+\\s+\\d{4})', re.I),\n",
        "        re.compile(r'for month of\\s+([A-Za-z]+\\s+\\d{4})', re.I),\n",
        "    ]\n",
        "    month_start_pages: List[Tuple[int, str]] = []\n",
        "    seen: set = set()\n",
        "    for i, page in enumerate(pages, start=1):\n",
        "        for pat in patterns:\n",
        "            m = pat.search(page)\n",
        "            if m:\n",
        "                month = m.group(1).strip()\n",
        "                if month not in seen:\n",
        "                    month_start_pages.append((i, month))\n",
        "                    seen.add(month)\n",
        "                break\n",
        "    if not month_start_pages:\n",
        "        return {}\n",
        "    month_ranges: List[Tuple[int, int, str]] = []\n",
        "    for idx, (start, month) in enumerate(month_start_pages):\n",
        "        end = month_start_pages[idx + 1][0] - 1 if idx + 1 < len(month_start_pages) else len(pages)\n",
        "        month_ranges.append((start, end, month))\n",
        "    month_map: Dict[int, str] = {}\n",
        "    for s, e, mth in month_ranges:\n",
        "        for p in range(s, e + 1):\n",
        "            month_map[p] = mth\n",
        "    return month_map\n",
        "\n",
        "# -- asset details parsing --\n",
        "def parse_asset_details_page(page_text: str) -> List[Dict[str, str]]:\n",
        "    out: List[Dict[str, str]] = []\n",
        "    def is_valid_date(s: str) -> bool:\n",
        "        if not s:\n",
        "            return False\n",
        "        if not DATE_RE.match(s):\n",
        "            return False\n",
        "        if re.match(r'^\\d{5,6}$', s):\n",
        "            return False\n",
        "        if '/' not in s and '-' not in s:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    for raw in page_text.split('\\n'):\n",
        "        line = raw.strip()\n",
        "        if not line:\n",
        "            continue\n",
        "        if not REGION_PREFIX.match(line):\n",
        "            continue\n",
        "        toks = line.split()\n",
        "        if len(toks) < 8:\n",
        "            continue\n",
        "        try:\n",
        "            region = toks[0]\n",
        "            fonum = toks[1]\n",
        "            i = 2\n",
        "            while i < len(toks) and not toks[i].isdigit():\n",
        "                i += 1\n",
        "            foshort = ' '.join(toks[2:i]).strip()\n",
        "            if i >= len(toks) or not toks[i].isdigit():\n",
        "                continue\n",
        "            loc = toks[i]\n",
        "            i += 1\n",
        "            j = i\n",
        "            while j < len(toks) and not re.fullmatch(r'\\d{4,6}', toks[j]):\n",
        "                j += 1\n",
        "            if j >= len(toks):\n",
        "                continue\n",
        "            lname = ' '.join(toks[i:j]).strip()\n",
        "            asset = toks[j]\n",
        "            j += 1\n",
        "            if j >= len(toks):\n",
        "                continue\n",
        "            clazz = toks[j]\n",
        "            j += 1\n",
        "            remaining = toks[j:]\n",
        "            date_rel_idx = [k for k, t in enumerate(remaining) if is_valid_date(t)]\n",
        "            desc = ''\n",
        "            type_tok = ''\n",
        "            acquire = ''\n",
        "            effective = ''\n",
        "            disposed = ''\n",
        "            serial = ''\n",
        "            age = ''\n",
        "            years_in_storage = ''\n",
        "            if date_rel_idx:\n",
        "                first_rel = date_rel_idx[0]\n",
        "                if first_rel > 0:\n",
        "                    type_tok = remaining[first_rel - 1]\n",
        "                rel_i = first_rel\n",
        "                dates: List[str] = []\n",
        "                while rel_i < len(remaining) and is_valid_date(remaining[rel_i]):\n",
        "                    dates.append(remaining[rel_i])\n",
        "                    rel_i += 1\n",
        "                acquire = dates[0] if len(dates) > 0 else ''\n",
        "                effective = dates[1] if len(dates) > 1 else ''\n",
        "                disposed = dates[2] if len(dates) > 2 else ''\n",
        "                if rel_i < len(remaining) and SERIAL_RE.match(remaining[rel_i]):\n",
        "                    serial = remaining[rel_i]\n",
        "                    rel_i += 1\n",
        "                desc_end = first_rel - 1 if type_tok else first_rel\n",
        "                desc = ' '.join(remaining[:desc_end]).strip()\n",
        "                tail = remaining[rel_i:]\n",
        "            else:\n",
        "                pseudo_date_idx = [k for k, t in enumerate(remaining) if re.match(r'^\\d{5,6}$', t)]\n",
        "                if pseudo_date_idx:\n",
        "                    first_pseudo = pseudo_date_idx[0]\n",
        "                    if first_pseudo > 0:\n",
        "                        type_tok = remaining[first_pseudo - 1]\n",
        "                    rel_i = first_pseudo\n",
        "                    while rel_i < len(remaining) and re.match(r'^\\d{5,6}$', remaining[rel_i]):\n",
        "                        rel_i += 1\n",
        "                    if rel_i < len(remaining) and SERIAL_RE.match(remaining[rel_i]):\n",
        "                        serial = remaining[rel_i]\n",
        "                        rel_i += 1\n",
        "                    desc_end = first_pseudo - 1 if type_tok else first_pseudo\n",
        "                    desc = ' '.join(remaining[:desc_end]).strip()\n",
        "                    tail = remaining[rel_i:]\n",
        "                else:\n",
        "                    desc = ' '.join(remaining).strip()\n",
        "                    if remaining and remaining[-1].lower() == 'store':\n",
        "                        type_tok = remaining[-1]\n",
        "                        if len(remaining) > 1 and SERIAL_RE.match(remaining[-2]):\n",
        "                            serial = remaining[-2]\n",
        "                            desc = ' '.join(remaining[:-2]).strip()\n",
        "                        else:\n",
        "                            desc = ' '.join(remaining[:-1]).strip()\n",
        "                    tail = []\n",
        "            if tail:\n",
        "                ints = [t for t in tail if re.match(r'^\\d{1,3}$', t)]\n",
        "                if len(ints) >= 1:\n",
        "                    years_in_storage = ints[-1]\n",
        "                if len(ints) >= 2:\n",
        "                    age = ints[-2]\n",
        "            rec = {\n",
        "                'Region': region, 'FONUM': fonum, 'FOSHORT': foshort, 'Loc': loc, 'LNAME': lname,\n",
        "                'Asset': asset, 'Class': clazz, 'Desc': desc, 'Type': type_tok,\n",
        "                'Acquire': acquire, 'Effective': effective, 'SerialNum': serial,\n",
        "            }\n",
        "            if disposed:\n",
        "                rec['Disposed'] = disposed\n",
        "            if age:\n",
        "                rec['Age'] = age\n",
        "            if years_in_storage:\n",
        "                rec['Years_in_Storage'] = years_in_storage\n",
        "            out.append(rec)\n",
        "        except Exception:\n",
        "            continue\n",
        "    return out\n",
        "\n",
        "# -- Floor summary section parsing --\n",
        "def parse_floor_summary_parts(parts: List[str]) -> Dict[str, Optional[str]]:\n",
        "    rec: Dict[str, Optional[str]] = {}\n",
        "    tokens = parts[0].split()\n",
        "    if not tokens:\n",
        "        return rec\n",
        "    rec['FONUM'] = tokens[0]\n",
        "    rec['Location'] = ' '.join(tokens[1:]) if len(tokens) > 1 else ''\n",
        "    open_date = None\n",
        "    close_date = None\n",
        "    if len(parts) > 2:\n",
        "        dates = parts[2].split()\n",
        "        if dates:\n",
        "            open_date = dates[0]\n",
        "            if len(dates) > 1:\n",
        "                close_date = dates[1]\n",
        "    rec['Open'] = open_date\n",
        "    rec['Closed'] = close_date\n",
        "    rec['CountA'] = parts[3] if len(parts) > 3 else None\n",
        "    if len(parts) > 4:\n",
        "        sub = parts[4].split(None, 1)\n",
        "        rec['CountB'] = sub[0] if sub else None\n",
        "        rec['Service'] = sub[1] if len(sub) > 1 else None\n",
        "    else:\n",
        "        rec['CountB'] = None\n",
        "        rec['Service'] = None\n",
        "    if len(parts) > 5:\n",
        "        sub = parts[5].split(None, 1)\n",
        "        rec['CountC'] = sub[0]\n",
        "        rec['Region2'] = sub[1] if len(sub) > 1 else None\n",
        "    else:\n",
        "        rec['CountC'] = None\n",
        "        rec['Region2'] = None\n",
        "    if len(parts) > 6:\n",
        "        sub = parts[6].split(None, 1)\n",
        "        rec['CountD'] = sub[0]\n",
        "        rec['Country'] = sub[1] if len(sub) > 1 else None\n",
        "    else:\n",
        "        rec['CountD'] = None\n",
        "        rec['Country'] = None\n",
        "    rec['SiteCode'] = parts[7] if len(parts) > 7 else None\n",
        "    rec['Code'] = parts[8] if len(parts) > 8 else None\n",
        "    rec['Person'] = parts[9] if len(parts) > 9 else None\n",
        "    rec['DuplicateName'] = parts[10] if len(parts) > 10 else None\n",
        "    rec['Occupancy'] = parts[11] if len(parts) > 11 else None\n",
        "    rec['RegionCode'] = parts[12] if len(parts) > 12 else None\n",
        "    rec['Fields'] = '|'.join(parts)\n",
        "    return rec\n",
        "\n",
        "# -- Floor asset details + summary detection/parsing --\n",
        "def parse_floor_details_page(page_text: str) -> Tuple[List[Dict[str, str]], List[Dict[str, Optional[str]]]]:\n",
        "    rows: List[Dict[str, str]] = []\n",
        "    extras: List[Dict[str, Optional[str]]] = []\n",
        "    for line in page_text.split('\\n'):\n",
        "        if not line.strip():\n",
        "            continue\n",
        "        if not (re.match(r'^\\s*\\d', line) and 'Floor' in line):\n",
        "            continue\n",
        "        parts = [p.strip() for p in re.split(r'\\s{2,}', line) if p.strip()]\n",
        "        if len(parts) < 3:\n",
        "            continue\n",
        "        candidate_region = parts[1]\n",
        "        if candidate_region not in ALLOWED_FLOOR_REGIONS:\n",
        "            extras.append(parse_floor_summary_parts(parts))\n",
        "            continue\n",
        "        # detail row\n",
        "        first = parts[0]\n",
        "        toks = first.split(None, 1)\n",
        "        loc = toks[0]\n",
        "        place = toks[1] if len(toks) > 1 else ''\n",
        "        region = parts[1] if len(parts) > 1 else ''\n",
        "        svc = parts[2] if len(parts) > 2 else ''\n",
        "        asset = ''\n",
        "        serial = ''\n",
        "        if len(parts) > 3:\n",
        "            asset_serial = parts[3].split(None, 1)\n",
        "            asset = asset_serial[0]\n",
        "            serial = asset_serial[1] if len(asset_serial) > 1 else ''\n",
        "        type_ = ''\n",
        "        desc = ''\n",
        "        i = 4\n",
        "        if i < len(parts):\n",
        "            td = parts[i].split()\n",
        "            type_ = td[0] if td else ''\n",
        "            desc = ' '.join(td[1:]) if len(td) > 1 else ''\n",
        "            i += 1\n",
        "        acquire = ''\n",
        "        effective = ''\n",
        "        disposed = ''\n",
        "        dates: List[str] = []\n",
        "        leftover: List[str] = []\n",
        "        while i < len(parts) and len(dates) < 3:\n",
        "            token = parts[i]\n",
        "            for st in token.split():\n",
        "                if DATE_RE.match(st):\n",
        "                    dates.append(st)\n",
        "                else:\n",
        "                    leftover.append(st)\n",
        "            i += 1\n",
        "        if len(dates) > 0:\n",
        "            acquire = dates[0]\n",
        "        if len(dates) > 1:\n",
        "            effective = dates[1]\n",
        "        if len(dates) > 2:\n",
        "            disposed = dates[2]\n",
        "        clazz = ''\n",
        "        mfg = ''\n",
        "        class_mfg_parts: List[str] = leftover.copy()\n",
        "        while i < len(parts) and len(' '.join(class_mfg_parts).split()) < 2:\n",
        "            class_mfg_parts.append(parts[i])\n",
        "            i += 1\n",
        "        cm = ' '.join(class_mfg_parts).split()\n",
        "        if cm:\n",
        "            clazz = cm[0]\n",
        "        if len(cm) > 1:\n",
        "            mfg = cm[1]\n",
        "        lname = parts[i] if i < len(parts) else ''\n",
        "        i += 1\n",
        "        fonum = ''\n",
        "        foshort = ''\n",
        "        if i < len(parts):\n",
        "            fo_parts = parts[i].split(None, 1)\n",
        "            fonum = fo_parts[0]\n",
        "            foshort = fo_parts[1] if len(fo_parts) > 1 else ''\n",
        "            i += 1\n",
        "        cat = parts[i] if i < len(parts) else ''\n",
        "        i += 1\n",
        "        year = parts[i] if i < len(parts) else ''\n",
        "        i += 1\n",
        "        age = ''\n",
        "        report_date = ''\n",
        "        if i < len(parts):\n",
        "            remaining = parts[i:]\n",
        "            if len(remaining) == 1:\n",
        "                token = remaining[0]\n",
        "                if DATE_RE.match(token):\n",
        "                    report_date = token\n",
        "                else:\n",
        "                    age = token\n",
        "            else:\n",
        "                age_candidate = remaining[0]\n",
        "                last_token = remaining[-1]\n",
        "                if DATE_RE.match(last_token):\n",
        "                    report_date = last_token\n",
        "                    age = age_candidate\n",
        "                else:\n",
        "                    age = ' '.join(remaining)\n",
        "        rows.append({\n",
        "            'Loc': loc, 'Place': place, 'Region': region, 'Service': svc, 'Asset': asset,\n",
        "            'SerialNum': serial, 'Type': type_, 'Desc': desc, 'Acquire': acquire,\n",
        "            'Effective': effective, 'Disposed': disposed, 'Class': clazz, 'MFG': mfg,\n",
        "            'LNAME': lname, 'FONUM': fonum, 'FOSHORT': foshort, 'Cat': cat,\n",
        "            'Year': year, 'Age': age, 'ReportDate': report_date\n",
        "        })\n",
        "    return rows, extras\n",
        "\n",
        "# -- Site status parsing --\n",
        "def parse_site_line(row: str) -> Optional[Dict[str, str]]:\n",
        "    toks = row.split()\n",
        "    if not toks or not toks[0].isdigit():\n",
        "        return None\n",
        "    open_idx = None\n",
        "    for i in range(1, len(toks)):\n",
        "        if DATE_RE.match(toks[i]):\n",
        "            open_idx = i\n",
        "            break\n",
        "    if open_idx is None or open_idx < 2:\n",
        "        return None\n",
        "    loc = toks[0]\n",
        "    place = toks[open_idx - 1]\n",
        "    lname = ' '.join(toks[1:open_idx - 1])\n",
        "    open_date = toks[open_idx]\n",
        "    closed_date = toks[open_idx + 1] if open_idx + 1 < len(toks) and DATE_RE.match(toks[open_idx + 1]) else ''\n",
        "    base = open_idx + (2 if closed_date else 1)\n",
        "    ksi = toks[base] if base < len(toks) else ''\n",
        "    cmty_num = toks[base + 1] if base + 1 < len(toks) else ''\n",
        "    svc_idx = None\n",
        "    j = base + 2\n",
        "    while j < len(toks):\n",
        "        if toks[j] in {'Army', 'Navy', 'Air'}:\n",
        "            svc_idx = j; break\n",
        "        if toks[j] == 'Marine' and j + 1 < len(toks) and toks[j + 1] == 'Corps':\n",
        "            svc_idx = j; break\n",
        "        j += 1\n",
        "    if svc_idx is None:\n",
        "        return None\n",
        "    cmty = ' '.join(toks[base + 2:svc_idx])\n",
        "    if toks[svc_idx] == 'Marine' and svc_idx + 1 < len(toks) and toks[svc_idx + 1] == 'Corps':\n",
        "        svc = 'Marine Corps'\n",
        "        fonum_start = svc_idx + 2\n",
        "    else:\n",
        "        svc = toks[svc_idx]\n",
        "        fonum_start = svc_idx + 1\n",
        "    if fonum_start >= len(toks):\n",
        "        return None\n",
        "    fonum = toks[fonum_start]\n",
        "    foshort = ' '.join(toks[fonum_start + 1:]) if fonum_start + 1 < len(toks) else ''\n",
        "    return {\n",
        "        'Loc': loc, 'LNAME': lname, 'Place': place,\n",
        "        'Open': open_date, 'Closed': closed_date, 'KSI': ksi,\n",
        "        'CmtyNum': cmty_num, 'Cmty': cmty, 'SVC': svc,\n",
        "        'FONUM': fonum, 'FOSHORT': foshort,\n",
        "    }\n",
        "\n",
        "def parse_site_status_page(page_text: str) -> List[Dict[str, str]]:\n",
        "    lines = [ln.rstrip() for ln in page_text.split('\\n') if ln.strip()]\n",
        "    rows: List[str] = []\n",
        "    buf: List[str] = []\n",
        "    def flush():\n",
        "        if buf:\n",
        "            rows.append(' '.join(buf))\n",
        "            buf.clear()\n",
        "    for ln in lines:\n",
        "        if re.match(r'^\\s*\\d+\\b', ln):\n",
        "            flush()\n",
        "            buf.append(ln.strip())\n",
        "        else:\n",
        "            if buf:\n",
        "                buf.append(ln.strip())\n",
        "            else:\n",
        "                continue\n",
        "    flush()\n",
        "    out: List[Dict[str, str]] = []\n",
        "    for r in rows:\n",
        "        rec = parse_site_line(r)\n",
        "        if rec:\n",
        "            out.append(rec)\n",
        "    return out\n",
        "\n",
        "# =============================================================================\n",
        "# D) Extract detailed tables (using read_pdf_text_layout)\n",
        "# =============================================================================\n",
        "def extract_detailed_tables(pdf_path: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
        "    text = read_pdf_text_layout(pdf_path)\n",
        "    pages = text.split('\\f')\n",
        "    month_map = detect_month_map(pages)\n",
        "\n",
        "    asset_data: List[Dict[str, str]] = []\n",
        "    floor_data: List[Dict[str, str]] = []\n",
        "    extra_floor_data: List[Dict[str, Optional[str]]] = []\n",
        "    site_data: List[Dict[str, str]] = []\n",
        "\n",
        "    current_month: Optional[str] = None\n",
        "    for p_num, page_text in enumerate(pages, start=1):\n",
        "        if not page_text.strip():\n",
        "            continue\n",
        "        month = month_map.get(p_num)\n",
        "        if month:\n",
        "            current_month = month\n",
        "\n",
        "        for rec in parse_asset_details_page(page_text):\n",
        "            if current_month:\n",
        "                rec['Month'] = current_month\n",
        "            asset_data.append(rec)\n",
        "\n",
        "        floor_recs, extra_recs = parse_floor_details_page(page_text)\n",
        "        for rec in floor_recs:\n",
        "            if current_month:\n",
        "                rec['Month'] = current_month\n",
        "            floor_data.append(rec)\n",
        "        for rec in extra_recs:\n",
        "            if current_month:\n",
        "                rec['Month'] = current_month\n",
        "            extra_floor_data.append(rec)\n",
        "\n",
        "        for rec in parse_site_status_page(page_text):\n",
        "            if current_month:\n",
        "                rec['Month'] = current_month\n",
        "            site_data.append(rec)\n",
        "\n",
        "    asset_df = pd.DataFrame(asset_data).drop_duplicates()\n",
        "    floor_df = pd.DataFrame(floor_data).drop_duplicates()\n",
        "    site_df = pd.DataFrame(site_data).drop_duplicates()\n",
        "    extra_floor_df = pd.DataFrame(extra_floor_data).drop_duplicates()\n",
        "\n",
        "    # Years in storage pivot\n",
        "    if not asset_df.empty:\n",
        "        yrs_col = None\n",
        "        for c in asset_df.columns:\n",
        "            if c.lower() in {'years_in_storage', 'years', 'yrs', 'yrs_in_storage'}:\n",
        "                yrs_col = c\n",
        "                break\n",
        "        if yrs_col is None:\n",
        "            asset_df['Years_in_Storage'] = 0\n",
        "            yrs_col = 'Years_in_Storage'\n",
        "        age_num = pd.to_numeric(asset_df.get('Age'), errors='coerce')\n",
        "        yis_num = pd.to_numeric(asset_df[yrs_col], errors='coerce').fillna(0)\n",
        "        try:\n",
        "            INT64_NULLABLE = pd.Int64Dtype()\n",
        "            asset_df['Age_int'] = age_num.astype(INT64_NULLABLE)\n",
        "            asset_df['Years_in_Storage_int'] = yis_num.astype(INT64_NULLABLE)\n",
        "        except Exception:\n",
        "            asset_df['Age_int'] = age_num.astype('float64')\n",
        "            asset_df['Years_in_Storage_int'] = yis_num.astype('int64')\n",
        "        filt = asset_df['Age_int'].notna() & (asset_df['Age_int'] <= 50)\n",
        "        pivot_tables: List[pd.DataFrame] = []\n",
        "        if 'Month' in asset_df.columns:\n",
        "            for mth, grp in asset_df[filt].groupby('Month', dropna=False):\n",
        "                pt = pd.pivot_table(\n",
        "                    grp,\n",
        "                    index='Age_int',\n",
        "                    columns='Years_in_Storage_int',\n",
        "                    values='Asset',\n",
        "                    aggfunc='count',\n",
        "                    fill_value=0\n",
        "                )\n",
        "                if isinstance(pt, pd.Series):\n",
        "                    pt = pt.to_frame()\n",
        "                pt = pt.reset_index().rename(columns={'Age_int': 'Age'})\n",
        "                pt.columns = ['Age'] + [f'Storage_{c}' for c in pt.columns[1:]]\n",
        "                if mth is not None:\n",
        "                    pt.insert(0, 'Month', mth)\n",
        "                pivot_tables.append(pt)\n",
        "        else:\n",
        "            pt = pd.pivot_table(\n",
        "                asset_df[filt],\n",
        "                index='Age_int',\n",
        "                columns='Years_in_Storage_int',\n",
        "                values='Asset',\n",
        "                aggfunc='count',\n",
        "                fill_value=0\n",
        "            )\n",
        "            if isinstance(pt, pd.Series):\n",
        "                pt = pt.to_frame()\n",
        "            pt = pt.reset_index().rename(columns={'Age_int': 'Age'})\n",
        "            pt.columns = ['Age'] + [f'Storage_{c}' for c in pt.columns[1:]]\n",
        "            pivot_tables.append(pt)\n",
        "        years_storage_df = pd.concat(pivot_tables, ignore_index=True) if pivot_tables else pd.DataFrame()\n",
        "    else:\n",
        "        years_storage_df = pd.DataFrame()\n",
        "\n",
        "    return asset_df, floor_df, site_df, years_storage_df, extra_floor_df\n",
        "\n",
        "# =============================================================================\n",
        "# E) Save output results (keep your 8 tables including extra_floor_df)\n",
        "# =============================================================================\n",
        "def save_outputs(outdir: str,\n",
        "                 region_df: pd.DataFrame,\n",
        "                 field_df: pd.DataFrame,\n",
        "                 installed_df: pd.DataFrame,\n",
        "                 asset_df: pd.DataFrame,\n",
        "                 floor_df: pd.DataFrame,\n",
        "                 site_df: pd.DataFrame,\n",
        "                 years_storage_df: pd.DataFrame,\n",
        "                 extra_floor_df: pd.DataFrame) -> None:\n",
        "    os.makedirs(outdir, exist_ok=True)\n",
        "    if not region_df.empty:\n",
        "        region_df.to_csv(os.path.join(outdir, 'assets_by_region_service.csv'), index=False)\n",
        "    if not field_df.empty:\n",
        "        field_df.to_csv(os.path.join(outdir, 'assets_by_field_office.csv'), index=False)\n",
        "    if not installed_df.empty:\n",
        "        installed_df.to_csv(os.path.join(outdir, 'installed_assets_location_manufacture.csv'), index=False)\n",
        "    if not asset_df.empty:\n",
        "        asset_df.to_csv(os.path.join(outdir, 'asset_details.csv'), index=False)\n",
        "    if not floor_df.empty:\n",
        "        floor_df.to_csv(os.path.join(outdir, 'floor_asset_details.csv'), index=False)\n",
        "    if not site_df.empty:\n",
        "        site_df.to_csv(os.path.join(outdir, 'site_operational_status.csv'), index=False)\n",
        "    if not years_storage_df.empty:\n",
        "        years_storage_df.to_csv(os.path.join(outdir, 'years_in_storage.csv'), index=False)\n",
        "    if not extra_floor_df.empty:\n",
        "        # Eighth table: Floor Summary (retain/output)\n",
        "        extra_floor_df.to_csv(os.path.join(outdir, 'floor_summary_details.csv'), index=False)\n",
        "\n",
        "# =============================================================================\n",
        "# F) Main execution function\n",
        "# =============================================================================\n",
        "def run_extraction(pdf_path: str, outdir: str='/content/output'):\n",
        "    # Summary tables\n",
        "    region_df, field_df, installed_df = extract_region_field_installed_v2(pdf_path)\n",
        "    # Detailed tables (including the eighth table)\n",
        "    asset_df, floor_df, site_df, years_storage_df, extra_floor_df = extract_detailed_tables(pdf_path)\n",
        "    # Save all tables\n",
        "    save_outputs(outdir, region_df, field_df, installed_df, asset_df, floor_df, site_df, years_storage_df, extra_floor_df)\n",
        "    print('Extraction complete.')\n",
        "    for name, df in [('Region', region_df), ('FieldOffice', field_df),\n",
        "                     ('Installed', installed_df), ('AssetDetails', asset_df),\n",
        "                     ('FloorDetails', floor_df), ('SiteOperational', site_df),\n",
        "                     ('YearsInStorage', years_storage_df), ('FloorSummary', extra_floor_df)]:\n",
        "        if not df.empty:\n",
        "            print(f'[{name}] rows={len(df):,}, cols={len(df.columns)}')\n",
        "\n",
        "# -- Execute parsing for multiple years --\n",
        "year_jobs = [\n",
        "    (\"/content/FY2020_Asset_Report.pdf\", \"/content/FY2020_Asset_Report_output\"),\n",
        "    (\"/content/FY2022_Asset_Report.pdf\", \"/content/FY2022_Asset_Report_output\"),\n",
        "    (\"/content/FY2023_Asset_Report.pdf\", \"/content/FY2023_Asset_Report_output\"),\n",
        "    (\"/content/FY2024_Asset_Report.pdf\", \"/content/FY2024_Asset_Report_output\"),\n",
        "]\n",
        "\n",
        "for pdf_path, outdir in year_jobs:\n",
        "    print(f\"\\n=== Running extraction for: {pdf_path} ===\")\n",
        "    run_extraction(pdf_path=pdf_path, outdir=outdir)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAoQJU5ENJSe",
        "outputId": "d29d912b-1265-41f0-bb91-8dcf2df6004f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Running extraction for: /content/FY2020_Asset_Report.pdf ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4293828176.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  return pd.concat(kept, ignore_index=True) if kept else pd.DataFrame()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n",
            "[Region] rows=40, cols=9\n",
            "[FieldOffice] rows=210, cols=9\n",
            "[Installed] rows=566, cols=19\n",
            "[AssetDetails] rows=3,084, cols=16\n",
            "[FloorDetails] rows=16,157, cols=21\n",
            "[SiteOperational] rows=582, cols=12\n",
            "[FloorSummary] rows=582, cols=19\n",
            "\n",
            "=== Running extraction for: /content/FY2022_Asset_Report.pdf ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4293828176.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  return pd.concat(kept, ignore_index=True) if kept else pd.DataFrame()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n",
            "[Region] rows=52, cols=9\n",
            "[FieldOffice] rows=273, cols=9\n",
            "[Installed] rows=819, cols=19\n",
            "[AssetDetails] rows=3,014, cols=17\n",
            "[FloorDetails] rows=22,445, cols=21\n",
            "[SiteOperational] rows=836, cols=12\n",
            "[YearsInStorage] rows=112, cols=58\n",
            "[FloorSummary] rows=836, cols=19\n",
            "\n",
            "=== Running extraction for: /content/FY2023_Asset_Report.pdf ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4293828176.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  return pd.concat(kept, ignore_index=True) if kept else pd.DataFrame()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n",
            "[Region] rows=48, cols=9\n",
            "[FieldOffice] rows=252, cols=9\n",
            "[Installed] rows=888, cols=19\n",
            "[AssetDetails] rows=4,911, cols=17\n",
            "[FloorDetails] rows=20,445, cols=21\n",
            "[SiteOperational] rows=994, cols=12\n",
            "[YearsInStorage] rows=125, cols=56\n",
            "[FloorSummary] rows=2,858, cols=19\n",
            "\n",
            "=== Running extraction for: /content/FY2024_Asset_Report.pdf ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4293828176.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  return pd.concat(kept, ignore_index=True) if kept else pd.DataFrame()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extraction complete.\n",
            "[Region] rows=36, cols=9\n",
            "[FieldOffice] rows=189, cols=9\n",
            "[Installed] rows=743, cols=19\n",
            "[AssetDetails] rows=4,526, cols=17\n",
            "[FloorDetails] rows=9,240, cols=21\n",
            "[SiteOperational] rows=753, cols=12\n",
            "[YearsInStorage] rows=97, cols=62\n",
            "[FloorSummary] rows=7,316, cols=19\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
